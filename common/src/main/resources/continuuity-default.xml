<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
    Copyright (c) to Continuuity Inc. All rights reserved.
-->
<configuration>

    <!--
        Common configuration across all components.
    -->
    <property>
        <name>zookeeper.quorum</name>
        <value>localhost:2181</value>
        <description>Specifies the zookeeper host:port</description>
    </property>

    <property>
        <name>command.port.enabled</name>
        <value>false</value>
        <description>Specifies whether command port should be enabled.</description>
    </property>

    <property>
        <name>thrift.max.read.buffer</name>
        <value>16777216</value>
        <description>
            Specifies the max read buffer size used by
            thrift server. Value should be set to something greater
            than max frame that is sent on RPC channel.
        </description>
    </property>

    <property>
        <name>state.storage.connection.url</name>
        <value>jdbc:hsqldb:file:data/flow-state-management;user=sa</value>
        <description>
            Specifies the JDBC connection url for connecting to local or
            remote storage for storing flow state, runs and active flows.
        </description>
    </property>

    <property>
        <name>weave.zookeeper.namespace</name>
        <value>/weave</value>
        <description>Namespace prefix for weave zookeeper</description>
    </property>

    <property>
        <name>hdfs.namespace</name>
        <value>/continuuity</value>
        <description>Namespace for files written by reactor.</description>
    </property>

    <property>
        <name>hdfs.user</name>
        <value>yarn</value>
        <description>User name for accessing HDFS.</description>
    </property>

    <property>
        <name>yarn.user</name>
        <value>yarn</value>
        <description>User name for running applications in YARN.</description>
    </property>

    <property>
        <name>local.data.dir</name>
        <value>data</value>
        <description>Data directory for local mode</description>
    </property>

    <!--
        Resource manager configurations.
    -->
    <property>
        <name>resource.manager.server.port</name>
        <value>45000</value>
        <description>Specifies the port on which Flow resource Manager will run.</description>
    </property>

    <property>
        <name>resource.manager.server.threads</name>
        <value>10</value>
        <description>Specifies number of threads to be started for serving requests on flow resource manager</description>
    </property>

    <property>
        <name>resource.manager.remote.dir</name>
        <value>data/resource/manager/remote</value>
        <description>Specifies the base directory for storing resources</description>
    </property>

    <property>
        <name>resource.manager.local.dir</name>
        <value>data/resource/manager/local</value>
        <description>Specifies the base directory for storing resources</description>
    </property>

    <property>
        <name>resource.manager.server.address</name>
        <value>localhost</value>
        <description>Specifies the address to which the resource managerment service should bind to.</description>
    </property>

    <property>
        <name>resource.manager.cloud.hostname</name>
        <value>50.23.86.172</value>
        <description>Specifies the resource manager hostname in cloud.</description>
    </property>

    <property>
        <name>resource.manager.cloud.port</name>
        <value>45000</value>
        <description>Specifies the resource manager port in cloud.</description>
    </property>


    <!--
        Flow Manager configuration.
    -->
    <property>
        <name>flow.manager.server.address</name>
        <value>localhost</value>
        <description>Specifies the address to which the flow manager service should bind to.</description>
    </property>

    <property>
        <name>flow.manager.server.port</name>
        <value>45001</value>
        <description>Specifies the port on which Flow resource Manager will run.</description>
    </property>

    <property>
        <name>flow.manager.server.threads</name>
        <value>10</value>
        <description>Specifies number of threads to be started for serving requests on flow resource manager</description>
    </property>

    <!--
        Gateway configuration
    -->
    <property>
        <name>gateway.server.address</name>
        <value>localhost</value>
        <description>Specifies the hostname on which the Gateway will listen</description>
    </property>

    <property>
        <name>gateway.port</name>
        <value>9999</value>
        <description>Specifies the port on which the Gateway will listen</description>
    </property>

    <property>
        <name>gateway.connectors</name>
        <value>data.rest,stream.flume,procedure.rest,monitor.rest,app.rest,systemstats.rest,meta.rest</value>
        <description>Specifies the list of Collectors we will use</description>
    </property>

    <!-- Todd: I'm really unhappy that we have to use so much XML to describe
         the Collectors, but that's 'cos we are using the Hadoop config object -->
    <property>
        <name>data.rest.class</name>
        <value>com.continuuity.gateway.accessor.DatasetRestAccessor</value>
        <description>Specifies the class for the REST accessor</description>
    </property>

    <property>
        <name>data.rest.port</name>
        <value>10002</value>
    </property>

    <property>
        <name>data.rest.threads</name>
        <value>20</value>
    </property>

    <property>
        <name>data.rest.middle</name>
        <value>/data/</value>
    </property>

    <property>
        <name>gateway.connection.backlog</name>
        <value>20000</value>
        <description>Max connection backlog of gateway</description>
    </property>

    <property>
        <name>gateway.max.cached.stream.events.num</name>
        <value>10000</value>
        <description>Max number of stream events cached before flushing</description>
    </property>

    <property>
        <name>gateway.max.cached.events.per.stream.num</name>
        <value>5000</value>
        <description>Max number of stream events of a single stream cached before flushing</description>
    </property>

    <property>
        <name>gateway.max.cached.stream.events.bytes</name>
        <value>52428800</value>
        <description>Max size of stream events cached before flushing</description>
    </property>

    <property>
        <name>gateway.stream.events.flush.interval.ms</name>
        <value>150</value>
        <description>Specifies the interval at which cached stream events get flushed</description>
    </property>

    <property>
        <name>gateway.stream.callback.exec.num.threads</name>
        <value>5</value>
        <description>Number of threads in stream events callback executor</description>
    </property>

    <property>
        <name>gateway.exec.threads</name>
        <value>20</value>
        <description>Number of netty server executor threads</description>
    </property>

    <property>
        <name>gateway.boss.threads</name>
        <value>1</value>
        <description>Number of netty server boss threads</description>
    </property>

    <property>
        <name>gateway.worker.threads</name>
        <value>10</value>
        <description>Number of netty server worker threads</description>
    </property>


    <!-- Metadata Connector -->
    <property>
        <name>meta.rest.class</name>
        <value>com.continuuity.gateway.accessor.MetaDataRestAccessor</value>
        <description>Specifies the class for the meta data REST accessor</description>
    </property>

    <property>
        <name>meta.rest.port</name>
        <value>10011</value>
    </property>

    <property>
        <name>meta.rest.threads</name>
        <value>5</value>
    </property>

    <property>
        <name>meta.rest.middle</name>
        <value>/metadata/</value>
    </property>

    <!-- AppFabric Connector -->
    <property>
        <name>app.rest.class</name>
        <value>com.continuuity.gateway.connector.AppFabricRestConnector</value>
        <description>Specifies the class for the REST accessor</description>
    </property>

    <property>
        <name>app.rest.port</name>
        <value>10007</value>
    </property>

    <property>
        <name>app.rest.threads</name>
        <value>5</value>
    </property>

    <property>
        <name>app.rest.middle</name>
        <value>/app</value>
    </property>

    <!-- Procedure Handler -->
    <property>
        <name>procedure.rest.class</name>
        <value>com.continuuity.gateway.accessor.QueryRestAccessor</value>
        <description>Specifies the class for the REST accessor</description>
    </property>

    <property>
        <name>procedure.rest.port</name>
        <value>10010</value>
    </property>

    <property>
        <name>procedure.rest.threads</name>
        <value>20</value>
    </property>

    <property>
        <name>procedure.rest.middle</name>
        <value>/procedure/</value>
    </property>

    <!-- Metrics handler -->
    <property>
        <name>monitor.rest.class</name>
        <value>com.continuuity.gateway.accessor.MonitorRestAccessor</value>
        <description>Specifies the class for the REST metrics
            connector</description>
    </property>

    <property>
        <name>monitor.rest.port</name>
        <value>10005</value>
    </property>

    <property>
        <name>monitor.rest.threads</name>
        <value>5</value>
    </property>

    <property>
        <name>monitor.rest.middle</name>
        <value>/monitor/</value>
    </property>

    <!-- Stream handles -->
    <property>
        <name>stream.rest.port</name>
        <value>10000</value>
    </property>

    <property>
        <name>stream.flume.class</name>
        <value>com.continuuity.gateway.collector.NettyFlumeCollector</value>
        <description>Specifies the class for the Flume collector</description>
    </property>

    <property>
        <name>stream.flume.port</name>
        <value>10004</value>
    </property>

    <property>
        <name>stream.flume.threads</name>
        <value>20</value>
    </property>

    <!-- System stats handler -->
    <property>
        <name>systemstats.rest.class</name>
        <value>com.continuuity.gateway.accessor.SystemStatsRestAccessor</value>
        <description>Specifies the class for the REST system stats
            connector</description>
    </property>

    <property>
        <name>systemstats.rest.port</name>
        <value>10006</value>
    </property>

    <property>
        <name>systemstats.rest.threads</name>
        <value>1</value>
    </property>

    <property>
        <name>systemstats.rest.middle</name>
        <value>/systemstats/</value>
    </property>

    <!--
        Data Fabric configuration
    -->
    <property>
        <name>data.local.jdbc</name>
        <value>jdbc:hsqldb:file:data/fabricdb;user=sa</value>
        <description>Specifies local file system implementation</description>
    </property>

    <property>
        <name>data.local.hsqldb.cache_rows</name>
        <value>64000</value>
        <description>Specifies maximum number of rows of cached 
            tables that are held in memory</description>
    </property>

    <property>
        <name>data.local.hsqldb.cache_size</name>
        <value>64000</value>
        <description>Specifies total size (in kilobytes) of rows in 
            memory cache used with cached tables</description>
    </property>

    <property>
        <name>data.local.leveldb</name>
        <value>${local.data.dir}/ldb</value>
        <description>Specifies the database directory</description>
    </property>

    <property>
        <name>data.local.leveldb.blocksize</name>
        <value>1024</value>
        <description>Specifies block size (in bytes)</description>
    </property>

    <property>
        <name>data.local.leveldb.cachesize</name>
        <value>104857600</value>
        <description>Specifies cache size (in bytes)</description>
    </property>

    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>localhost</value>
        <description>Specifies the zookeeper ensemble for hbase,
            as a comma-separated list of host:port,...</description>
    </property>

    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>62982</value>
        <description>Specifies the zookeeper ensemble for hbase,
            as a comma-separated list of host:port,...</description>
    </property>

    <property>
        <name>data.opex.server.port</name>
        <value>15165</value>
        <description>The port number for the operation executor
            service</description>
    </property>

    <property>
        <name>data.opex.server.address</name>
        <value>localhost</value>
        <description>The inet address for the operation executor
            service</description>
    </property>

    <property>
        <name>data.opex.server.threads</name>
        <value>25</value>
        <description>The number of threads for the operation executor
            service</description>
    </property>

    <property>
        <name>data.opex.client.count</name>
        <value>5</value>
        <description>The number of pooled instanced of the operation
            executor client</description>
    </property>

    <property>
        <name>data.opex.client.provider</name>
        <!-- value>pool</value -->
        <value>thread-local</value>
        <description>The provider strategy for operation executor clients.
            Valid values are "pool" and "thread-local". </description>
    </property>

    <property>
        <name>data.queue.table.name</name>
        <value>data.queue</value>
        <description>Specifies the name of the table for queues.</description>
    </property>

    <!--
        Overlord Metrics & Other overlord related configuration.
    -->
    <property>
        <name>overlord.metrics.collection.server.address</name>
        <value>localhost</value>
        <description>Specifies the server address for metrics
            collection server.</description>
    </property>

    <property>
        <name>overlord.metrics.collection.server.port</name>
        <value>45003</value>
        <description>Specifies the port on which metrics collection
            server will run on.</description>
    </property>

    <property>
        <name>ovelord.metrics.server.opentsdb.enabled</name>
        <value>false</value>
        <description>Specifies whether opentsdb for metric storage is
            enabled or no.</description>
    </property>

    <property>
        <name>overlord.metrics.frontend.server.address</name>
        <value>localhost</value>
        <description>Specifies the server address of metrics frontend
            server</description>
    </property>

    <property>
        <name>overlord.metrics.frontend.server.port</name>
        <value>45002</value>
        <description>Specifies the port on which frontend metrics server
            is started on</description>
    </property>

    <property>
        <name>overlord.metrics.connection.url</name>
        <value>jdbc:hsqldb:file:data/metricsdb?user=sa</value>
        <description>Specifies the connection string where metrics are stored
            </description>
    </property>

    <property>
        <name>opentsdb.server.address</name>
        <value>localhost</value>
        <description>Specifies the open TSDB server address.</description>
    </property>

    <property>
        <name>opentsdb.server.port</name>
        <value>4242</value>
        <description>Specifies the port on which opentsdb will run.</description>
    </property>

    <property>
        <name>overlord.metrics.timeseries.metrics</name>
        <value>processed.count,busyness.count,dataops.count,emitted.count,stream.count,instance.count,flowlet.failure.count,tuples.attempt.read.count,tuples.read.count,query.success.count,query.failed.count</value>
        <description>Specifies a comma seperate list of metrics that
        need to be added to timeseries</description>
    </property>

    <!--
        Metadata service configuration
    -->
    <property>
        <name>metadata.server.address</name>
        <value>localhost</value>
        <description>Specifies the server address of metadata
            server</description>
    </property>

    <property>
        <name>metadata.server.port</name>
        <value>45004</value>
        <description>Specifies the port on which metdata server
            is started on</description>
    </property>


    <!--
      Log collection service configuration
    -->
    <property>
        <name>log.collection.server.address</name>
        <value>localhost</value>
        <description>Specifies the hostname where the collection service runs</description>
    </property>

    <property>
        <name>log.collection.port</name>
        <value>12157</value>
        <description>Port the log collection service runs on</description>
    </property>

    <property>
        <name>log.collection.root</name>
        <value>data/logs</value>
        <description>Root location for collecting logs</description>
    </property>

    <!--
        Account service configuration
    -->
    <property>
        <name>account.server.host</name>
        <value>localhost</value>
        <description>Specifies the host for account server</description>
    </property>

    <property>
        <name>account.server.port</name>
        <value>8080</value>
        <description>Specifies the port for account server</description>
    </property>

    <!-- App Fabric related changes -->
    <property>
        <name>app.server.port</name>
        <value>45000</value>
        <description>App Fabric Server Port</description>
    </property>

    <property>
        <name>app.server.address</name>
        <value>localhost</value>
        <description>Host address on which the app fabric server is started.</description>
    </property>

    <property>
        <name>app.output.dir</name>
        <value>/programs</value>
        <description>Directory where all archives are stored.</description>
    </property>

    <property>
        <name>app.temp.dir</name>
        <value>/tmp</value>
        <description>Directory temp.</description>
    </property>


    <!-- UI Setting -->
    <property>
        <name>webapp.main</name>
        <value>web-app/developer/server/main.js</value>
        <description>Specifies location of the webapp's main.js file</description>
    </property>

    <!-- Sets whether Devsuite is in Cloud or no -->
    <!-- AppFabric will become Reactor later -->
    <property>
        <name>appfabric.environment</name>
        <value>devsuite</value>
        <description>Sets the environment the appfabric is in.</description>
    </property>

    <!-- New Metrics system settings -->
    <property>
        <name>metrics.query.server.address</name>
        <value>localhost</value>
        <description>Host address where the metrics query server is started.</description>
    </property>

    <property>
        <name>metrics.query.server.port</name>
        <value>45005</value>
        <description>Port for metrics query server to listen on.</description>
    </property>

    <property>
        <name>metrics.data.table.retention.resolution.1.seconds</name>
        <value>7200</value>
        <description>Retention hours for the resolution 1 sec table.</description>
    </property>

    <!--
        Logging Configuration
    -->
    <property>
        <name>kafka.seed.brokers</name>
        <value>localhost:9092</value>
        <description>List of Kafka brokers (comma separated)</description>
    </property>

    <property>
        <name>log.publish.num.partitions</name>
        <value>1</value>
        <description>Number of Kafka partitions to publish the logs to</description>
    </property>

    <property>
        <name>log.run.account</name>
        <value>continuuity</value>
        <description>Account to run the logging service</description>
    </property>

    <property>
        <name>log.base.dir</name>
        <value>/tmp/logs/avro</value>
        <description>Base log directory</description>
    </property>

    <property>
        <name>log.retention.duration.days</name>
        <value>7</value>
        <description>Log file retention duration in days</description>
    </property>

    <property>
        <name>log.file.rotation.interval.mins</name>
        <value>1440</value>
        <description>Log file rotation interval (for single node only)</description>
    </property>

    <!--
        Kafka Configuration
    -->
    <property>
        <name>kafka.bind.hostname</name>
        <value>0.0.0.0</value>
        <description>Kafka server hostname to bind to</description>
    </property>

    <property>
        <name>kafka.port</name>
        <value>9092</value>
        <description>Kafka server port</description>
    </property>

    <property>
        <name>kafka.num.partitions</name>
        <value>1</value>
        <description>Default number of partitions for a topic</description>
    </property>

    <property>
        <name>kafka.log.dir</name>
        <value>/tmp/kafka-logs</value>
        <description>Directory to store Kafka logs</description>
    </property>

    <property>
        <name>kafka.zookeeper.namespace</name>
        <value>continuuity_kafka</value>
        <description>ZK namespace for Kafka</description>
    </property>

    <property>
        <name>kafka.default.replication.factor</name>
        <value>1</value>
        <description>Kafka replication factor</description>
    </property>

</configuration>
