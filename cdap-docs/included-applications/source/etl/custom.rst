.. meta::
    :author: Cask Data, Inc.
    :copyright: Copyright © 2015 Cask Data, Inc.

.. _included-apps-custom-etl-plugins:

===========================
Creating Custom ETL Plugins
===========================

Overview
========
This section is intended for developers writing custom ETL plugins. Users of these should
refer to the :ref:`Included Applications <included-apps-index>`.

CDAP provides for the creation of custom ETL plugins for batch/real-time sources/sinks and
transformations to extend the existing ``cdap-etl-batch`` and ``cdap-etl-realtime`` system artifacts.


Plugin Types and Maven Archetypes
=================================
In ETL applications, there are five plugin types:

- Batch Source (*batchsource*)
- Batch Sink (*batchsink*)
- Real-time Source (*realtimesource*)
- Real-time Sink (*realtimesink*)
- Transformation (*transform*)

There are five corresponding Maven archetypes available for starting a plugin project.

Available Annotations
---------------------
These annotations may be used with the plugin classes:

- ``@Plugin``: The class to be exposed as a plugin needs to be annotated with the ``@Plugin``
  annotation and the type of the plugin should be specified (*source*, *sink*, *transformation*).
  By default, the plugin type will be ‘plugin’.

- ``@Name``: Annotation used to name the plugin as well as the properties in the
  Configuration class of the plugin.

- ``@Description``: Annotation used to add a description.

- ``@Nullable``: Annotation indicates that the specific configuration property is
  optional. Such a plugin class can be used without that property being specified.


Creating a Batch Source
=======================
A batch source plugin can be created from a Maven archetype. This command will create a
project for the plugin from the archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-batch-source-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

In order to implement a Batch Source (to be used in the ETL Batch artifact), you extend
the ``BatchSource`` class. You need to define the types of the KEY and VALUE that the Batch
Source will receive and the type of object that the Batch Source will emit to the
subsequent stage (which could be either a Transformation or a Batch Sink). After defining
the types, only one method is required to be implemented:

  ``prepareRun()``

.. rubric:: Methods

- ``prepareRun()``: Used to configure the Hadoop Job configuration (for example, set the
  ``InputFormatClass``).
- ``configurePipeline()``: Used to create any streams or datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``initialize()``: Initialize the Batch Source. Guaranteed to be executed before any call
  to the plugin’s ``transform`` method.
- ``transform()``: This method will be called for every input key-value pair generated by 
  the batch job. By default, the value is emitted to the subsequent stage.

Example::

  @Plugin(type = "batchsource")
  @Name("MyBatchSource")
  @Description("Demo Source")
  public class MyBatchSource extends BatchSource<LongWritable, String, String> {

    @Override
    public void prepareRun(BatchSourceContext context) {
      Job job = context.getHadoopJob();
      job.setInputFormatClass(...);
      // Other Hadoop job configuration related to Input
    }
  }


Creating a Batch Sink
=====================
A batch sink plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-batch-sink-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

In order to implement a Batch Sink (to be used in the ETL Batch artifact), you extend the
``BatchSink`` class. Similar to a Batch Source, you need to define the types of the KEY and
VALUE that the Batch Sink will write in the Batch job and the type of object that it will
accept from the previous stage (which could be either a Transformation or a Batch Source).

After defining the types, only one method is required to be implemented:

  ``prepareRun()``

.. rubric:: Methods

- ``prepareRun()``: Used to configure the Hadoop Job configuration (for ex, set ``OutputFormatClass``).
- ``configurePipeline()``: Used to create any datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``initialize()``: Initialize the Batch Sink runtime. Guaranteed to be executed before
  any call to the plugin’s ``transform`` method.
- ``transform()``: This method will be called for every object that is received from the
  previous stage. The logic inside the method will transform the object to the key-value
  pair expected by the Batch Sink's output format. If you don't override this method, the
  incoming object is set as the Key and the Value is set to null.

Example::

  @Plugin(type = "batchsink")
  @Name("MyBatchSink")
  @Description("Demo Sink")
  public class MyBatchSink extends BatchSink<String, String, NullWritable> {

    @Override
    public void prepareRun(BatchSinkContext context) {
      Job job = context.getHadoopJob();
      job.setOutputFormatClass(...);
      // Other Hadoop job configuration related to Output
    }
  }


Creating a Real-Time Source
===========================
A real-time source plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-realtime-source-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

The only method that needs to be implemented is:

	``poll()``

.. rubric:: Methods

- ``initialize()``: Initialize the real-time source runtime. Guaranteed to be executed
  before any call to the poll method. Usually used to setup the connection to external
  sources.
- ``configurePipeline()``: Used to create any streams or datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``poll()``: Poll method will be invoked during the run of the plugin and in each call,
  the source is expected to emit zero or more objects for the next stage to process. 
- ``destroy()``: Cleanup method executed during the shutdown of the Source. Could be used
  to tear down any external connections made during the initialize method.

Example::

  /**
   * Real-Time Source to poll data from external sources.
   */
  @Plugin(type = "realtimesource")
  @Name("Source")
  @Description("Real-Time Source")
  public class Source extends RealtimeSource<StructuredRecord> {

    private final SourceConfig config;

    public Source(SourceConfig config) {
      this.config = config;
    }

    /**
     * Config class for Source.
     */
    public static class SourceConfig extends PluginConfig {

      @Name("param")
      @Description("Source Param")
      private String param;
      // Note:  only primitives (included boxed types) and string are the types that are supported

    }
  
    @Nullable
    @Override
    public SourceState poll(Emitter<StructuredRecord> writer, SourceState currentState) {
      // Poll for new data
      // Write structured record to the writer
      // writer.emit(writeDefaultRecords(writer);
      return currentState;
    }

    @Override
    public void initialize(RealtimeContext context) throws Exception {
      super.initialize(context);
      // Get Config param and use to initialize
      // String param = config.param
      // Perform init operations, external operations etc.
    }

    @Override
    public void destroy() {
      super.destroy();
      // Handle destroy lifecycle
    }

    private void writeDefaultRecords(Emitter<StructuredRecord> writer){
      Schema.Field bodyField = Schema.Field.of("body", Schema.of(Schema.Type.STRING));
      StructuredRecord.Builder recordBuilder = StructuredRecord.builder(Schema.recordOf("defaultRecord", bodyField));
      recordBuilder.set("body", "Hello");
      writer.emit(recordBuilder.build());
    }
  }


Creating a Real-Time Sink
=========================
A real-time sink plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-realtime-sink-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

The only method that needs to be implemented is:

 ``write()``

.. rubric:: Methods

- ``initialize()``: Initialize the real-time sink runtime. Guaranteed to be executed before
  any call to the ``write`` method. 
- ``configurePipeline()``: Used to create any datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``write()``: The write method will be invoked for a set of objects that needs to be
  persisted. A ``DataWriter`` object can be used to write data to CDAP streams and/or datasets.
  The method is expected to return the number of objects written; this is used for collecting
  metrics.
- ``destroy()``: Cleanup method executed during the shutdown of the Sink. 

Example::

  @Plugin(type = "realtimesink")
  @Name("Demo")
  @Description("Demo Real-Time Sink")
  public class DemoSink extends RealtimeSink<String> {

    @Override
    public int write(Iterable<String> objects, DataWriter dataWriter) {
      int written = 0;
      for (String object : objects) {
        written += 1;
        . . .
      }
      return written;
    }
  }


Creating a Transformation
=========================
In ETL applications, a transformation operation is applied on one object at a time,
converting it into zero or more transformed outputs. A Transformation plugin can be created
using this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-transform-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

The only method that needs to be implemented is:

	``transform()``

.. rubric:: Methods

- ``initialize()``: Used to perform any initialization step that might be required during
  the runtime of the ``Transform``. It is guaranteed that this method will be invoked
  before the ``transform`` method.
- ``transform()``: This method contains the logic that will be applied on each
  incoming data object. An emitter can be used to pass the results to the subsequent stage
  (which could be either another Transformation or a Sink).
- ``destroy()``: Used to perform any cleanup before the plugin shuts down.

Below is an example of a ``DuplicateTransform`` that emits copies of the incoming record
based on the value in the record. In addition, a user metric indicating the number of
copies in each transform is emitted. The user metrics can be queried by using the CDAP 
:ref:`Metrics HTTP RESTful API <http-restful-api-metrics>`::


  @Plugin(type = "transform")
  @Name("Duplicator")
  @Description("Transformation Example that makes copies")

  public class DuplicateTransform extends Transform<StructuredRecord, StructuredRecord> {
  
  private final Config config;

    public static final class Config extends PluginConfig {
    
      @Name("count")
      @Description("Field that indicates number of copies to make")
      private String fieldName; 
    } 
  
    @Override
    public void transform(StructuredRecord input, Emitter<StructuredRecord> emitter) {
      Integer copies = input.get(config.fieldName);
      for (int i = 0; i < copies; i++) {
        emitter.emit(input);
      }
      getContext().getMetrics().count("copies", copies);
    }

    @Override
    public void destroy() {
    
    }
  }


Test Framework for Plugins
==========================

.. include:: ../../../developers-manual/source/testing/testing.rst
   :start-after: .. _test-framework-strategies-artifacts:
   :end-before:  .. _test-framework-validating-sql:

Additional information on unit testing with CDAP is in the Developers’ Manual section
on :ref:`Testing a CDAP Application <test-framework>`.


Source State in a Real-Time Source
==================================
Real-time plugins are executed in workers. During failure, there is the possibility that
the data that is emitted from the Source will not be processed by subsequent stages. In
order to avoid such data loss, SourceState can be used to persist the information about
the external source (for example, offset) if supported by the Source. 

In case of failure, when the poll method is invoked, the offset last persisted is passed
to the poll method, which can be used to fetch the data from the last processed point. The
updated Source State information is returned by the poll method. After the data is
processed by any Transformations and then finally persisted by the Sink, the new Source
State information is also persisted. This ensures that there will be no data loss in case
of failures.

::

  @Plugin(type = "realtimesource")
  @Name("Demo")
  @Description("Demo Real-Time Source")
  public class DemoSource extends RealtimeSource<String> {
    private static final Logger LOG = LoggerFactory.getLogger(TestSource.class);
    private static final String COUNT = "count";

    @Nullable
    @Override
    public SourceState poll(Emitter<String> writer, SourceState currentState) {
      try {
        TimeUnit.MILLISECONDS.sleep(100);
      } catch (InterruptedException e) {
        LOG.error("Some Error in Source");
      }

      int prevCount;
      if (currentState.getState(COUNT) != null) {
        prevCount = Bytes.toInt(currentState.getState(COUNT));
        prevCount++;
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      } else {
        prevCount = 1;
        currentState = new SourceState();
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      }

      LOG.info("Emitting data! {}", prevCount);
      writer.emit("Hello World!");
      return currentState;
    }
  }
  
  
.. _included-apps-custom-etl-plugins-plugin-packaging:

Plugin Packaging and Deployment
===============================
To package and deploy your plugin, follow these instructions on `plugin packaging <#plugin-packaging>`__,
`deployment <#deploying-a-system-artifact>`__ and `verification <#deployment-verification>`__.

By using one of the ``etl-plugin`` Maven archetypes, your project will be set up to generate
the required JAR manifest. If you move the plugin class to a different Java package after
the project is created, you will need to modify the configuration of the
``maven-bundle-plugin`` in the ``pom.xml`` file to reflect the package name changes.

If you are developing plugins for the ``cdap-etl-batch`` artifact, be aware that for
classes inside the plugin JAR that you have added to the Hadoop Job configuration directly
(for example, your custom ``InputFormat`` class), you will need to add the Java packages
of those classes to the "Export-Package" as well. This is to ensure those classes are
visible to the Hadoop MapReduce framework during the plugin execution. Otherwise, the
execution will typically fail with a ``ClassNotFoundException``.

Plugin Deployment
-----------------

.. include:: ../../../developers-manual/source/building-blocks/plugins.rst 
   :start-after: .. _plugins-deployment-artifact:
   :end-before:  .. _plugins-use-case:
