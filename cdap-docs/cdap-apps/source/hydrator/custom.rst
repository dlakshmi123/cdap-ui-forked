.. meta::
    :author: Cask Data, Inc.
    :copyright: Copyright © 2015 Cask Data, Inc.

.. _cdap-apps-custom-etl-plugins:

===========================
Creating Custom ETL Plugins
===========================

Overview
========
This section is intended for developers writing custom ETL plugins. Users of these should
refer to the :ref:`Included Applications <cdap-apps-index>`.

CDAP provides for the creation of custom ETL plugins for batch/real-time sources/sinks and
transformations to extend the existing ``cdap-etl-batch`` and ``cdap-etl-realtime`` system artifacts.


Plugin Types and Maven Archetypes
=================================
In ETL applications, there are five plugin types:

- Batch Source (*batchsource*)
- Batch Sink (*batchsink*)
- Real-time Source (*realtimesource*)
- Real-time Sink (*realtimesink*)
- Transformation (*transform*)

There are five corresponding Maven archetypes available for starting a plugin project.

Available Annotations
---------------------
These annotations may be used with the plugin classes:

- ``@Plugin``: The class to be exposed as a plugin needs to be annotated with the ``@Plugin``
  annotation and the type of the plugin should be specified (*source*, *sink*, *transformation*).
  By default, the plugin type will be ‘plugin’.

- ``@Name``: Annotation used to name the plugin as well as the properties in the
  Configuration class of the plugin.

- ``@Description``: Annotation used to add a description.

- ``@Nullable``: Annotation indicates that the specific configuration property is
  optional. Such a plugin class can be used without that property being specified.


Creating a Batch Source
=======================
A batch source plugin can be created from a Maven archetype. This command will create a
project for the plugin from the archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-batch-source-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

In order to implement a Batch Source (to be used in the ETL Batch artifact), you extend
the ``BatchSource`` class. You need to define the types of the KEY and VALUE that the Batch
Source will receive and the type of object that the Batch Source will emit to the
subsequent stage (which could be either a Transformation or a Batch Sink). After defining
the types, only one method is required to be implemented:

  ``prepareRun()``

.. rubric:: Methods

- ``prepareRun()``: Used to configure the Hadoop Job configuration (for example, set the
  ``InputFormatClass``).
- ``configurePipeline()``: Used to create any streams or datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``initialize()``: Initialize the Batch Source. Guaranteed to be executed before any call
  to the plugin’s ``transform`` method.
- ``transform()``: This method will be called for every input key-value pair generated by 
  the batch job. By default, the value is emitted to the subsequent stage.

Example::

  @Plugin(type = "batchsource")
  @Name("MyBatchSource")
  @Description("Demo Source")
  public class MyBatchSource extends BatchSource<LongWritable, String, String> {

    @Override
    public void prepareRun(BatchSourceContext context) {
      Job job = context.getHadoopJob();
      job.setInputFormatClass(...);
      // Other Hadoop job configuration related to Input
    }
  }


Creating a Batch Sink
=====================
A batch sink plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-batch-sink-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

In order to implement a Batch Sink (to be used in the ETL Batch artifact), you extend the
``BatchSink`` class. Similar to a Batch Source, you need to define the types of the KEY and
VALUE that the Batch Sink will write in the Batch job and the type of object that it will
accept from the previous stage (which could be either a Transformation or a Batch Source).

After defining the types, only one method is required to be implemented:

  ``prepareRun()``

.. rubric:: Methods

- ``prepareRun()``: Used to configure the Hadoop Job configuration (for ex, set ``OutputFormatClass``).
- ``configurePipeline()``: Used to create any datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``initialize()``: Initialize the Batch Sink runtime. Guaranteed to be executed before
  any call to the plugin’s ``transform`` method.
- ``transform()``: This method will be called for every object that is received from the
  previous stage. The logic inside the method will transform the object to the key-value
  pair expected by the Batch Sink's output format. If you don't override this method, the
  incoming object is set as the Key and the Value is set to null.

Example::

  @Plugin(type = "batchsink")
  @Name("MyBatchSink")
  @Description("Demo Sink")
  public class MyBatchSink extends BatchSink<String, String, NullWritable> {

    @Override
    public void prepareRun(BatchSinkContext context) {
      Job job = context.getHadoopJob();
      job.setOutputFormatClass(...);
      // Other Hadoop job configuration related to Output
    }
  }


Creating a Real-Time Source
===========================
A real-time source plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-realtime-source-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

The only method that needs to be implemented is:

	``poll()``

.. rubric:: Methods

- ``initialize()``: Initialize the real-time source runtime. Guaranteed to be executed
  before any call to the poll method. Usually used to setup the connection to external
  sources.
- ``configurePipeline()``: Used to create any streams or datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``poll()``: Poll method will be invoked during the run of the plugin and in each call,
  the source is expected to emit zero or more objects for the next stage to process. 
- ``destroy()``: Cleanup method executed during the shutdown of the Source. Could be used
  to tear down any external connections made during the initialize method.

Example::

  /**
   * Real-Time Source to poll data from external sources.
   */
  @Plugin(type = "realtimesource")
  @Name("Source")
  @Description("Real-Time Source")
  public class Source extends RealtimeSource<StructuredRecord> {

    private final SourceConfig config;

    public Source(SourceConfig config) {
      this.config = config;
    }

    /**
     * Config class for Source.
     */
    public static class SourceConfig extends PluginConfig {

      @Name("param")
      @Description("Source Param")
      private String param;
      // Note:  only primitives (included boxed types) and string are the types that are supported

    }
  
    @Nullable
    @Override
    public SourceState poll(Emitter<StructuredRecord> writer, SourceState currentState) {
      // Poll for new data
      // Write structured record to the writer
      // writer.emit(writeDefaultRecords(writer);
      return currentState;
    }

    @Override
    public void initialize(RealtimeContext context) throws Exception {
      super.initialize(context);
      // Get Config param and use to initialize
      // String param = config.param
      // Perform init operations, external operations etc.
    }

    @Override
    public void destroy() {
      super.destroy();
      // Handle destroy lifecycle
    }

    private void writeDefaultRecords(Emitter<StructuredRecord> writer){
      Schema.Field bodyField = Schema.Field.of("body", Schema.of(Schema.Type.STRING));
      StructuredRecord.Builder recordBuilder = StructuredRecord.builder(Schema.recordOf("defaultRecord", bodyField));
      recordBuilder.set("body", "Hello");
      writer.emit(recordBuilder.build());
    }
  }


Creating a Real-Time Sink
=========================
A real-time sink plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-realtime-sink-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

The only method that needs to be implemented is:

 ``write()``

.. rubric:: Methods

- ``initialize()``: Initialize the real-time sink runtime. Guaranteed to be executed before
  any call to the ``write`` method. 
- ``configurePipeline()``: Used to create any datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``write()``: The write method will be invoked for a set of objects that needs to be
  persisted. A ``DataWriter`` object can be used to write data to CDAP streams and/or datasets.
  The method is expected to return the number of objects written; this is used for collecting
  metrics.
- ``destroy()``: Cleanup method executed during the shutdown of the Sink. 

Example::

  @Plugin(type = "realtimesink")
  @Name("Demo")
  @Description("Demo Real-Time Sink")
  public class DemoSink extends RealtimeSink<String> {

    @Override
    public int write(Iterable<String> objects, DataWriter dataWriter) {
      int written = 0;
      for (String object : objects) {
        written += 1;
        . . .
      }
      return written;
    }
  }


Creating a Transformation
=========================
In ETL applications, a transformation operation is applied on one object at a time,
converting it into zero or more transformed outputs. A Transformation plugin can be created
using this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-transform-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

The only method that needs to be implemented is:

	``transform()``

.. rubric:: Methods

- ``initialize()``: Used to perform any initialization step that might be required during
  the runtime of the ``Transform``. It is guaranteed that this method will be invoked
  before the ``transform`` method.
- ``transform()``: This method contains the logic that will be applied on each
  incoming data object. An emitter can be used to pass the results to the subsequent stage
  (which could be either another Transformation or a Sink).
- ``destroy()``: Used to perform any cleanup before the plugin shuts down.

Below is an example of a ``DuplicateTransform`` that emits copies of the incoming record
based on the value in the record. In addition, a user metric indicating the number of
copies in each transform is emitted. The user metrics can be queried by using the CDAP 
:ref:`Metrics HTTP RESTful API <http-restful-api-metrics>`::


  @Plugin(type = "transform")
  @Name("Duplicator")
  @Description("Transformation Example that makes copies")

  public class DuplicateTransform extends Transform<StructuredRecord, StructuredRecord> {
  
  private final Config config;

    public static final class Config extends PluginConfig {
    
      @Name("count")
      @Description("Field that indicates number of copies to make")
      private String fieldName; 
    } 
  
    @Override
    public void transform(StructuredRecord input, Emitter<StructuredRecord> emitter) {
      Integer copies = input.get(config.fieldName);
      for (int i = 0; i < copies; i++) {
        emitter.emit(input);
      }
      getContext().getMetrics().count("copies", copies);
    }

    @Override
    public void destroy() {
    
    }
  }


Test Framework for Plugins
==========================

.. include:: ../../../developers-manual/source/testing/testing.rst
   :start-after: .. _test-framework-strategies-artifacts:
   :end-before:  .. _test-framework-validating-sql:

Additional information on unit testing with CDAP is in the Developers’ Manual section
on :ref:`Testing a CDAP Application <test-framework>`.


Source State in a Real-Time Source
==================================
Real-time plugins are executed in workers. During failure, there is the possibility that
the data that is emitted from the Source will not be processed by subsequent stages. In
order to avoid such data loss, SourceState can be used to persist the information about
the external source (for example, offset) if supported by the Source. 

In case of failure, when the poll method is invoked, the offset last persisted is passed
to the poll method, which can be used to fetch the data from the last processed point. The
updated Source State information is returned by the poll method. After the data is
processed by any Transformations and then finally persisted by the Sink, the new Source
State information is also persisted. This ensures that there will be no data loss in case
of failures.

::

  @Plugin(type = "realtimesource")
  @Name("Demo")
  @Description("Demo Real-Time Source")
  public class DemoSource extends RealtimeSource<String> {
    private static final Logger LOG = LoggerFactory.getLogger(TestSource.class);
    private static final String COUNT = "count";

    @Nullable
    @Override
    public SourceState poll(Emitter<String> writer, SourceState currentState) {
      try {
        TimeUnit.MILLISECONDS.sleep(100);
      } catch (InterruptedException e) {
        LOG.error("Some Error in Source");
      }

      int prevCount;
      if (currentState.getState(COUNT) != null) {
        prevCount = Bytes.toInt(currentState.getState(COUNT));
        prevCount++;
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      } else {
        prevCount = 1;
        currentState = new SourceState();
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      }

      LOG.info("Emitting data! {}", prevCount);
      writer.emit("Hello World!");
      return currentState;
    }
  }
  
  
.. _cdap-apps-custom-etl-plugins-plugin-packaging:

Plugin Packaging and Deployment
===============================
To package and deploy your plugin, follow these instructions on `plugin packaging <#plugin-packaging>`__,
`deployment <#deploying-a-system-artifact>`__ and `verification <#deployment-verification>`__.

To control how your plugin appears in the CDAP UI, include an appropriate :ref:`plugin
widget JSON file <cdap-apps-custom-widget-json>`, as described below.

By using one of the ``etl-plugin`` Maven archetypes, your project will be set up to generate
the required JAR manifest. If you move the plugin class to a different Java package after
the project is created, you will need to modify the configuration of the
``maven-bundle-plugin`` in the ``pom.xml`` file to reflect the package name changes.

If you are developing plugins for the ``cdap-etl-batch`` artifact, be aware that for
classes inside the plugin JAR that you have added to the Hadoop Job configuration directly
(for example, your custom ``InputFormat`` class), you will need to add the Java packages
of those classes to the "Export-Package" as well. This is to ensure those classes are
visible to the Hadoop MapReduce framework during the plugin execution. Otherwise, the
execution will typically fail with a ``ClassNotFoundException``.

Plugin Deployment
-----------------

.. include:: ../../../developers-manual/source/building-blocks/plugins.rst 
   :start-after: .. _plugins-deployment-artifact:
   :end-before:  .. _plugins-use-case:

.. _cdap-apps-custom-widget-json:

Plugin Widget JSON
-------------------------
When a plugin is displayed in the CDAP UI, its properties are represented
by widgets in the :ref:`Cask Hydrator <cdap-apps-cask-hydrator>`. Each property of a
plugin is represented, by default, as a textbox in the user interface. 

To customize the plugin display, a plugin can include a widget JSON file that
specifies the particular widgets and sets of widget attributes used to display the plugin
properties in the CDAP UI.

The widget JSON is composed of two lists:

- a list of property configuration groups and
- a list of output properties. 

.. highlight:: xml

For example::

  {
    "configuration-groups": [
      {"group-1"},
      {"group-2"},
      ...
    ],
    "outputs": [
      {"ouput-property-1"},
      {"ouput-property-2"},
      ...
    ]
  }

Configuration Groups
....................
Configuration groups are a simple grouping of properties of a plugin. A configuration
group is represented as a JSON object with a label and a list of plugin properties for that
group.

For example, in a *Batch Source* plugin, properties such as *Dataset Name*, *Dataset Base
Path*, *Duration*, and *Delay* can be grouped as the *Batch Source Configuration*.

In the case of the *Batch Source* plugin, it could look like this::

  {
    "configuration-groups": [
      {
        "label": "Batch Source Configuration",
        "properties": [
          {"field1"},
          {"field2"},
          {"field3"}
        ]
      }
    ],
    outputs: [
      {output-property1},
      {output-property2},
      ..
    ]
  }

Once a group is established, we can configure how each of the properties inside the group is
represented in the CDAP UI. 

The configuration of each property of the plugin is composed of:

- **widget-type:** The type of widget needed to represent this property.
- **label:** Label to be used in the CDAP UI for the property.
- **name:** Name of the field (as supplied by the CDAP UI backend).
- **widget-attributes:** A map of attributes that the widget type requires be defined in
  order to render the property in the CDAP UI. The attributes vary depending on the
  widget type.
  
Note that with the exception of the value of the *label*, all property values are case-sensitive.

To find the available field names, you can use the Artifact HTTP RESTful API to :ref:`
retrieve plugin details <http-restful-api-artifact-plugin-detail>` for an artifact, which
will include all the available names. (If the artifact is your own, you will already know the
available field names.)

In the case of our *Batch Source* plugin example, the ``configuration-groups`` can be represented by::

  "configuration-groups": [
    {
      "label": "Batch Source",
      "properties": [
        {
          "widget-type": "dataset-selector",
          "label": "Dataset Name",
          "name": "name"
        },
        {
          "widget-type": "textbox",
          "label": "Dataset Base Path",
          "name": "basePath"
        },
        {
          "widget-type": "textbox",
          "label": "Duration",
          "name": "duration"
        },
        {
          "widget-type": "textbox",
          "label": "Delay",
          "name": "delay"
        }
      ]

A widget in the CDAP UI represents a component that will be rendered and used to set a
value of a property of a plugin. These are the different widgets |---| their type, a
description, their attributes (if any), and their output data type |---| that we support in
Cask Hydrator as of version |version|:

.. list-table::
   :widths: 20 25 25 25
   :header-rows: 1

   * - Widget Type
     - Description
     - Attributes
     - Output Data Type
   * - ``textbox``
     - Default HTML textbox, used to enter any string
     - ``default``: default value for the widget
     - ``string``
   * - ``number``
     - Default HTML number textbox that only accepts valid numbers
     - | ``default``: default value for the widget
       | ``min``: minimum value for the number box
       | ``max``: maximum value for the number box
     - string
   * - ``passwordbox``
     - Default HTML password entry box
     - No attributes
     - string
   * - csv
     - Comma-separated values; each value is entered in a separate box
     - No attributes
     - comma-separated string
   * - dsv
     - Delimiter-separated values; each value is entered in a separate box
     - ``delimiter``: delimiter used to separate the values
     - delimiter-separated string
   * - ``json-editor``
     - JSON editor that pretty-prints and auto-formats JSON while it is being entered
     - ``default``: default serialized JSON value for the widget
     - string
   * - ``javascript-editor``, ``python-editor``
     - An editor to write Javascript (``javascript-editor``) or Python (``python-editor``)
       code as a value for a property
     - ``default``: default string value for the widget
     - string
   * - ``keyvalue``
     - A key-value editor for constructing maps of key-value pairs
     - | ``delimiter``: delimiter for the key-value pairs
       | ``kv-delimiter``: delimiter between key and value
     - string
   * - ``keyvalue-dropdown``
     - Similar to *keyvalue* widget, but with a drop-down value list
     - | ``delimiter``: delimiter for the key-value pairs
       | ``kv-delimiter``: delimiter between key and value
       | ``dropdownOptions``: list of drop-down options to display
     - string
   * - ``select``
     - An HTML drop-down with a list of values; allows one choice from the list
     - | ``values``: list of values for the drop-down
       | ``default``: default value from the list
     - string
   * - ``dataset-selector``, ``stream-selector``
     - A type-ahead textbox with a list of datasets (``dataset-selector``) or streams
       (``stream-selector``) from the CDAP instance
     - No attributes
     - string
   * - ``schema``
     - A four-column, editable table to represent a schema of a plugin
     - | ``schema-types``: list of schema types for each field from which the user can chose when setting the schema
       | ``schema-default-type``: default type for each newly-added field in the schema
     - string
   * - ``non-editable-schema-editor``
     - A non-editable widget for displaying a schema
     - ``schema``: schema that will be used as the output schema for the plugin
     - string

Outputs
.......
The *outputs* is a list of plugin properties that represent the output schema of a particular plugin.

The output schema for a plugin can be represented in two different ways, either:

- via an *implicit* schema; or
- via an *explicit* ``Schema`` property.

An **implicit** schema is a pre-determined output schema for a plugin that the plugin developer
enforces. The implicit schema is not associated with any properties of the plugin, but
just enforces the output schema of the plugin, for visual display purposes. An example of
this is the `Twitter real-time source plugin
<https://github.com/caskdata/hydrator-plugins/blob/release/1.2/core-plugins/widgets/Twitter-realtimesource.json>`__.

An **explicit** ``Schema`` property is one that can be defined as the output schema and can be
appropriately configured to make it editable through the CDAP UI.

Output properties are configured in a similar manner as individual properties in configuration
groups. They are composed of a name and a widget type, one of either ``schema`` or
``non-editable-schema-editor``. With the ``schema`` widget type, a list of widget attributes can be included;
with ``non-editable-schema-editor``, a schema is added instead.

For example:

The output properties of the Twitter real-time source, with an explicit, non-editable schema property,
composed of the fields *id*, *time*, *favCount*, *rtCount*, *geoLat*, *geoLong*, and *isRetweet*::

  "outputs": [
    {
      "widget-type": "non-editable-schema-editor",
      "schema": {
        "id": "long",
        "message": "string",
        "lang": [
          "string",
          "null"
        ],
        "time": [
          "long",
          "null"
        ],
        "favCount": "int",
        "rtCount": "int",
        "source": [
          "string",
          "null"
        ],
        "geoLat": [
          "double",
          "null"
        ],
        "geoLong": [
          "double",
          "null"
        ],
        "isRetweet": "boolean"
      }
    }
  ]

In contrast, our "Batch Source" plugin could have a configurable output schema::

  "outputs": [
    {
      "name": "schema",
      "widget-type": "schema",
      "widget-attributes": {
        "schema-types": [
          "boolean",
          "int",
          "long",
          "float",
          "double",
          "string",
          "map<string, string>"
        ],
        "schema-default-type": "string"
      }
    }
  ]


Widget types for output properties are limited to ensure that the schema that is propagated across
different plugins in the CDAP UI is consistent. 

Example Widget JSON
..........................
Based on the above definitions, we could write the complete widget JSON for our *Batch Source* plugin
(with the properties of *name*, *basePath*, *duration*, *delay*, and an output *schema*) as::

  {
    "metadata": {
      "spec-version": "1.0"
    },
    "configuration-groups": [
      {
        "label": "Batch Source",
        "properties": [
          {
            "widget-type": "dataset-selector",
            "label": "Dataset Name",
            "name": "name"
          },
          {
            "widget-type": "textbox",
            "label": "Dataset Base Path",
            "name": "basePath"
          },
          {
            "widget-type": "textbox",
            "label": "Duration",
            "name": "duration"
          },
          {
            "widget-type": "textbox",
            "label": "Delay",
            "name": "delay"
          }
        ]
      }
    ],
    "outputs": [
      {
        "name": "schema",
        "widget-type": "schema",
        "widget-attributes": {
          "schema-types": [
            "boolean",
            "int",
            "long",
            "float",
            "double",
            "string",
            "map<string, string>"
          ],
          "schema-default-type": "string"
        }
      }
    ]
  }
  