Index: src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java	(revision 1383334)
+++ src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java	(working copy)
@@ -693,7 +693,7 @@
 
       //checkAndPut with empty value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), -1L, put, lockId, true);
       assertTrue(res);
 
       //Putting data in key
@@ -702,25 +702,25 @@
 
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), -1L, put, lockId, true);
       assertTrue(res);
 
       // not empty anymore
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), -1L, put, lockId, true);
       assertFalse(res);
 
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), -1L, delete, lockId, true);
       assertFalse(res);
 
       put = new Put(row1);
       put.add(fam1, qf1, val2);
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertTrue(res);
 
       //checkAndDelete with correct value
@@ -728,12 +728,12 @@
       delete.deleteColumn(fam1, qf1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), -1L, delete, lockId, true);
       assertTrue(res);
 
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), -1L, delete, lockId, true);
       assertTrue(res);
 
       //checkAndPut looking for a null value
@@ -741,7 +741,7 @@
       put.add(fam1, qf1, val1);
 
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new NullComparator(), put, lockId, true);
+          new NullComparator(), -1L, put, lockId, true);
       assertTrue(res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -769,14 +769,14 @@
 
       //checkAndPut with wrong value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), put, lockId, true);
+          new BinaryComparator(val2), -1L, put, lockId, true);
       assertEquals(false, res);
 
       //checkAndDelete with wrong value
       Delete delete = new Delete(row1);
       delete.deleteFamily(fam1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), -1L, delete, lockId, true);
       assertEquals(false, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -803,14 +803,14 @@
 
       //checkAndPut with correct value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertEquals(true, res);
 
       //checkAndDelete with correct value
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertEquals(true, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -850,7 +850,7 @@
       store.memstore.kvset.size();
 
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertEquals(true, res);
       store.memstore.kvset.size();
 
@@ -877,7 +877,7 @@
       put.add(fam1, qual1, value1);
       try {
         boolean res = region.checkAndMutate(row, fam1, qual1, CompareOp.EQUAL,
-            new BinaryComparator(value2), put, null, false);
+            new BinaryComparator(value2), -1L, put, null, false);
         fail();
       } catch (DoNotRetryIOException expected) {
         // expected exception.
@@ -928,7 +928,7 @@
       delete.deleteColumn(fam2, qf1);
       delete.deleteColumn(fam1, qf3);
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), -1L, delete, lockId, true);
       assertEquals(true, res);
 
       Get get = new Get(row1);
@@ -944,7 +944,7 @@
       delete = new Delete(row1);
       delete.deleteFamily(fam2);
       res = region.checkAndMutate(row1, fam2, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), -1L, delete, lockId, true);
       assertEquals(true, res);
 
       get = new Get(row1);
@@ -955,7 +955,7 @@
       //Row delete
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), delete, lockId, true);
+          new BinaryComparator(val1), -1L, delete, lockId, true);
       assertEquals(true, res);
       get = new Get(row1);
       r = region.get(get, null);
Index: src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java	(revision 1383334)
+++ src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java	(working copy)
@@ -97,6 +97,17 @@
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import com.continuuity.hbase.ttqueue.HBQConfig;
+import com.continuuity.hbase.ttqueue.HBQConstants;
+import com.continuuity.hbase.ttqueue.HBQConsumer;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQExpirationConfig;
+import com.continuuity.hbase.ttqueue.HBQShardConfig;
+import com.continuuity.hbase.ttqueue.HBReadPointer;
+
 /**
  * Run tests that use the HBase clients; {@link HTable} and {@link HTablePool}.
  * Sets up the HBase mini cluster once at start and runs through all client tests.
@@ -4423,22 +4434,22 @@
     put1.add(FAMILY, QUALIFIER, VALUE);
 
     // row doesn't exist, so using non-null value should be considered "not match".
-    boolean ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, put1);
+    boolean ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, -1L, put1);
     assertEquals(ok, false);
 
     // row doesn't exist, so using "null" to check for existence should be considered "match".
-    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, put1);
+    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, -1L, put1);
     assertEquals(ok, true);
 
     // row now exists, so using "null" to check for existence should be considered "not match".
-    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, put1);
+    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, -1L, put1);
     assertEquals(ok, false);
 
     Put put2 = new Put(ROW);
     put2.add(FAMILY, QUALIFIER, value2);
 
     // row now exists, use the matching value to check
-    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, put2);
+    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, -1L, put2);
     assertEquals(ok, true);
 
     Put put3 = new Put(anotherrow);
@@ -4446,7 +4457,7 @@
 
     // try to do CheckAndPut on different rows
     try {
-        ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, value2, put3);
+        ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, value2, -1L, put3);
         fail("trying to check and modify different rows should have failed.");
     } catch(Exception e) {}
 
@@ -4769,5 +4780,42 @@
   @org.junit.Rule
   public org.apache.hadoop.hbase.ResourceCheckerJUnitRule cu =
     new org.apache.hadoop.hbase.ResourceCheckerJUnitRule();
+
+  // Continuuity tests
+
+  @Test
+  public void testHBQSimple() throws Exception {
+   final byte[] TABLENAME = Bytes.toBytes("testHBQSimple");
+   final byte[] FAMILY = HBQConstants.HBQ_FAMILY;
+   HColumnDescriptor hcd = new HColumnDescriptor(FAMILY);
+   HTableDescriptor desc = new HTableDescriptor(TABLENAME);
+   desc.addFamily(hcd);
+   TEST_UTIL.getHBaseAdmin().createTable(desc);
+   Configuration c = TEST_UTIL.getConfiguration();
+   HTable h = new HTable(c, TABLENAME);
+
+   HBReadPointer readPointer =
+       new HBReadPointer(System.currentTimeMillis(), Long.MAX_VALUE);
+
+   // do a simple enqueue and dequeue
+   byte [] queueName = Bytes.toBytes("simpleQueue");
+   byte [] queueData = Bytes.toBytes("someTestQueueData");
+   
+   HBQEnqueueResult enqueueResult = h.enqueue(new HBQEnqueue(queueName,
+       queueData, readPointer, new HBQShardConfig(10, 10)));
+   assertNotNull(enqueueResult);
+   assertTrue(enqueueResult.isSuccess());
+   assertEquals(1L, enqueueResult.getEntryPointer().getShardId());
+   assertEquals(1L, enqueueResult.getEntryPointer().getEntryId());
+   
+   HBQDequeueResult dequeueResult = h.dequeue(new HBQDequeue(queueName,
+       new HBQConsumer(1, 1, 1), new HBQConfig(true), readPointer,
+       new HBQExpirationConfig(10000L, 10000L)));
+   assertTrue("Expected success but got " + dequeueResult.toString(),
+       dequeueResult.isSuccess());
+   assertEquals(1L, dequeueResult.getEntryPointer().getShardId());
+   assertEquals(1L, dequeueResult.getEntryPointer().getEntryId());
+   assertTrue(Bytes.equals(dequeueResult.getData(), queueData));
+  }
 }
 
Index: src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/rest/RowResource.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/rest/RowResource.java	(working copy)
@@ -420,7 +420,7 @@
 
       table = pool.getTable(this.tableResource.getName());
       boolean retValue = table.checkAndPut(key, valueToPutParts[0],
-        valueToPutParts[1], valueToCheckCell.getValue(), put);
+        valueToPutParts[1], valueToCheckCell.getValue(), -1L, put);
       if (LOG.isDebugEnabled()) {
         LOG.debug("CHECK-AND-PUT " + put.toString() + ", returns " + retValue);
       }
@@ -482,7 +482,7 @@
 
       table = pool.getTable(tableResource.getName());
       boolean retValue = table.checkAndDelete(key, parts[0], parts[1],
-        valueToDeleteCell.getValue(), delete);
+        valueToDeleteCell.getValue(), -1, delete);
       if (LOG.isDebugEnabled()) {
         LOG.debug("CHECK-AND-DELETE " + delete.toString() + ", returns "
           + retValue);
Index: src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java	(working copy)
@@ -62,6 +62,18 @@
 import org.apache.hadoop.hbase.rest.model.TableSchemaModel;
 import org.apache.hadoop.hbase.util.Bytes;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 /**
  * HTable interface to remote tables accessed via REST gateway
  */
@@ -584,7 +596,7 @@
   }
 
   public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Put put) throws IOException {
+      byte[] value, long readVersion, Put put) throws IOException {
     // column to check-the-value
     put.add(new KeyValue(row, family, qualifier, value));
 
@@ -623,7 +635,7 @@
   }
 
   public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Delete delete) throws IOException {
+      byte[] value, long readVersion, Delete delete) throws IOException {
     Put put = new Put(row);
     // column to check-the-value
     put.add(new KeyValue(row, family, qualifier, value));
@@ -741,4 +753,48 @@
   public void setWriteBufferSize(long writeBufferSize) throws IOException {
     throw new IOException("setWriteBufferSize not supported");
   }
+
+  // Continuuity methods (unimplemented)
+
+  @Override
+  public HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException {
+    return null;
+  }
+
+  @Override
+  public HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException {
+    return null;
+  }
+
+  @Override
+  public HBQInvalidateResult invalidate(HBQInvalidate invalidate)
+      throws IOException {
+    return null;
+  }
+
+  @Override
+  public boolean ack(HBQAck ack) throws IOException {
+    return false;
+  }
+
+  @Override
+  public boolean unack(HBQUnack unack) throws IOException {
+    return false;
+  }
+
+  @Override
+  public boolean finalize(HBQFinalize finalize) throws IOException {
+    return false;
+  }
+
+  @Override
+  public long getGroupID(HBQMetaOperation operation) throws IOException {
+    return 0;
+  }
+
+  @Override
+  public HBQQueueMeta getQueueMeta(HBQMetaOperation operation)
+      throws IOException {
+    return null;
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java	(working copy)
@@ -225,7 +225,7 @@
     throws TIOError, TException {
     HTableInterface htable = getTable(table.array());
     try {
-      return htable.checkAndPut(row.array(), family.array(), qualifier.array(), (value == null) ? null : value.array(), putFromThrift(put));
+      return htable.checkAndPut(row.array(), family.array(), qualifier.array(), (value == null) ? null : value.array(), -1L, putFromThrift(put));
     } catch (IOException e) {
       throw getTIOError(e);
     } finally {
@@ -278,9 +278,9 @@
 
     try {
       if (value == null) {
-        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), null, deleteFromThrift(deleteSingle));
+        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), null, -1L, deleteFromThrift(deleteSingle));
       } else {
-        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), value.array(), deleteFromThrift(deleteSingle));
+        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), value.array(), -1L, deleteFromThrift(deleteSingle));
       }
     } catch (IOException e) {
       throw getTIOError(e);
Index: src/main/java/org/apache/hadoop/hbase/client/Delete.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/Delete.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/client/Delete.java	(working copy)
@@ -212,6 +212,17 @@
     return this;
   }
 
+  public Delete undeleteColumns(byte [] family, byte [] qualifier, long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if (list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(new KeyValue(this.row, family, qualifier, timestamp,
+      KeyValue.Type.UndeleteColumn));
+    familyMap.put(family, list);
+    return this;
+  }
+  
   /**
    * Delete the latest version of the specified column.
    * This is an expensive call in that on the server-side, it first does a
Index: src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java	(working copy)
@@ -30,6 +30,18 @@
 import org.apache.hadoop.hbase.client.coprocessor.Batch;
 import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 /**
  * Used to communicate with a single HBase table.
  *
@@ -223,7 +235,7 @@
    * @return true if the new put was executed, false otherwise
    */
   boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Put put) throws IOException;
+      byte[] value, long readVersion, Put put) throws IOException;
 
   /**
    * Deletes the specified cells/row.
@@ -261,7 +273,7 @@
    * @return true if the new delete was executed, false otherwise
    */
   boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Delete delete) throws IOException;
+      byte[] value, long readVersion, Delete delete) throws IOException;
 
   /**
    * Performs multiple mutations atomically on a single row. Currently
@@ -527,4 +539,22 @@
    * @throws IOException if a remote or network exception occurs.
    */
   public void setWriteBufferSize(long writeBufferSize) throws IOException;
+
+  // Continuuity Queue Methods
+
+  HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException;
+
+  HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException;
+
+  HBQInvalidateResult invalidate(HBQInvalidate invalidate) throws IOException;
+
+  boolean ack(HBQAck ack) throws IOException;
+
+  boolean unack(HBQUnack unack) throws IOException;
+
+  boolean finalize(HBQFinalize finalize) throws IOException;
+
+  long getGroupID(HBQMetaOperation operation) throws IOException;
+
+  HBQQueueMeta getQueueMeta(HBQMetaOperation operation) throws IOException;
 }
Index: src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/HTablePool.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/client/HTablePool.java	(working copy)
@@ -35,6 +35,18 @@
 import org.apache.hadoop.hbase.util.PoolMap;
 import org.apache.hadoop.hbase.util.PoolMap.PoolType;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 /**
  * A simple pool of HTable instances.
  * 
@@ -401,8 +413,8 @@
 
     @Override
     public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-        byte[] value, Put put) throws IOException {
-      return table.checkAndPut(row, family, qualifier, value, put);
+        byte[] value, long readVersion, Put put) throws IOException {
+      return table.checkAndPut(row, family, qualifier, value, readVersion, put);
     }
 
     @Override
@@ -417,8 +429,8 @@
 
     @Override
     public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-        byte[] value, Delete delete) throws IOException {
-      return table.checkAndDelete(row, family, qualifier, value, delete);
+        byte[] value, long readVersion, Delete delete) throws IOException {
+      return table.checkAndDelete(row, family, qualifier, value, readVersion, delete);
     }
 
     @Override
@@ -532,5 +544,49 @@
     public void setWriteBufferSize(long writeBufferSize) throws IOException {
       table.setWriteBufferSize(writeBufferSize);
     }
+
+    // Continuuity Methods (unimplemented)
+
+    @Override
+    public HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException {
+      return null;
+    }
+
+    @Override
+    public HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException {
+      return null;
+    }
+
+    @Override
+    public HBQInvalidateResult invalidate(HBQInvalidate invalidate)
+        throws IOException {
+      return null;
+    }
+
+    @Override
+    public boolean ack(HBQAck ack) throws IOException {
+      return false;
+    }
+
+    @Override
+    public boolean unack(HBQUnack unack) throws IOException {
+      return false;
+    }
+
+    @Override
+    public boolean finalize(HBQFinalize finalize) throws IOException {
+      return false;
+    }
+
+    @Override
+    public long getGroupID(HBQMetaOperation operation) throws IOException {
+      return 0;
+    }
+
+    @Override
+    public HBQQueueMeta getQueueMeta(HBQMetaOperation operation)
+        throws IOException {
+      return null;
+    }
   }
 }
Index: src/main/java/org/apache/hadoop/hbase/client/HTable.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/HTable.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/client/HTable.java	(working copy)
@@ -26,17 +26,15 @@
 import java.lang.reflect.Proxy;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Collections;
 import java.util.NavigableMap;
 import java.util.TreeMap;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.SynchronousQueue;
-import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -58,6 +56,18 @@
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Threads;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 /**
  * <p>Used to communicate with a single HBase table.
  *
@@ -865,12 +875,12 @@
   @Override
   public boolean checkAndPut(final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Put put)
+      final long readVersion, final Put put)
   throws IOException {
     return new ServerCallable<Boolean>(connection, tableName, row, operationTimeout) {
           public Boolean call() throws IOException {
             return server.checkAndPut(location.getRegionInfo().getRegionName(),
-                row, family, qualifier, value, put) ? Boolean.TRUE : Boolean.FALSE;
+                row, family, qualifier, value, readVersion, put) ? Boolean.TRUE : Boolean.FALSE;
           }
         }.withRetries();
   }
@@ -882,13 +892,13 @@
   @Override
   public boolean checkAndDelete(final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Delete delete)
+      final long readVersion, final Delete delete)
   throws IOException {
     return new ServerCallable<Boolean>(connection, tableName, row, operationTimeout) {
           public Boolean call() throws IOException {
             return server.checkAndDelete(
                 location.getRegionInfo().getRegionName(),
-                row, family, qualifier, value, delete)
+                row, family, qualifier, value, readVersion, delete)
             ? Boolean.TRUE : Boolean.FALSE;
           }
         }.withRetries();
@@ -1276,4 +1286,190 @@
     return operationTimeout;
   }
 
+  // Continuuity - HBase Native TTQueue Operations
+
+  @Override
+  public HBQEnqueueResult enqueue(final HBQEnqueue enqueue) throws IOException {
+    byte [] metaRow = enqueue.getMetaRow();
+    final HBQEnqueueResult resultOne = new ServerCallable<HBQEnqueueResult>(
+        connection, tableName, metaRow, operationTimeout) {
+          @Override
+          public HBQEnqueueResult call() throws IOException {
+            return server.enqueue(location.getRegionInfo().getRegionName(),
+                row, enqueue);
+          }
+        }.withRetries();
+    // if failure or data is written, operation is complete
+    if (!resultOne.isSuccess() || resultOne.getDataWritten()) return resultOne;
+    // need to continue operation on another server
+    byte [] shardRow = enqueue.getDataRow(
+        Bytes.toBytes(resultOne.getEntryPointer().getShardId() -
+            (resultOne.getInsertShardEnd() ? 1 : 0)));
+    HBQEnqueueResult resultTwo = new ServerCallable<HBQEnqueueResult>(
+        connection, tableName, shardRow, operationTimeout) {
+          @Override
+          public HBQEnqueueResult call() throws IOException {
+            return server.enqueue_data(location.getRegionInfo().getRegionName(),
+                row, enqueue, resultOne);
+          }
+        }.withRetries();
+    // if failure or data is written, operation is complete
+    if (!resultTwo.isSuccess() || resultTwo.getDataWritten()) return resultTwo;
+    // the only valid case for this should be that the first result returned a
+    // shard end and the second result didn't
+    if (!resultOne.getInsertShardEnd() || resultTwo.getInsertShardEnd()) {
+      LOG.warn("[HBQ] Fell through to third request but didn't have to write " +
+          "a shard end (resultOne=" + resultOne + ") " +
+          "(resultTwo=" + resultTwo + ")");
+      throw new IOException("Invalid state, three requests required without " +
+          "writing a shard end marker");
+    }
+    // finish operation on another server
+    shardRow = enqueue.getDataRow(
+        Bytes.toBytes(resultOne.getEntryPointer().getShardId()));
+    HBQEnqueueResult resultThree = new ServerCallable<HBQEnqueueResult>(
+        connection, tableName, shardRow, operationTimeout) {
+          @Override
+          public HBQEnqueueResult call() throws IOException {
+            return server.enqueue_data(location.getRegionInfo().getRegionName(),
+                row, enqueue, resultOne);
+          }
+        }.withRetries();
+    // regardless of the status of the return, it is the final result
+    return resultThree;
+  }
+
+  @Override
+  public HBQDequeueResult dequeue(final HBQDequeue dequeue) throws IOException {
+    byte [] metaRow = dequeue.getMetaRow();
+    HBQDequeueResult result = new ServerCallable<HBQDequeueResult>(
+        connection, tableName, metaRow, operationTimeout) {
+          @Override
+          public HBQDequeueResult call() throws IOException {
+            return server.dequeue(location.getRegionInfo().getRegionName(),
+                row, dequeue);
+          }
+        }.withRetries();
+    // TODO: determine if we should put a maximum on this loop
+    //       (doing so will change behavior by enforcing a max number of hops)
+    // Keep looping as long as the dequeue is still in progress
+    while (result.isInProgress()) {
+      // need to run dequeue_continue on next row (a shard row or group meta)
+      byte [] nextRow = result.getNextRow();
+      final HBQDequeueResult previousResult = result;
+      // Execute dequeue_continue on region server, passing previous result
+      result = new ServerCallable<HBQDequeueResult>(
+          connection, tableName, nextRow, operationTimeout) {
+            @Override
+            public HBQDequeueResult call() throws IOException {
+              return server.dequeue_continue(
+                  location.getRegionInfo().getRegionName(), row,
+                  previousResult);
+            }
+          }.withRetries();
+    }
+    // TODO: insert some logging and metrics to expose loop behavior
+    return result;
+  }
+
+  @Override
+  public HBQInvalidateResult invalidate(final HBQInvalidate invalidate)
+      throws IOException {
+    byte [] row = invalidate.getDataRow(
+        Bytes.toBytes(invalidate.getEntryPointer().getShardId()));
+    return new ServerCallable<HBQInvalidateResult>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public HBQInvalidateResult call() throws IOException {
+            return server.invalidate(location.getRegionInfo().getRegionName(),
+                row, invalidate);
+          }
+        }.withRetries();
+  }
+
+  @Override
+  public boolean ack(final HBQAck ack) throws IOException {
+    byte [] row = ack.getDataRow(
+        Bytes.toBytes(ack.getEntryPointer().getShardId()));
+    return new ServerCallable<Boolean>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public Boolean call() throws IOException {
+            return server.ack(location.getRegionInfo().getRegionName(), row,
+                ack)
+            ? Boolean.TRUE : Boolean.FALSE;
+          }
+        }.withRetries();
+  }
+
+  @Override
+  public boolean finalize(final HBQFinalize finalize) throws IOException {
+    byte [] row = finalize.getDataRow(
+        Bytes.toBytes(finalize.getEntryPointer().getShardId()));
+    return new ServerCallable<Boolean>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public Boolean call() throws IOException {
+            return server.finalize(location.getRegionInfo().getRegionName(), row,
+                finalize)
+            ? Boolean.TRUE : Boolean.FALSE;
+          }
+        }.withRetries();
+  }
+
+  @Override
+  public boolean unack(final HBQUnack unack) throws IOException {
+    byte [] row = unack.getDataRow(
+        Bytes.toBytes(unack.getEntryPointer().getShardId()));
+    return new ServerCallable<Boolean>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public Boolean call() throws IOException {
+            return server.unack(location.getRegionInfo().getRegionName(), row,
+                unack)
+            ? Boolean.TRUE : Boolean.FALSE;
+          }
+        }.withRetries();
+  }
+
+  /**
+   * Generates and returns a unique group id for this queue.
+   * 
+   * Note: uniqueness only guaranteed if you always use this call to generate
+   * groups ids.
+   * 
+   * @return a unique group id for this queue
+   */
+  @Override
+  public long getGroupID(final HBQMetaOperation operation) throws IOException {
+    byte [] row = operation.getMetaRow();
+    return new ServerCallable<Long>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public Long call() throws IOException {
+            return server.getGroupID(location.getRegionInfo().getRegionName(), row,
+                operation);
+          }
+        }.withRetries();
+  }
+  
+  /**
+   * Gets the meta information for this queue.  This includes all meta
+   * data available without walking the entire queue.
+   * @return global meta information for this queue and its groups
+   */
+  @Override
+  public HBQQueueMeta getQueueMeta(final HBQMetaOperation operation)
+      throws IOException {
+    byte [] row = operation.getMetaRow();
+    return new ServerCallable<HBQQueueMeta>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public HBQQueueMeta call() throws IOException {
+            return server.getQueueMeta(location.getRegionInfo().getRegionName(), row,
+                operation);
+          }
+        }.withRetries();
+  }
+
 }
Index: src/main/java/org/apache/hadoop/hbase/client/Increment.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/Increment.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/client/Increment.java	(working copy)
@@ -27,6 +27,7 @@
 import java.util.Set;
 import java.util.TreeMap;
 
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.Writable;
@@ -52,6 +53,7 @@
   private TimeRange tr = new TimeRange();
   private Map<byte [], NavigableMap<byte [], Long>> familyMap =
     new TreeMap<byte [], NavigableMap<byte [], Long>>(Bytes.BYTES_COMPARATOR);
+  private long writeVersion = HConstants.LATEST_TIMESTAMP;
 
   /** Constructor for Writable.  DO NOT USE */
   public Increment() {}
@@ -101,6 +103,24 @@
     return this;
   }
 
+  /**
+   * Sets the write version to use for this increment.
+   * @param writeVersion
+   * @return this Increment object
+   */
+  public Increment setWriteVersion(long writeVersion) {
+    this.writeVersion = writeVersion;
+    return this;
+  }
+  
+  /**
+   * Returns the write version to use for this increment.
+   * @return the write version to use
+   */
+  public long getWriteVersion() {
+    return this.writeVersion;
+  }
+  
   /* Accessors */
 
   /**
@@ -274,6 +294,7 @@
     this.tr = new TimeRange();
     tr.readFields(in);
     this.lockId = in.readLong();
+    this.writeVersion = in.readLong();
     int numFamilies = in.readInt();
     if (numFamilies == 0) {
       throw new IOException("At least one column required");
@@ -307,6 +328,7 @@
     Bytes.writeByteArray(out, this.row);
     tr.write(out);
     out.writeLong(this.lockId);
+    out.writeLong(this.writeVersion);
     if (familyMap.size() == 0) {
       throw new IOException("At least one column required");
     }
Index: src/main/java/org/apache/hadoop/hbase/regionserver/metrics/OperationMetrics.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/metrics/OperationMetrics.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/metrics/OperationMetrics.java	(working copy)
@@ -30,6 +30,8 @@
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 
+import com.continuuity.hbase.ttqueue.HBQOperation.HBQOperationType;
+
 /**
  * This class provides a simplified interface to expose time varying metrics
  * about GET/DELETE/PUT/ICV operations on a region and on Column Families. All
@@ -220,4 +222,7 @@
     }
   }
 
+  public void updateQueueMetrics(HBQOperationType type, long time) {
+    doSafeIncTimeVarying("hbq_", type.name().toLowerCase(), time);
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(working copy)
@@ -78,7 +78,6 @@
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.UnknownScannerException;
 import org.apache.hadoop.hbase.client.Append;
-import org.apache.hadoop.hbase.client.RowMutations;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.Increment;
@@ -88,6 +87,7 @@
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Row;
 import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.client.RowMutations;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.coprocessor.Exec;
 import org.apache.hadoop.hbase.client.coprocessor.ExecResult;
@@ -107,6 +107,7 @@
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
 import org.apache.hadoop.hbase.regionserver.metrics.OperationMetrics;
+import org.apache.hadoop.hbase.regionserver.metrics.RegionMetricsStorage;
 import org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
@@ -126,6 +127,30 @@
 import org.apache.hadoop.util.StringUtils;
 import org.cliffc.high_scale_lib.Counter;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQConfig;
+import com.continuuity.hbase.ttqueue.HBQConstants;
+import com.continuuity.hbase.ttqueue.HBQConsumer;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQEntryPointer;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQOperation.HBQOperationType;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+import com.continuuity.hbase.ttqueue.HBReadPointer;
+import com.continuuity.hbase.ttqueue.internal.EntryGroupMeta;
+import com.continuuity.hbase.ttqueue.internal.EntryGroupMeta.EntryGroupState;
+import com.continuuity.hbase.ttqueue.internal.EntryMeta;
+import com.continuuity.hbase.ttqueue.internal.EntryMeta.EntryState;
+import com.continuuity.hbase.ttqueue.internal.ExecutionMode;
+import com.continuuity.hbase.ttqueue.internal.GroupState;
+import com.continuuity.hbase.ttqueue.internal.ShardMeta;
 import com.google.common.base.Preconditions;
 import com.google.common.collect.ClassToInstanceMap;
 import com.google.common.collect.ImmutableList;
@@ -1921,6 +1946,7 @@
    * @throws IOException
    * @deprecated Instead use {@link HRegion#batchMutate(Pair[])}
    */
+  @SuppressWarnings("unchecked")
   @Deprecated
   public OperationStatus[] put(Pair<Put, Integer>[] putsAndLocks) throws IOException {
     Pair<Mutation, Integer>[] mutationsAndLocks = new Pair[putsAndLocks.length];
@@ -2306,14 +2332,15 @@
    * @param qualifier
    * @param compareOp
    * @param comparator
+   * @param readVersion 
    * @param lockId
    * @param writeToWAL
    * @throws IOException
    * @return true if the new put was execute, false otherwise
    */
   public boolean checkAndMutate(byte [] row, byte [] family, byte [] qualifier,
-      CompareOp compareOp, WritableByteArrayComparable comparator, Writable w,
-      Integer lockId, boolean writeToWAL)
+      CompareOp compareOp, WritableByteArrayComparable comparator,
+      long readVersion, Writable w, Integer lockId, boolean writeToWAL)
   throws IOException{
     checkReadOnly();
     //TODO, add check for value length or maybe even better move this to the
@@ -2334,6 +2361,8 @@
       Get get = new Get(row, lock);
       checkFamily(family);
       get.addColumn(family, qualifier);
+      get.setTimeRange(0, readVersion <= 0 || readVersion == Long.MAX_VALUE ?
+          Long.MAX_VALUE : readVersion + 1);
 
       // Lock row
       Integer lid = getLock(lockId, get.getRow(), true);
@@ -4563,7 +4592,9 @@
       Integer lid = getLock(lockid, row, true);
       this.updatesLock.readLock().lock();
       try {
-        long now = EnvironmentEdgeManager.currentTimeMillis();
+        long now = increment.getWriteVersion() == HConstants.LATEST_TIMESTAMP ?
+            EnvironmentEdgeManager.currentTimeMillis() :
+              increment.getWriteVersion();
         // Process each family
         for (Map.Entry<byte [], NavigableMap<byte [], Long>> family :
           increment.getFamilyMap().entrySet()) {
@@ -5226,4 +5257,1696 @@
        if (bc != null) bc.shutdown();
      }
   }
+
+  // Continuuity Queue Implementation
+
+  private static final String HBQ_METRIC_PREFIX = "hbq_hregion_";
+
+  private static final long DIRTY_WRITE = 1L;
+
+  private static final long DIRTY_READ = Long.MAX_VALUE;
+
+  public HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException {
+    byte [] metaRow = enqueue.getMetaRow();
+    checkRow(metaRow, "enqueue");
+    boolean flush = false;
+    boolean metaEnqueued = false;
+    ShardMeta shardMeta = null;
+    boolean moveShard = false;
+    long entryId = -1L;
+    boolean writeToWAL = enqueue.getWriteToWAL();
+    byte [] data = enqueue.getQueueData();
+    WALEdit walEdits = new WALEdit();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    HBQEnqueueResult result = null;
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "enqueue_count", 1);
+
+    // Lock row
+    startRegionOperation();
+    try {
+      Integer metaRowLock = getLock(null, metaRow, true);
+      this.updatesLock.readLock().lock();
+      try {
+
+        long ts_now = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_lock", ts_now - ts_start);
+
+        // Get a unique entry id using existing meta row lock
+        entryId = queueIncrement_existingLock(metaRowLock, metaRow,
+            HBQConstants.GLOBAL_ENTRYID_COUNTER, DIRTY_WRITE, writeToWAL,
+            walEdits, size);
+
+        long ts_entryIdInc = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_entryid_incr", ts_entryIdInc - ts_now);
+
+        // Determine shard state (initialize or read existing)
+        if (entryId == 1) {
+          // This is the first entry in the queue, initialize
+          shardMeta = new ShardMeta(1, data.length, 1);
+        } else {
+          // Read existing, determine updated shard state
+          shardMeta = ShardMeta.fromBytes(queueGet_noLock(metaRow,
+              HBQConstants.GLOBAL_SHARD_META, DIRTY_READ));
+          // Check if we need to move to next shard (max bytes or max entries)
+          if ((shardMeta.getShardBytes() + data.length >
+          enqueue.getShardConfig().getMaxBytesPerShard() &&
+          shardMeta.getShardEntries() > 1) ||
+          shardMeta.getShardEntries() ==
+          enqueue.getShardConfig().getMaxEntriesPerShard()) {
+            // Move to next shard
+            moveShard = true;
+            shardMeta = new ShardMeta(shardMeta.getShardId() + 1,
+                data.length, 1);
+          } else {
+            // Update current shard sizing
+            shardMeta = new ShardMeta(shardMeta.getShardId(),
+                shardMeta.getShardBytes() + data.length,
+                shardMeta.getShardEntries() + 1);
+          }
+        }
+
+        long ts_shardMetaRead = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_shardmeta_read",
+            ts_shardMetaRead - ts_entryIdInc);
+
+        // Write updated shard meta
+        queuePut_existingLock(metaRowLock, metaRow,
+            HBQConstants.GLOBAL_SHARD_META, shardMeta.getBytes(), DIRTY_WRITE,
+            writeToWAL, walEdits, size);
+
+        long ts_shardMetaWrite = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_shardmeta_write",
+            ts_shardMetaWrite - ts_shardMetaRead);
+
+        // Done with processing on global meta row
+        // Finalize and move to data rows
+
+        // Write out to WAL if enabled
+        long ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+        if (writeToWAL) {
+          txid = this.log.appendNoSync(this.regionInfo,
+              this.htableDescriptor.getName(), walEdits,
+              HConstants.DEFAULT_CLUSTER_ID,
+              EnvironmentEdgeManager.currentTimeMillis(),
+              this.htableDescriptor);
+          ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+          RegionMetricsStorage.incrTimeVaryingMetric(
+              HBQ_METRIC_PREFIX + "enqueue_wal_append",
+              ts_walAppend - ts_shardMetaWrite);
+        }
+        metaEnqueued = true;
+      } finally {
+        // Release meta row lock
+        this.updatesLock.readLock().unlock();
+        releaseRowLock(metaRowLock);
+      }
+      long ts_unlock = EnvironmentEdgeManager.currentTimeMillis();
+
+      // Only proceed if we finished with meta enqueue operations
+      if (!metaEnqueued) {
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "enqueue_meta_errors", 1);
+        return new HBQEnqueueResult("Error performing meta operations");
+      }
+      boolean rowMiss = false;
+
+      // If we moved shards, first we need to update previous data shard
+      if (moveShard) {
+        byte [] previousShardRow = enqueue.getDataRow(
+            Bytes.toBytes(shardMeta.getShardId() - 1));
+        if (rowIsInRange(this.regionInfo, previousShardRow)) {
+          // Row of previous data shard is in this region
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_same", 1);
+          TxidAndSize internalResult =
+              enqueue_data_internal(previousShardRow, entryId, writeToWAL,
+                  enqueue.getReadPointer().getWritePointer(),
+                  new EntryMeta(EntryMeta.EntryState.SHARD_END),
+                  new EntryMeta(EntryMeta.EntryState.SHARD_END).getBytes());
+          txid = internalResult.updateTxid(txid);
+          size = internalResult.updateSize(size);
+          long ts_postShardEnd = EnvironmentEdgeManager.currentTimeMillis();
+          RegionMetricsStorage.incrTimeVaryingMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_end_write",
+              ts_postShardEnd - ts_unlock);
+          ts_unlock = ts_postShardEnd;
+        } else {
+          // Row of previous data shard is NOT in this region
+          rowMiss = true;
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_diff", 1);
+          result = new HBQEnqueueResult(
+              new HBQEntryPointer(entryId, shardMeta.getShardId()), true);
+        }
+      }
+
+      // If we haven't missed a row yet, attempt to enqueue in this region
+      if (!rowMiss) {
+        byte [] shardRow = enqueue.getDataRow(
+            Bytes.toBytes(shardMeta.getShardId()));
+        if (rowIsInRange(this.regionInfo, shardRow)) {
+          // Row of data shard is in this region
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_same", 1);
+          TxidAndSize internalResult =
+              enqueue_data_internal(shardRow, entryId, writeToWAL,
+                  enqueue.getReadPointer().getWritePointer(),
+                  new EntryMeta(EntryMeta.EntryState.VALID),
+                  enqueue.getQueueData());
+          txid = internalResult.updateTxid(txid);
+          size = internalResult.updateSize(size);
+          result = new HBQEnqueueResult(
+              new HBQEntryPointer(entryId, shardMeta.getShardId()));
+        } else {
+          // Row of previous data shard is NOT in this region
+          rowMiss = true;
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_diff", 1);
+          result = new HBQEnqueueResult(
+              new HBQEntryPointer(entryId, shardMeta.getShardId()), false);
+        }
+      }
+      // Data block write was done
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "enqueue_data", ts_preSync - ts_unlock);
+
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_walsync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.ENQUEUE,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+
+    return result;
+  }
+
+  private static class TxidAndSize {
+    long txid;
+    long size;
+    TxidAndSize(long txid, long size) {
+      this.txid = txid;
+      this.size = size;
+    }
+    public TxidAndSize() {
+      this(0L, 0L);
+    }
+    long updateTxid(long currentTxid) {
+      if (this.txid > currentTxid) return this.txid;
+      return currentTxid;
+    }
+    public long updateSize(long currentSize) {
+      return currentSize + this.size;
+    }
+    public AtomicLong updateSize(AtomicLong currentSize) {
+      currentSize.addAndGet(this.size);
+      return currentSize;
+    }
+  }
+
+  public HBQEnqueueResult enqueue_data(HBQEnqueue enqueue,
+      HBQEnqueueResult previousResult)
+          throws IOException {
+    // TODO: Add metrics and logging
+    HBQEntryPointer entryPointer = previousResult.getEntryPointer();
+    byte [] shardRow = enqueue.getDataRow(
+        Bytes.toBytes(previousResult.getEntryPointer().getShardId() -
+            (previousResult.getInsertShardEnd() ? 1 : 0)));
+    checkRow(shardRow, "enqueue_data");
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    long size = 0;
+    long txid = 0;
+    boolean flush = false;
+    HBQEnqueueResult result = null;
+
+    startRegionOperation();
+    try {
+
+      // If we moved shards, first we need to update previous data shard
+      if (previousResult.getInsertShardEnd()) {
+        TxidAndSize internalResult =
+            enqueue_data_internal(shardRow, entryPointer.getEntryId(),
+                enqueue.getWriteToWAL(),
+                enqueue.getReadPointer().getWritePointer(),
+                new EntryMeta(EntryMeta.EntryState.SHARD_END),
+                new EntryMeta(EntryMeta.EntryState.SHARD_END).getBytes());
+        txid = internalResult.updateTxid(txid);
+        size = internalResult.updateSize(size);
+      }
+
+      // Attempt to enqueue in this region
+      shardRow = enqueue.getDataRow(Bytes.toBytes(entryPointer.getShardId()));
+      if (rowIsInRange(this.regionInfo, shardRow)) {
+        if (previousResult.getInsertShardEnd()) {
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_same", 1);
+        }
+        TxidAndSize internalResult =
+            enqueue_data_internal(shardRow, entryPointer.getEntryId(),
+                enqueue.getWriteToWAL(), enqueue.getReadPointer().getWritePointer(),
+                new EntryMeta(EntryMeta.EntryState.VALID),
+                enqueue.getQueueData());
+        txid = internalResult.updateTxid(txid);
+        size = internalResult.updateSize(size);
+        result = new HBQEnqueueResult(entryPointer);
+      } else {
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "enqueue_shard_region_diff", 1);
+        result = new HBQEnqueueResult(entryPointer, false);
+      }
+
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (enqueue.getWriteToWAL() && txid > 0) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_data_walsync",
+            ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.ENQUEUE,
+        ts_end - ts_start);
+
+    if (flush) {
+      requestFlush();
+    }
+    return result;
+  }
+
+  private TxidAndSize enqueue_data_internal(byte[] shardRow,
+      long entryId, boolean writeToWAL, long writePointer, EntryMeta entryMeta,
+      byte[] queueData)
+          throws IOException {
+    checkRow(shardRow, "enqueue_data_internal");
+    WALEdit walEdits = writeToWAL ? new WALEdit() : null;
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+
+    // Lock data row
+    Integer shardRowLock = getLock(null, shardRow, true);
+    this.updatesLock.readLock().lock();
+    try {
+
+      long ts_now = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "enqueue_data_lock", ts_now - ts_start);
+
+      if (entryMeta.isEndOfShard()) {
+        // Write end of shard meta only
+        queuePut_existingLock(shardRowLock, shardRow, dataMetaColumn(entryId),
+            entryMeta.getBytes(), writePointer,writeToWAL, walEdits, size);
+      } else {
+        // Write shard meta and actual data
+        queuePut_existingLock(shardRowLock, shardRow,
+            new byte [][] { dataMetaColumn(entryId), dataDataColumn(entryId) },
+            new byte [][] { entryMeta.getBytes(), queueData }, writePointer,
+            writeToWAL, walEdits, size);
+      }
+
+      long ts_dataWrite = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "enqueue_data_write",
+          ts_dataWrite - ts_now);
+
+      // Write out to WAL if enabled
+      long ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        txid = this.log.appendNoSync(this.regionInfo,
+            this.htableDescriptor.getName(), walEdits,
+            HConstants.DEFAULT_CLUSTER_ID,
+            EnvironmentEdgeManager.currentTimeMillis(),
+            this.htableDescriptor);
+        ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_data_wal_append",
+            ts_walAppend - ts_dataWrite);
+      }
+    } finally {
+      // Release meta row lock
+      this.updatesLock.readLock().unlock();
+      releaseRowLock(shardRowLock);
+    }
+    return new TxidAndSize(txid, size.get());
+  }
+
+  /**
+   * Performs a dequeue operation
+   * @param dequeue
+   * @return
+   * @throws IOException
+   */
+  public HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException {
+    byte [] groupMetaRow = dequeue.getGroupMetaRow();
+    checkRow(groupMetaRow, "dequeue");
+    boolean flush = false;
+    boolean writeToWAL = dequeue.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    long size = 0;
+    long txid = 0;
+    HBQConsumer consumer = dequeue.getConsumer();
+    HBQConfig config = dequeue.getConfig();
+    HBQDequeueResult result = null;
+    GroupState groupState = null;
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "dequeue_count", 1);
+
+    // Lock region
+    startRegionOperation();
+    try {
+      // Need to determine the group state for the dequeueing group
+      byte [] groupStateColumn = HBQConstants.GROUP_STATE;
+
+      // Before locking, be optimistic and do a dirty read of group meta
+      // to see if it already matches the current consumers configuration
+      byte [] existingValue = queueGet_noLock(groupMetaRow, groupStateColumn,
+          DIRTY_READ);
+
+      if (existingValue == null) {
+        // Group meta does not exist, initialize with new group state
+        groupState = new GroupState(consumer.getGroupSize(),
+            new HBQEntryPointer(1, 1), ExecutionMode.fromQueueConfig(config));
+        // TODO: For now, this will throw an exception if the compare-and-swap
+        //       fails and the value that appears is not the state we are
+        //       trying to change it to
+        TxidAndSize txidAndSize = updateGroupState(dequeue, null, groupState,
+            writeToWAL);
+        txid = txidAndSize.updateTxid(txid);
+        size = txidAndSize.updateSize(size);
+      } else {
+        // Group meta exists, see if it's what we expect it to be
+        groupState = GroupState.fromBytes(existingValue);
+        if (groupState.getGroupSize() != consumer.getGroupSize() ||
+            groupState.getMode() != ExecutionMode.fromQueueConfig(config)) {
+          // Group configuration has changed!
+
+          // Need to see if there are pending entries.  Group configuration
+          // changes are only permitted when there are no queue entries
+          // currently dequeued and un-acked.
+
+          if (groupHasPendingEntries(groupState, consumer.getGroupId())) {
+            // Cannot reconfig if there are pending entries, fail
+            String msg = "Attempted to change queue group configuration but " +
+                "group has pending entries not acked";
+            LOG.warn(msg);
+            return new HBQDequeueResult(msg);
+          }
+
+          // No pending entries, attempt to update group state
+          GroupState oldGroupState = groupState;
+          groupState = new GroupState(consumer.getGroupSize(),
+              groupState.getHead(), ExecutionMode.fromQueueConfig(config));
+          // TODO: This will also throw an exception if there are multiple
+          //       reconfigurations concurrently that conflict with each other
+          TxidAndSize txidAndSize =
+              updateGroupState(dequeue, oldGroupState, groupState, writeToWAL);
+          txid = txidAndSize.updateTxid(txid);
+          size = txidAndSize.updateSize(size);
+        }
+        // Group size and execution mode are the same so no changes needed
+      }
+      // GroupState is ready to go
+
+      // Starting from group head pointer, iterate through entries and shards
+      // to find the first available entry for the dequeueing consumer, or
+      // reach the end of the queue and return as empty
+
+      Pair<HBQDequeueResult, TxidAndSize> resultAndStats =
+          dequeue_data_internal(dequeue, groupState, groupState.getHead(),
+              writeToWAL);
+      result = resultAndStats.getFirst();
+
+      TxidAndSize txidAndSize = resultAndStats.getSecond();
+      txidAndSize.updateSize(size);
+      txidAndSize.updateTxid(txid);
+
+      // Check if group state needs to be updated (row is in this region so
+      // might as well just do it now, whether or not dequeue is done)
+      if (result.groupStateNeedsToBeUpdated()) {
+        txidAndSize = updateGroupState(dequeue, result.getCurrentGroupState(),
+            writeToWAL);
+        txidAndSize.updateSize(size);
+        txidAndSize.updateTxid(txid);
+        // TODO: should the update return the current group state?  would be
+        //       more optimal to always have the most up-to-date pointer
+        result.groupStateUpdated();
+      }
+
+      // Sync the WAL to highest txid of wal edits written in this dequeue
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL && txid > 0) {
+        this.log.sync(txid); // sync the transaction log up to highest txid
+        long ts_sync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "dequeue_walsync",
+            ts_sync - ts_preSync);
+      }
+
+      // Check if we need to flush the region
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.DEQUEUE,
+        ts_end - ts_start);
+
+    // TODO: Insert statistical metrics around whether data was found in this
+    //       request, group was updated, more hops necessary, etc
+
+    if (flush) {
+      requestFlush();
+    }
+    return result;
+  }
+
+  /**
+   * 
+   * @param previousResult
+   * @return
+   * @throws IOException
+   */
+  public HBQDequeueResult dequeue_continue(HBQDequeueResult previousResult)
+      throws IOException {
+    HBQDequeue dequeue = previousResult.getDequeue();
+    HBQDequeueResult result = null;
+    boolean writeToWAL = dequeue.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    long size = 0;
+    long txid = 0;
+    boolean flush = false;
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "dequeue_count", 1);
+
+    // Only in-progress dequeues should be called here
+    if (!previousResult.isInProgress()) {
+      throw new IOException("Received not-in-progress dequeue in " +
+          "dequeue_continue operation");
+    }
+
+    // Lock region
+    startRegionOperation();
+    try {
+
+      // Check if data already found and this is just a group state update
+      if (previousResult.dataEntrySearchDone()) {
+
+        // The data entry has already been found, this request is just to
+        // update the group state which is expected to be on this region
+
+        // Required to need a group state update, otherwise invalid request
+        if (!previousResult.groupStateNeedsToBeUpdated()) {
+          throw new IOException("Received in-progress dequeue with data entry " +
+              "found but group does not need to be updated, invalid state");
+        }
+
+        // Verify the group state row is on this region
+        byte [] groupMetaRow = dequeue.getGroupMetaRow();
+        checkRow(groupMetaRow, "dequeue_continue");
+
+        // Update the group state
+        TxidAndSize txidAndSize = updateGroupState(dequeue,
+            previousResult.getCurrentGroupState(), writeToWAL);
+        txidAndSize.updateSize(size);
+        txidAndSize.updateTxid(txid);
+        result = previousResult;
+        result.groupStateUpdated();
+
+      } else {
+
+        // Request is to continue the search for a queue data entry
+
+        // Starting from current pointer in previous result, iterate through
+        // entries and shards to find the first available entry for the
+        // dequeueing consumer, or reach the end of the queue and return as
+        // empty
+
+        // The current group state for this dequeue call will be the current
+        // state from any prior processing
+        GroupState currentGroupState = previousResult.getCurrentGroupState();
+
+        // Perform dequeue, following all shards that fall within this region
+        Pair<HBQDequeueResult, TxidAndSize> resultAndStats =
+            dequeue_data_internal(dequeue, currentGroupState,
+                previousResult.getCurrentEntryPointer(), writeToWAL);
+
+        result = resultAndStats.getFirst();
+        TxidAndSize txidAndSize = resultAndStats.getSecond();
+        txidAndSize.updateSize(size);
+        txidAndSize.updateTxid(txid);
+
+        // Check if group state needs to be updated and the group meta row
+        // is on this region, and if so, update group state here
+        if (result.groupStateNeedsToBeUpdated() &&
+            rowIsInRange(this.regionInfo, dequeue.getGroupMetaRow())) {
+          txidAndSize = updateGroupState(dequeue, result.getCurrentGroupState(),
+              writeToWAL);
+          txidAndSize.updateSize(size);
+          txidAndSize.updateTxid(txid);
+          // TODO: should the update return the current group state?  would be
+          //       more optimal to always have the most up-to-date pointer
+          result.groupStateUpdated();
+        }
+
+      }
+
+      // Sync the WAL to highest txid of wal edits written, if enabled
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL && txid > 0) {
+        this.log.sync(txid); // sync the transaction log up to highest txid
+        long ts_sync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "dequeue_continue_walsync",
+            ts_sync - ts_preSync);
+      }
+
+      // Check if we need to flush the region
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.DEQUEUE,
+        ts_end - ts_start);
+
+    // TODO: Insert statistical metrics around whether data was found in this
+    //       request, group was updated, more hops necessary, etc
+
+    if (flush) {
+      requestFlush();
+    }
+    return result;
+  }
+
+  /**
+   * Attempts a dequeue given the specified dequeue operation and previously
+   * determined group state.
+   * 
+   * Starting from the specified entry pointer, iterates through entries and
+   * shards to find the first available entry for the dequeueing consumer, or
+   * until it reaches the end of the queue and return as empty.
+   * 
+   * This method also determines if the group head pointer needs to be updated,
+   * and if so, will store the final group state in the result, and the caller
+   * will be responsible for performing the update.  It is impossible
+   * for this update to hard-fail (if it does, there is a bug or invalid
+   * behavior by consumers).
+   * 
+   * This method should be called while holding the overall region operation
+   * lock, but no other locks (row lock, updates lock).  Row locks will be
+   * acquired within this method on shard rows that are written to.
+   * 
+   * All writes (marking of the data block meta) will be performed according to
+   * the specified write-to-wal flag and written to the wal under a row lock and
+   * with append-no-sync.  The txid and aggregate size of writes will be
+   * returned.  It is the responsibility of the caller to synchronize and check
+   * if a region flush should be performed.
+   * 
+   * @param dequeue
+   * @param groupState
+   * @param groupHeadPointer
+   * @return
+   * @throws IOException
+   */
+  private Pair<HBQDequeueResult, TxidAndSize> dequeue_data_internal(
+      HBQDequeue dequeue, GroupState currentGroupState,
+      HBQEntryPointer currentEntryPointer, boolean writeToWAL)
+          throws IOException {
+    HBQConsumer consumer = dequeue.getConsumer();
+    HBQConfig config = dequeue.getConfig();
+    GroupState initialGroupState = currentGroupState;
+    boolean skippedEntry = false;
+
+    EntryMeta entryMeta = null;
+    EntryGroupMeta entryGroupMeta = null;
+    HBQEntryPointer entryPointer = currentEntryPointer;
+    long previousShardId = -1;
+    // TODO: Should there be a max retry on here?  This would effectively
+    //       change behavior and cap the maximum number of unacked entries
+    //       (but would you return empty or some other code like "full/max"?)
+    while (entryPointer != null) {
+
+      // We are pointed at {entryPointer}=(shardid,entryid) and we are either
+      // at the head of this group or we have skipped everything between where
+      // we are and the head.
+
+      byte [] shardRow = dequeue.getDataRow(
+          Bytes.toBytes(entryPointer.getShardId()));
+
+      if (previousShardId != entryPointer.getShardId()) {
+        // Moved to a new shard
+
+        // Check if shard is in the same region we are currently in
+        if (!rowIsInRange(this.regionInfo, shardRow)) {
+          // Row of the current shard is not on this region, need to return
+          // back to region server to continue dequeue
+          return new Pair<HBQDequeueResult, TxidAndSize>(
+              new HBQDequeueResult(dequeue, entryPointer, initialGroupState,
+                  currentGroupState),
+                  new TxidAndSize());
+        }
+      }
+
+      byte [] entryMetaColumn = dataMetaColumn(entryPointer.getEntryId());
+      byte [] entryDataColumn = dataDataColumn(entryPointer.getEntryId());
+
+      // Do a clean read of the current entry meta data
+      byte [] entryMetaBytes = queueGet_noLock(shardRow, entryMetaColumn,
+          dequeue.getPointer());
+
+      if (entryMetaBytes == null) {
+        // This entry doesn't exist or is not visible
+        // so queue is empty for this consumer
+
+        // Note, this is a slight change in behavior from the existing TTQ
+        // implementation which would return a RETRY if it found uncommitted
+        // data rather than just returning EMPTY now which seems like correct
+        // behavior
+
+        // Return empty with both group states (constructor detects if change)
+        return new Pair<HBQDequeueResult, TxidAndSize>(
+            new HBQDequeueResult(dequeue, null, null, initialGroupState,
+                currentGroupState),
+                new TxidAndSize());
+      }
+
+      // Queue entry exists and is visible, check the global state of it
+      entryMeta = EntryMeta.fromBytes(entryMetaBytes);
+
+      // Check if this entry is not a valid data entry
+      if (!entryMeta.isValid()) {
+        // Entry is invalid in some way.  We will skip it and move to next.
+        HBQEntryPointer nextEntryPointer = null;
+
+        if (entryMeta.isEndOfShard()) {
+          // If entry is an end-of-shard marker, move to next shard
+          nextEntryPointer = nextShard(entryPointer);
+        } else {
+          // Otherwise for invalidated/evicted, just move to next entry
+          nextEntryPointer = nextEntry(entryPointer);
+        }
+
+        // Before moving to next entry, check if group state can be updated
+        currentGroupState = attemptGroupPointerUpdate(skippedEntry,
+            currentGroupState, nextEntryPointer);
+
+        // Move on to the next entry
+        entryPointer = nextEntryPointer;
+        continue;
+      }
+
+      // Entry is visible and valid
+
+      // Do a dirty read of entry group meta data
+      byte [] entryGroupMetaColumn = dataGroupMetaColumn(
+          entryPointer.getEntryId(), consumer.getGroupId());
+      byte [] entryGroupMetaBytes = queueGet_noLock(shardRow,
+          entryGroupMetaColumn, DIRTY_READ);
+
+      if (entryGroupMetaBytes == null) {
+        // Group has not processed entry, consider available
+        entryGroupMeta = null;
+
+      } else {
+        // Group has already processed entry, check status
+        entryGroupMeta = EntryGroupMeta.fromBytes(entryGroupMetaBytes);
+
+        HBQEntryPointer nextEntryPointer = null;
+
+        // First check if entry is already acked/semi-acked (can be skipped)
+
+        if (entryGroupMeta.isAckedOrSemiAcked()) {
+
+          // Group has already acked this entry, move to next entry
+          nextEntryPointer = nextEntry(entryPointer);
+
+          // Before moving to next entry, if acked or semi-acked+timed_out,
+          // check for group state update
+          if (ackedOrTimedOutSemiAcked(entryGroupMeta,
+              dequeue.getExpirationConfig().getMaxAgeBeforeSemiAckedToAcked()))
+          {
+            currentGroupState = attemptGroupPointerUpdate(skippedEntry,
+                currentGroupState, nextEntryPointer);
+          } else {
+            // There is a semi-acked entry, we cannot safely move head pointer
+            // at this point or beyond
+            skippedEntry = true;
+          }
+
+          // Entry is acked, moved to next
+          entryPointer = nextEntryPointer;
+          continue;
+        }
+
+        // Second check if entry is already dequeued by current group
+        // (skipped if not timed out and either a different consumer or in
+        // multi-entry mode, or returned if same consumer and single entry mode
+        // or if timed out)
+
+        if (entryGroupMeta.isDequeued()) {
+
+          // Check if current consumer is in single entry mode and is the
+          // instance that already dequeued this entry
+          if (config.isSingleEntry() &&
+              entryGroupMeta.getInstanceId() == consumer.getInstanceId()) {
+            // Same consumer, give back the already dequeued but not acked entry
+            TxidAndSize txidAndSize = attemptToClaimEntry(dequeue, entryPointer,
+                entryGroupMeta, new EntryGroupMeta(EntryGroupState.DEQUEUED,
+                    now(), consumer.getInstanceId()));
+            if (txidAndSize == null) {
+              // Failed to claim entry, another consumer won,
+              // skip and move to next entry
+              skippedEntry = true;
+              entryPointer = nextEntry(entryPointer);
+              continue;
+            } else {
+              // Entry claimed.  Read and return entry.
+              byte [] queueData = queueGet_noLock(shardRow,
+                  dataDataColumn(entryPointer.getEntryId()), DIRTY_READ);
+              return new Pair<HBQDequeueResult, TxidAndSize>(
+                  new HBQDequeueResult(dequeue, entryPointer, queueData,
+                      initialGroupState, currentGroupState), txidAndSize);
+            }
+          }
+
+          // We should move to the next entry unless the dequeue is timed out
+          if (entryGroupMeta.getTimestamp() +
+              dequeue.getExpirationConfig().getMaxAgeBeforeExpirationInMillis()
+              >= now()) {
+            // Dequeue is not timed out, skip and move to next entry
+            skippedEntry = true;
+            entryPointer = nextEntry(entryPointer);
+            continue;
+          }
+
+          // Dequeued and timed-out, treat as available
+        }
+
+        // If we get here, the entry is available
+      }
+
+      // Entry is available for this consumer and group
+
+      // Do a dirty read of the data
+      byte [] queueData = queueGet_noLock(shardRow, entryDataColumn, DIRTY_READ);
+
+      // TODO: Add partitioner checks here
+
+      // Attempt to update group meta to be dequeued by this consumer
+      TxidAndSize txidAndSize = attemptToClaimEntry(dequeue, entryPointer,
+          entryGroupMeta, new EntryGroupMeta(EntryGroupState.DEQUEUED,
+              now(), consumer.getInstanceId()));
+      if (txidAndSize == null) {
+        // Failed to claim entry, another consumer won,
+        // skip and move to next entry
+        skippedEntry = true;
+        entryPointer = nextEntry(entryPointer);
+        continue;
+      } else {
+        // Entry claimed, return entry.
+        return new Pair<HBQDequeueResult, TxidAndSize>(
+            new HBQDequeueResult(dequeue, entryPointer, queueData,
+                initialGroupState, currentGroupState), txidAndSize);
+      }
+    }
+    // Logic should never fall through here
+    throw new IOException("Reached invalid state in dequeue, fell out of loop");
+  }
+
+  public HBQInvalidateResult invalidate(HBQInvalidate invalidate)
+      throws IOException {
+    byte [] shardRow = invalidate.getMetaRow();
+    checkRow(shardRow, "invalidate");
+    boolean flush = false;
+    boolean writeToWAL = invalidate.getWriteToWAL();
+    WALEdit walEdits = writeToWAL ? new WALEdit() : null;
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    HBQInvalidateResult result = null;
+    long entryId = invalidate.getEntryPointer().getEntryId();
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "invalidate_count", 1);
+
+    // Lock row
+    startRegionOperation();
+    try {
+      Integer shardRowLock = getLock(null, shardRow, true);
+      this.updatesLock.readLock().lock();
+      try {
+
+        long ts_now = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "invalidate_lock", ts_now - ts_start);
+
+        // Update meta data for data entry to INVALID
+        queuePut_existingLock(shardRowLock, shardRow, dataMetaColumn(entryId),
+            new EntryMeta(EntryState.INVALID).getBytes(),
+            invalidate.getPointer().getWritePointer(), writeToWAL, walEdits,
+            size);
+
+        // Delete data column since it's invalidated
+        queueDelete_existingLock(shardRowLock, shardRow,
+            dataDataColumn(entryId), invalidate.getPointer().getWritePointer(),
+            writeToWAL, walEdits, size);
+
+        long ts_dataWrite = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "invalidate_put-and-delete",
+            ts_dataWrite - ts_now);
+
+        // Write out to WAL if enabled
+        long ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+        if (writeToWAL) {
+          txid = this.log.appendNoSync(this.regionInfo,
+              this.htableDescriptor.getName(), walEdits,
+              HConstants.DEFAULT_CLUSTER_ID,
+              EnvironmentEdgeManager.currentTimeMillis(),
+              this.htableDescriptor);
+          ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+          RegionMetricsStorage.incrTimeVaryingMetric(
+              HBQ_METRIC_PREFIX + "invalidate_wal-append",
+              ts_walAppend - ts_dataWrite);
+        }
+      } finally {
+        // Release meta row lock
+        this.updatesLock.readLock().unlock();
+        releaseRowLock(shardRowLock);
+      }
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "invalidate_wal-sync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.INVALIDATE,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+    return result;
+  }
+
+  public boolean ack(HBQAck ack) throws IOException {
+    HBQEntryPointer entryPointer = ack.getEntryPointer();
+    byte [] shardRow = ack.getDataRow(Bytes.toBytes(entryPointer.getShardId()));
+    checkRow(shardRow, "ack");
+    boolean flush = false;
+    boolean writeToWAL = ack.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    boolean result = false;
+    HBQConsumer consumer = ack.getConsumer();
+
+    RegionMetricsStorage.incrNumericMetric(HBQ_METRIC_PREFIX + "ack_count", 1);
+
+    startRegionOperation();
+    try {
+
+      // Perform dirty read of entry group meta for this entry
+      byte [] entryGroupMetaColumn = dataGroupMetaColumn(
+          entryPointer.getEntryId(), consumer.getGroupId());
+      byte [] entryGroupMetaBytes = queueGet_noLock(shardRow,
+          entryGroupMetaColumn, DIRTY_READ);
+
+      EntryGroupMeta groupMeta = EntryGroupMeta.fromBytes(entryGroupMetaBytes);
+
+      // Check if instance id matches
+      if (groupMeta.getInstanceId() != consumer.getInstanceId()) {
+        throw new IOException(
+            "Attempt to ack an entry of a different consumer instance.");
+      }
+
+      // Instance ids match, check if in an invalid state for ack'ing
+      if (groupMeta.isAvailable() || groupMeta.isAckedOrSemiAcked()) {
+        throw new IOException(
+            "Attempt to ack an entry that is not in ack'able state.");
+      }
+
+      // It is in the right state, attempt atomic semi_ack
+      // (ack passed if this CAS works, fails if this CAS fails)
+      byte [] newValue = new EntryGroupMeta(EntryGroupState.SEMI_ACKED,
+          now(), consumer.getInstanceId()).getBytes();
+
+      TxidAndSize txidAndSize = queueCAS(shardRow, entryGroupMetaColumn,
+          entryGroupMetaBytes, newValue, writeToWAL, null);
+
+      // If cas failed, fail ack
+      if (txidAndSize == null) return false;
+
+      // CAS was successful, update txid and size, sync log, and finish
+
+      txid = txidAndSize.updateTxid(txid);
+      size = txidAndSize.updateSize(size);
+
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "ack_walsync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.ACK,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+
+    return result;
+
+  }
+
+  public boolean finalize(HBQFinalize finalize) throws IOException {
+    HBQEntryPointer entryPointer = finalize.getEntryPointer();
+    byte [] shardRow = finalize.getDataRow(
+        Bytes.toBytes(entryPointer.getShardId()));
+    checkRow(shardRow, "finalize");
+    boolean flush = false;
+    boolean writeToWAL = finalize.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    boolean result = false;
+    HBQConsumer consumer = finalize.getConsumer();
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "finalize_count", 1);
+
+    startRegionOperation();
+    try {
+
+      // Perform dirty read of entry group meta for this entry
+      byte [] entryGroupMetaColumn = dataGroupMetaColumn(
+          entryPointer.getEntryId(), consumer.getGroupId());
+      byte [] entryGroupMetaBytes = queueGet_noLock(shardRow,
+          entryGroupMetaColumn, DIRTY_READ);
+
+      if (entryGroupMetaBytes == null) {
+        throw new IOException("No existing entry group meta found on finalize");
+      }
+
+      EntryGroupMeta groupMeta = EntryGroupMeta.fromBytes(entryGroupMetaBytes);
+
+      // Should only be in semi-acked state
+      if (!groupMeta.isSemiAcked()) {
+        throw new IOException(
+            "Attempt to finalize an entry that is not in semi-acked state.");
+      }
+
+      // It is in the right state, attempt atomic semi_ack to ack transition
+      // (finalize passed if this CAS works, fails if this CAS fails)
+      byte [] newValue = new EntryGroupMeta(EntryGroupState.ACKED,
+          now(), consumer.getInstanceId()).getBytes();
+
+      TxidAndSize txidAndSize = queueCAS(shardRow, entryGroupMetaColumn,
+          entryGroupMetaBytes, newValue, writeToWAL, null);
+
+      // If cas failed, fail finalize
+      if (txidAndSize == null) return false;
+
+      // TODO: Perform evict-on-ack here
+
+      // CAS was successful, update txid and size, sync log, and finish
+
+      txid = txidAndSize.updateTxid(txid);
+      size = txidAndSize.updateSize(size);
+
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "finalize_walsync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.FINALIZE,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+
+    return result;
+  }
+
+  public boolean unack(HBQUnack unack) throws IOException {
+    HBQEntryPointer entryPointer = unack.getEntryPointer();
+    byte [] shardRow = unack.getDataRow(
+        Bytes.toBytes(entryPointer.getShardId()));
+    checkRow(shardRow, "unack");
+    boolean flush = false;
+    boolean writeToWAL = unack.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    boolean result = false;
+    HBQConsumer consumer = unack.getConsumer();
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "unack_count", 1);
+
+    startRegionOperation();
+    try {
+
+      // Perform dirty read of entry group meta for this entry
+      byte [] entryGroupMetaColumn = dataGroupMetaColumn(
+          entryPointer.getEntryId(), consumer.getGroupId());
+      byte [] entryGroupMetaBytes = queueGet_noLock(shardRow,
+          entryGroupMetaColumn, DIRTY_READ);
+
+      if (entryGroupMetaBytes == null) {
+        throw new IOException("No existing entry group meta found on unack");
+      }
+
+      EntryGroupMeta groupMeta = EntryGroupMeta.fromBytes(entryGroupMetaBytes);
+
+      // Should only be in semi-acked state
+      if (!groupMeta.isSemiAcked()) {
+        throw new IOException(
+            "Attempt to unack an entry that is not in semi-acked state.");
+      }
+
+      // It is in the right state, attempt atomic semi_ack to dequeued transition
+      // (finalize passed if this CAS works, fails if this CAS fails)
+      byte [] newValue = new EntryGroupMeta(EntryGroupState.DEQUEUED,
+          now(), consumer.getInstanceId()).getBytes();
+
+      TxidAndSize txidAndSize = queueCAS(shardRow, entryGroupMetaColumn,
+          entryGroupMetaBytes, newValue, writeToWAL, null);
+
+      // If cas failed, fail unack
+      if (txidAndSize == null) return false;
+
+      // CAS was successful, update txid and size, sync log, and finish
+
+      txid = txidAndSize.updateTxid(txid);
+      size = txidAndSize.updateSize(size);
+
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "unack_walsync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.UNACK,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+
+    return result;
+  }
+
+  public Long getGroupID(HBQMetaOperation operation) throws IOException {
+    Increment increment = new Increment(operation.getGroupIdRow());
+    increment.addColumn(HBQConstants.HBQ_FAMILY, HBQConstants.GROUP_ID_GEN, 1L);
+    Result result = increment(increment, null, true);
+    return Bytes.toLong(result.raw()[0].getValue());
+  }
+
+  public HBQQueueMeta getQueueMeta(HBQMetaOperation operation)
+      throws IOException {
+    throw new IOException("Currently unsupported");
+  }
+
+  // Simple private helper methods
+
+  private long now() {
+    return EnvironmentEdgeManager.currentTimeMillis();
+  }
+
+  private HBQEntryPointer nextEntry(HBQEntryPointer entryPointer) {
+    return new HBQEntryPointer(entryPointer.getEntryId() + 1,
+        entryPointer.getShardId());
+  }
+
+  private HBQEntryPointer nextShard(HBQEntryPointer entryPointer) {
+    return new HBQEntryPointer(entryPointer.getEntryId(),
+        entryPointer.getShardId() + 1);
+  }
+
+  // Row and column makers
+
+  private byte[] dataDataColumn(long entryId) {
+    return Bytes.add(Bytes.toBytes(entryId), HBQConstants.ENTRY_DATA);
+  }
+
+  private byte[] dataMetaColumn(long entryId) {
+    return Bytes.add(Bytes.toBytes(entryId), HBQConstants.ENTRY_META);
+  }
+
+  private byte[] dataGroupMetaColumn(long entryId, long groupId) {
+    return Bytes.add(Bytes.toBytes(entryId), HBQConstants.ENTRY_GROUP_META,
+        Bytes.toBytes(groupId));
+  }
+
+  // Core database operation private helper methods
+
+  /**
+   * Writes the specified value to the specified row and column and with the
+   * specified write version.  Utilizes an existing row lock, wal edits, and
+   * size tracker.
+   * 
+   * The actual writing to the HLog and syncing before returning are the
+   * responsibility of the caller, this method only adds KVs to the walEdits
+   * list if enabled writeToWAL is enabled.
+   * 
+   * @param rowLock
+   * @param row
+   * @param column
+   * @param value
+   * @param writeVersion
+   * @param writeToWAL
+   * @param walEdits
+   * @param size
+   */
+  private void queuePut_existingLock(Integer rowLock, byte [] row,
+      byte [] column, byte [] value, long writeVersion,
+      boolean writeToWAL, WALEdit walEdits, AtomicLong size) {
+    queuePut_existingLock(rowLock, row,
+        new byte [][]  { column }, new byte [][] { value }, writeVersion,
+        writeToWAL, walEdits, size);
+  }
+
+  /**
+   * Writes the specified values to the specified row and columns and with the
+   * specified write version.  Utilizes an existing row lock, wal edits, and
+   * size tracker.
+   * 
+   * The actual writing to the HLog and syncing before returning are the
+   * responsibility of the caller, this method only adds KVs to the walEdits
+   * list if enabled writeToWAL is enabled.
+   * 
+   * @param rowLock
+   * @param row
+   * @param columns
+   * @param values
+   * @param writeVersion
+   * @param writeToWAL
+   * @param walEdits
+   * @param size
+   */
+  private void queuePut_existingLock(Integer rowLock, byte[] row,
+      byte[][] columns, byte[][] values, long writeVersion, boolean writeToWAL,
+      WALEdit walEdits, AtomicLong size) {
+    assert(columns.length == values.length);
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    Store store = this.stores.get(family);
+    // Create new KeyValues to store
+    List<KeyValue> kvs = new ArrayList<KeyValue>(columns.length);
+    for (int i=0; i<columns.length; i++) {
+      KeyValue kv = new KeyValue(row, family, columns[i], writeVersion,
+          values[i]);
+      kvs.add(kv);
+      // Write to WALEdits if enabled, actual writing and syncing done by caller
+      if (writeToWAL) {
+        walEdits.add(kv);
+      }
+    }
+    // Add new kv
+    long sizeDelta = 0;
+    for (KeyValue kv : kvs) {
+      sizeDelta += store.add(kv);
+    }
+    size.addAndGet(sizeDelta);
+  }
+
+  /**
+   * Deletes any values at the specified row and columns and with a write
+   * version less than or equal to the specified write version.  Utilizes an
+   * existing row lock, wal edits, and size tracker.
+   * 
+   * The actual writing to the HLog and syncing before returning are the
+   * responsibility of the caller, this method only adds KVs to the walEdits
+   * list if enabled writeToWAL is enabled.
+   * 
+   * @param rowLock
+   * @param row
+   * @param column
+   * @param writeVersion
+   * @param writeToWAL
+   * @param walEdits
+   * @param size
+   */
+  private void queueDelete_existingLock(Integer rowLock, byte[] row,
+      byte[] column, long writeVersion, boolean writeToWAL, WALEdit walEdits,
+      AtomicLong size) {
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    Store store = this.stores.get(family);
+    // Create new KeyValue to store
+    KeyValue kv = new KeyValue(row, family, column, writeVersion,
+        KeyValue.Type.DeleteColumn);
+    // Write to WALEdits if enabled, actual writing and syncing done by caller
+    if (writeToWAL) {
+      walEdits.add(kv);
+    }
+    // Add new kv
+    size.addAndGet(store.add(kv));
+  }
+
+  /**
+   * Gets the latest version of the specified row and column that has a version
+   * less than the specified max version.  Returns null if nothing exists.
+   * @param row
+   * @param column
+   * @param version
+   * @return
+   * @throws IOException
+   */
+  private byte[] queueGet_noLock(byte[] row, byte[] column, long maxVersion)
+      throws IOException {
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    // Get previous value
+    Get get = new Get(row);
+    get.addColumn(family, column);
+    get.setTimeRange(0, maxVersion);
+    List<KeyValue> results = get(get, false);
+    if (results.isEmpty()) return null;
+    return results.get(0).getValue();
+  }
+
+
+  /**
+   * Gets the latest version of the specified row and column that is visible
+   * according to the specified read pointer.  Returns null if nothing exists.
+   * @param row
+   * @param column
+   * @param pointer
+   * @return
+   * @throws IOException
+   */
+  private byte[] queueGet_noLock(byte[] row, byte[] column,
+      HBReadPointer pointer) throws IOException {
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    // Get previous value
+    Get get = new Get(row);
+    get.addColumn(family, column);
+    get.setTimeRange(0, pointer.getReadPointer());
+    get.setMaxVersions();
+    List<KeyValue> results = get(get, false);
+    if (results.isEmpty()) return null;
+    for (KeyValue kv : results) {
+      if (pointer.isVisible(kv.getTimestamp())) {
+        return kv.getValue();
+      }
+    }
+    return null;
+  }
+
+  /**
+   * Increments the specified row and column by 1, returning the
+   * post-incremented value.  Utilizes an existing row lock, wal edits, and
+   * size tracker.
+   * 
+   * The actual writing to the HLog and syncing before returning are the
+   * responsibility of the caller, this method only adds KVs to the walEdits
+   * list if enabled writeToWAL is enabled.
+   * 
+   * @param rowLock
+   * @param row
+   * @param column
+   * @param writeVersion
+   * @param writeToWAL
+   * @param walEdits
+   * @param size
+   * @return
+   * @throws IOException
+   */
+  private long queueIncrement_existingLock(Integer rowLock, byte [] row,
+      byte [] column, long writeVersion, boolean writeToWAL, WALEdit walEdits,
+      AtomicLong size) throws IOException {
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    Store store = this.stores.get(family);
+    // Get previous value
+    Get get = new Get(row);
+    get.addColumn(family, column);
+    List<KeyValue> results = get(get, false);
+    // Determine updated amount to store
+    long amount = 1;
+    if (!results.isEmpty()) {
+      KeyValue kv = results.get(0);
+      amount += Bytes.toLong(kv.getBuffer(), kv.getValueOffset(),
+          kv.getValueLength());
+    }
+    // Create new KeyValue to store
+    KeyValue newKV = new KeyValue(row, family, column, writeVersion,
+        Bytes.toBytes(amount));
+    // Write to WALEdits if enabled, actual writing and syncing done by caller
+    if (writeToWAL) {
+      walEdits.add(newKV);
+    }
+    List<KeyValue> kvs = Arrays.asList(new KeyValue [] { newKV });
+    // Upsert new kv
+    long sizeDelta = store.upsert(kvs);
+    size.addAndGet(sizeDelta);
+    return amount;
+  }
+
+  private interface QueueCASValueComparer {
+    boolean checkExistingValueOkay(byte [] existingValue);
+  }
+
+  /**
+   * Perform a dirty, atomic compare-and-swap operation.
+   * <p>
+   * If write-to-wal is specified, performs append-no-sync for any edits and
+   * returns the txid and size of edits.
+   * <p>
+   * Contains an optional existing value comparer that allows for cases that an
+   * existing value is found that does not match the expected value, so no
+   * update to the new value should be performed, but if the existing value
+   * passes the specified comparer, the operation will be treated as successful.
+   * <p>
+   * An expected value of null means that this operation expects there to be
+   * no existing value.  A new value of null is not supported.
+   * <p>
+   * If operation fails, returns null.
+   * <p>
+   * Should be called with a region operation lock but no row or update locks.
+   * 
+   * @param row
+   * @param column
+   * @param expectedValue value to verify currently exists,
+   *                      or null for does not exist
+   * @param newValue new value to replace if expected is found
+   * @param existingValueComparer optional existing value comparer to use if
+   *                              expected value not found
+   * @return txid and size if writes occurred and wal is on, zero valued txid
+   *         and size if writes occurred and wal is off OR existing value
+   *         comparer passed, or returns null if operation failed completely
+   * @throws IOException
+   */
+  private TxidAndSize queueCAS(byte [] row, byte [] column,
+      byte [] expectedValue, byte [] newValue, boolean writeToWAL,
+      QueueCASValueComparer existingValueComparer)
+          throws IOException {
+    WALEdit walEdits = writeToWAL ? new WALEdit() : null;
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+
+    // Lock row
+    Integer rowLock = getLock(null, row, true);
+    this.updatesLock.readLock().lock();
+    try {
+
+      long ts_now = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "queue_cas_lock", ts_now - ts_start);
+
+      // Read existing value using specified read pointer
+      byte [] existingValue = queueGet_noLock(row, column, DIRTY_READ);
+
+      long ts_dataRead = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "queue_cas_get",
+          ts_dataRead - ts_now);
+
+      // Check if it is what is expected
+      if ((expectedValue == null && existingValue == null) ||
+          (expectedValue != null && existingValue != null &&
+          Bytes.equals(expectedValue, existingValue))) {
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "queue_cas_expected_match", 1L);
+
+        // Existing and expected match, perform update to new value
+        queuePut_existingLock(rowLock, row, column, newValue, ts_now,
+            writeToWAL, walEdits, size);
+
+        long ts_dataWrite = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "queue_cas_put",
+            ts_dataWrite - ts_dataRead);
+
+        // Write out to WAL if enabled
+        long ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+        if (writeToWAL) {
+          txid = this.log.appendNoSync(this.regionInfo,
+              this.htableDescriptor.getName(), walEdits,
+              HConstants.DEFAULT_CLUSTER_ID,
+              EnvironmentEdgeManager.currentTimeMillis(),
+              this.htableDescriptor);
+          ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+          RegionMetricsStorage.incrTimeVaryingMetric(
+              HBQ_METRIC_PREFIX + "queue_cas_wal_append",
+              ts_walAppend - ts_dataWrite);
+        }
+        return new TxidAndSize(txid, size.get());
+      }
+
+      // Existing is not what was expected, check comparer
+      if (existingValueComparer != null &&
+          existingValueComparer.checkExistingValueOkay(existingValue)) {
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "queue_cas_comparer_match", 1L);
+        // Not a straight match but comparer passes, treat as success but
+        // no writes so no size change
+        return new TxidAndSize();
+      }
+
+      // Compare and swap operation failed, return null
+      RegionMetricsStorage.incrNumericMetric(
+          HBQ_METRIC_PREFIX + "queue_cas_fail", 1L);
+      return null;
+
+    } finally {
+      // Release meta row lock
+      this.updatesLock.readLock().unlock();
+      releaseRowLock(rowLock);
+    }
+  }
+
+  // Complex operation private helper methods
+
+  /**
+   * Attempts to perform an atomic update of the entry group meta.
+   * <p>
+   * For behavior of operation, see
+   * {@link #queueCAS(byte[], byte[], byte[], byte[], boolean, QueueCASValueComparer)}
+   * (this operation does not use a value comparer).
+   * 
+   * @param dequeue
+   * @param existingEntryGroupMeta
+   * @param updatedEntryGroupMeta
+   * @return
+   */
+  private TxidAndSize attemptToClaimEntry(HBQDequeue dequeue,
+      HBQEntryPointer entryPointer, EntryGroupMeta existingEntryGroupMeta,
+      EntryGroupMeta newEntryGroupMeta)
+          throws IOException {
+    return queueCAS(
+        dequeue.getDataRow(Bytes.toBytes(entryPointer.getShardId())),
+        dataGroupMetaColumn(entryPointer.getEntryId(),
+            dequeue.getConsumer().getGroupId()),
+            existingEntryGroupMeta == null ?
+                null : existingEntryGroupMeta.getBytes(),
+                newEntryGroupMeta.getBytes(), dequeue.getWriteToWAL(), null);
+  }
+
+  /**
+   * Updates the group state from the specified existing state to the
+   * new state, for the queue and consumer group specified in the dequeue
+   * operation.
+   * 
+   * This method is similar to an atomic compare-and-swap in that it
+   * expects to see the existing value and wants to update it to the new
+   * value, however, it is different in that it will also accept the stored
+   * group state to already be the new group state.
+   * 
+   * If the current group state is neither the expected existing or new state,
+   * an exception is thrown.
+   * 
+   * This method should be called while holding the overall region operation
+   * lock, but no other locks (row lock, updates lock).  A row lock will be
+   * acquired within this method on this group's meta row.
+   * 
+   * If the write-to-wal flag is true, any writes performed will be written out
+   * to the wal under the row lock with an append-no-sync and the txid will
+   * be returned (txid is 0 if no writes were performed or the wal is disabled).
+   * 
+   * @param dequeue
+   * @param groupStateExisting
+   * @param groupStateNew
+   * @param writeToWAL
+   * @return
+   * @throws IOException the current state is neither the expected existing
+   *                     or new state
+   */
+  private TxidAndSize updateGroupState(final HBQDequeue dequeue,
+      final GroupState groupStateExisting, final GroupState groupStateNew,
+      final boolean writeToWAL)
+          throws IOException {
+    // Also accept the existing value having same non-pointer configs as new
+    final QueueCASValueComparer comparer = new QueueCASValueComparer() {
+      @Override
+      public boolean checkExistingValueOkay(byte[] existingValue) {
+        GroupState existing = GroupState.fromBytes(existingValue);
+        return existing.nonPointerConfigEquals(groupStateNew);
+      }
+    };
+    TxidAndSize txidAndSize = queueCAS(dequeue.getGroupMetaRow(),
+        HBQConstants.GROUP_STATE,
+        groupStateExisting == null ? null : groupStateExisting.getBytes(),
+            groupStateNew.getBytes(), dequeue.getWriteToWAL(), comparer);
+    if (txidAndSize == null) {
+      throw new IOException("Group state update failed, concurrent mods");
+    }
+    return txidAndSize;
+  }
+
+  /**
+   * Updates the group state to the specified new state, for the queue and
+   * consumer group specified in the dequeue operation.
+   * 
+   * This method attempts to make the value of specified group state the
+   * specified current group state by reading the existing value, comparing it,
+   * and determining if it is valid to perform an update, already updated in a
+   * compatible way, or incompatible.  Throws an exception if incompatible as
+   * it should only happen when consumers are not behaving as expected.
+   * 
+   * This method should be called while holding the overall region operation
+   * lock, but no other locks (row lock, updates lock).  A row lock will be
+   * acquired within this method on this group's meta row.
+   * 
+   * If the write-to-wal flag is true, any writes performed will be written out
+   * to the wal under the row lock with an append-no-sync and the txid will
+   * be returned (txid is 0 if no writes were performed or the wal is disabled).
+   * 
+   * @param dequeue
+   * @param newGroupState
+   * @param writeToWAL
+   * @return
+   * @throws IOException the current state is neither the expected existing
+   *                     or new state
+   */
+  private TxidAndSize updateGroupState(HBQDequeue dequeue,
+      final GroupState newGroupState, boolean writeToWAL)
+          throws IOException {
+    // Also accept the existing value having same non-pointer configs as new
+    // and entry pointer greater or equal to specified
+    final QueueCASValueComparer comparer = new QueueCASValueComparer() {
+      @Override
+      public boolean checkExistingValueOkay(byte[] existingValue) {
+        GroupState existing = GroupState.fromBytes(existingValue);
+        if (!existing.nonPointerConfigEquals(newGroupState)) return false;
+        if (!existing.isGreaterThanOrEqual(newGroupState)) return false;
+        return true;
+      }
+    };
+    TxidAndSize txidAndSize = queueCAS(dequeue.getGroupMetaRow(),
+        HBQConstants.GROUP_STATE, newGroupState.getBytes(),
+        newGroupState.getBytes(), dequeue.getWriteToWAL(), comparer);
+    if (txidAndSize == null) {
+      throw new IOException("Group state update failed, concurrent mods");
+    }
+    return txidAndSize;
+  }
+
+  /**
+   * Checks whether the specified group has any pending entries, beginning
+   * with the head pointer in the specified group state.
+   * <p>
+   * Warning: this is a linear and expensive operation and in the worst case
+   * can end up being a distributed crawl of a long queue
+   * 
+   * TODO: Implement this (currently group reconfig is not supported)
+   * 
+   * @param groupState
+   * @param groupId
+   * @return
+   */
+  private boolean groupHasPendingEntries(GroupState groupState, long groupId) {
+    return false;
+  }
+
+  private boolean ackedOrTimedOutSemiAcked(EntryGroupMeta entryGroupMeta,
+      long maxAgeBeforeSemiAckedToAcked) {
+    return entryGroupMeta.isAcked() ||
+        (entryGroupMeta.isSemiAcked() &&
+            entryGroupMeta.getTimestamp() + maxAgeBeforeSemiAckedToAcked <=
+            now());
+  }
+
+  /**
+   * Attempts to update the group pointer by checking whether the entry pointer
+   * is one entry away from the current group state.  If the specified group
+   * state is null, returns null.
+   * @param groupState
+   * @param entryPointer
+   * @return
+   */
+  private GroupState attemptGroupPointerUpdate(boolean skippedEntry,
+      GroupState groupState, HBQEntryPointer entryPointer) {
+    if (groupState == null) return null;
+    if (skippedEntry) return groupState;
+    if (entryPointer.getEntryId() == 1 &&
+        entryPointer.getShardId() == groupState.getHead().getShardId() + 1) {
+      return new GroupState(groupState, entryPointer);
+    }
+    return null;
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(working copy)
@@ -165,6 +165,17 @@
 import org.apache.zookeeper.KeeperException;
 import org.codehaus.jackson.map.ObjectMapper;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
 import com.google.common.base.Function;
 import com.google.common.collect.Lists;
 
@@ -2070,8 +2081,8 @@
 
   private boolean checkAndMutate(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final CompareOp compareOp,
-      final WritableByteArrayComparable comparator, final Writable w,
-      Integer lock) throws IOException {
+      final WritableByteArrayComparable comparator, final long readVersion,
+      final Writable w, Integer lock) throws IOException {
     checkOpen();
     this.requestCount.incrementAndGet();
     HRegion region = getRegion(regionName);
@@ -2080,7 +2091,7 @@
         this.cacheFlusher.reclaimMemStoreMemory();
       }
       return region.checkAndMutate(row, family, qualifier, compareOp,
-        comparator, w, lock, true);
+        comparator, readVersion, w, lock, true);
     } catch (Throwable t) {
       throw convertThrowableToIOE(cleanup(t));
     }
@@ -2100,7 +2111,7 @@
    */
   public boolean checkAndPut(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final byte[] value,
-      final Put put) throws IOException {
+      final long readVersion, final Put put) throws IOException {
     checkOpen();
     if (regionName == null) {
       throw new IOException("Invalid arguments to checkAndPut "
@@ -2118,7 +2129,7 @@
       }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-        CompareOp.EQUAL, comparator, put,
+        CompareOp.EQUAL, comparator, readVersion, put,
       lock);
     if (region.getCoprocessorHost() != null) {
       result = region.getCoprocessorHost().postCheckAndPut(row, family,
@@ -2141,7 +2152,8 @@
    */
   public boolean checkAndPut(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final CompareOp compareOp,
-      final WritableByteArrayComparable comparator, final Put put)
+      final WritableByteArrayComparable comparator,
+      final Put put)
        throws IOException {
     checkOpen();
     if (regionName == null) {
@@ -2158,7 +2170,7 @@
       }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      compareOp, comparator, put, lock);
+      compareOp, comparator, Long.MAX_VALUE, put, lock);
     if (region.getCoprocessorHost() != null) {
       result = region.getCoprocessorHost().postCheckAndPut(row, family,
         qualifier, compareOp, comparator, put, result);
@@ -2180,7 +2192,7 @@
    */
   public boolean checkAndDelete(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final byte[] value,
-      final Delete delete) throws IOException {
+      final long readVersion, final Delete delete) throws IOException {
     checkOpen();
 
     if (regionName == null) {
@@ -2198,7 +2210,7 @@
       }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      CompareOp.EQUAL, comparator, delete, lock);
+      CompareOp.EQUAL, comparator, readVersion, delete, lock);
     if (region.getCoprocessorHost() != null) {
       result = region.getCoprocessorHost().postCheckAndDelete(row, family,
         qualifier, CompareOp.EQUAL, comparator, delete, result);
@@ -2306,7 +2318,7 @@
      }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      compareOp, comparator, delete, lock);
+      compareOp, comparator, Long.MAX_VALUE, delete, lock);
    if (region.getCoprocessorHost() != null) {
      result = region.getCoprocessorHost().postCheckAndDelete(row, family,
        qualifier, compareOp, comparator, delete, result);
@@ -3752,4 +3764,257 @@
       HRegionInfo info = region.getRegionInfo();
       return CompactionRequest.getCompactionState(info.getRegionId()).name();
   }
+
+  @Override
+  public HBQEnqueueResult enqueue(byte[] regionName, byte[] row,
+      HBQEnqueue enqueue) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      HBQEnqueueResult result = region.enqueue(enqueue);
+      // If there was any kind of failure, return result
+      if (!result.isSuccess()) return result;
+      // If data has been written, return result
+      if (result.getDataWritten()) return result;
+      // Data needs to be written to another region, see if it's on this server
+      byte [] tableName = region.getTableDesc().getName();
+      byte [] shardRow = enqueue.getDataRow(
+          Bytes.toBytes(result.getEntryPointer().getShardId() -
+              (result.getInsertShardEnd() ? 1 : 0)));
+      region = findRegion(tableName, shardRow);
+      // If region wasn't found on this server, return back to client
+      if (region == null) return result;
+      // Found on this server, perform it again on new region
+      result = region.enqueue_data(enqueue, result);
+      // If there was any kind of failure, return result
+      if (!result.isSuccess()) return result;
+      // If data has been written, return result
+      if (result.getDataWritten()) return result;
+      // If data has still not been written, must have written shard end
+      if (result.getInsertShardEnd()) {
+        // This case should be invalid, throwing exception
+        throw new IOException("Fell into invalid state during enqueue");
+      }
+      // Actual data needs to be written to another region, check this server
+      shardRow = enqueue.getDataRow(
+          Bytes.toBytes(result.getEntryPointer().getShardId()));
+      region = findRegion(tableName, shardRow);
+      // If region wasn't found on this server, return back to client
+      if (region == null) return result;
+      // Found on this server, perform it again on new region
+      result = region.enqueue_data(enqueue, result);
+      // In all cases, return the result back to client
+      return result;
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public HBQEnqueueResult enqueue_data(byte[] regionName, byte[] row,
+      HBQEnqueue enqueue, HBQEnqueueResult previousResult) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      HBQEnqueueResult result = region.enqueue_data(enqueue, previousResult);
+      // If there was any kind of failure, return result
+      if (!result.isSuccess()) return result;
+      // If data has been written, return result
+      if (result.getDataWritten()) return result;
+      // If data has still not been written, must have written shard end
+      if (result.getInsertShardEnd() || !previousResult.getInsertShardEnd()) {
+        // This case should be invalid, throwing exception
+        throw new IOException("Fell into invalid state during enqueue");
+      }
+      // Actual data needs to be written to another region, check this server
+      byte [] tableName = region.getTableDesc().getName();
+      byte [] shardRow = enqueue.getDataRow(
+          Bytes.toBytes(result.getEntryPointer().getShardId()));
+      region = findRegion(tableName, shardRow);
+      // If region wasn't found on this server, return back to client
+      if (region == null) return result;
+      // Found on this server, perform it again on new region
+      result = region.enqueue_data(enqueue, result);
+      // In all cases, return the result back to client
+      return result;
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public HBQDequeueResult dequeue(byte[] regionName, byte[] row,
+      HBQDequeue dequeue) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      
+      // Perform dequeue, starting at the region hosting the global meta row
+      HBQDequeueResult result = region.dequeue(dequeue);
+      
+      // If result is not in-progress, return immediately
+      if (!result.isInProgress()) return result;
+      
+      // Dequeue is still in progress
+      
+      // Group state is already updated within dequeue call, if necessary,
+      // so we are just looking for the next data block.
+      
+      // Check if the next row we are looking for is on this server
+      byte [] tableName = region.getTableDesc().getName();
+      byte [] shardRow = result.getNextRow();
+      region = findRegion(tableName, shardRow);
+      
+      // If region not on this server, return back to client
+      if (region == null) return result;
+      
+      // Region is on this server, perform dequeue_continue on this server
+      // using updated region and row, and return result directly
+      return dequeue_continue(region.getRegionName(), shardRow, result);
+      
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public HBQDequeueResult dequeue_continue(byte[] regionName, byte[] row,
+      HBQDequeueResult previousResult) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      
+      // Perform dequeue_continue on the requested region
+      HBQDequeueResult result = region.dequeue_continue(previousResult);
+
+      // Keep looping as long as dequeue is in progress
+      // (note, loop is broken out of if the next row is on another server
+      while (result.isInProgress()) {
+        
+        // Dequeue is still in progress
+        
+        // Must find region of next row to continue processing.
+        byte [] nextRow = result.getNextRow();
+  
+        // Check if the next row is on this server
+        byte [] tableName = region.getTableDesc().getName();
+        region = findRegion(tableName, nextRow);
+  
+        // If region not on this server, return back to client
+        if (region == null) return result;
+        
+        // Region is on this server, perform dequeue_continue on this server
+        // using updated region and row, and return result directly
+        result = dequeue_continue(region.getRegionName(), nextRow, result);
+      }
+      
+      return result;
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+  
+  @Override
+  public HBQInvalidateResult invalidate(byte[] regionName, byte[] row,
+      HBQInvalidate invalidate) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.invalidate(invalidate);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public boolean ack(byte[] regionName, byte[] row, HBQAck ack)
+      throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.ack(ack);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public boolean unack(byte[] regionName, byte[] row, HBQUnack unack)
+      throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.unack(unack);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public boolean finalize(byte[] regionName, byte[] row, HBQFinalize finalize)
+      throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.finalize(finalize);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public Long getGroupID(byte[] regionName, byte[] row,
+      HBQMetaOperation operation) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.getGroupID(operation);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public HBQQueueMeta getQueueMeta(byte[] regionName, byte[] row,
+      HBQMetaOperation operation) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.getQueueMeta(operation);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  // Private helpers
+
+  /**
+   * Checks the online regions of this server to see if the region for the
+   * specified table and row are located on this server.  If the region is on
+   * this server, the region is returned.  Otherwise, null is returned.
+   * 
+   * @param tableName
+   * @param row
+   * @return region of the specified table and row, or null if not found
+   */
+  private HRegion findRegion(byte[] tableName, byte[] row) {
+    List<HRegion> regionsOfTable = this.getOnlineRegions(tableName);
+    if (regionsOfTable == null || regionsOfTable.isEmpty()) return null;
+    for (HRegion region : regionsOfTable) {
+      if (HRegion.rowIsInRange(region.getRegionInfo(), row)) {
+        return region;
+      }
+    }
+    return null;
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java	(working copy)
@@ -48,6 +48,8 @@
   private byte deleteType = 0;
   private long deleteTimestamp = 0L;
 
+  private boolean undelete = false;
+  
   /**
    * Constructor for ScanDeleteTracker
    */
@@ -75,12 +77,25 @@
         familyStamp = timestamp;
         return;
       }
-
-      if (deleteBuffer != null && type < deleteType) {
+      if (deleteBuffer == null && type == KeyValue.Type.UndeleteColumn.getCode()) {
+        undelete = true;
+      }
+      else if (deleteBuffer != null) {
         // same column, so ignore less specific delete
         if (Bytes.equals(deleteBuffer, deleteOffset, deleteLength,
-            buffer, qualifierOffset, qualifierLength)){
-          return;
+            buffer, qualifierOffset, qualifierLength)) {
+          if (undelete && timestamp == deleteTimestamp &&
+              type == KeyValue.Type.DeleteColumn.getCode()) {
+            undelete = false;
+            deleteBuffer = null;
+            return;
+          }
+          if (type < deleteType||
+              (type == KeyValue.Type.UndeleteColumn.getCode() &&
+               deleteTimestamp > timestamp &&
+               deleteType != KeyValue.Type.Delete.getCode())) return;
+        } else {
+          undelete = type == KeyValue.Type.UndeleteColumn.getCode();
         }
       }
       // new column, or more general delete type
@@ -118,6 +133,9 @@
         if (deleteType == KeyValue.Type.DeleteColumn.getCode()) {
           return DeleteResult.COLUMN_DELETED;
         }
+        if (deleteType == KeyValue.Type.UndeleteColumn.getCode()) {
+          return DeleteResult.NOT_DELETED;
+        }
         // Delete (aka DeleteVersion)
         // If the timestamp is the same, keep this one
         if (timestamp == deleteTimestamp) {
@@ -154,6 +172,7 @@
     hasFamilyStamp = false;
     familyStamp = 0L;
     deleteBuffer = null;
+    undelete = false;
   }
 
   @Override
Index: src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java	(working copy)
@@ -556,6 +556,9 @@
     // test that triggers the pathological case if we don't avoid MSLAB
     // here.
     long addedSize = internalAdd(kv);
+    // jgray : for now, we don't delete existing stuff for omid
+    boolean omidSupport = true;
+    if (omidSupport) return addedSize;
 
     // Get the KeyValues for the row/family/qualifier regardless of timestamp.
     // For this case we want to clean up any other puts
Index: src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java	(working copy)
@@ -39,6 +39,18 @@
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.io.IOUtils;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 import java.io.File;
 import java.io.FileOutputStream;
 import java.io.IOException;
@@ -414,13 +426,13 @@
       }
 
       public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-          byte[] value, Put put) throws IOException {
-        return table.checkAndPut(row, family, qualifier, value, put);
+          byte[] value, long readVersion, Put put) throws IOException {
+        return table.checkAndPut(row, family, qualifier, value, readVersion, put);
       }
 
       public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-          byte[] value, Delete delete) throws IOException {
-        return table.checkAndDelete(row, family, qualifier, value, delete);
+          byte[] value, long readVersion, Delete delete) throws IOException {
+        return table.checkAndDelete(row, family, qualifier, value, readVersion, delete);
       }
 
       public long incrementColumnValue(byte[] row, byte[] family,
@@ -545,6 +557,50 @@
       public void setWriteBufferSize(long writeBufferSize) throws IOException {
         table.setWriteBufferSize(writeBufferSize);
       }
+
+      // Continuuity methods (unimplemented)
+
+      @Override
+      public HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException {
+        return null;
+      }
+
+      @Override
+      public HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException {
+        return null;
+      }
+
+      @Override
+      public HBQInvalidateResult invalidate(HBQInvalidate invalidate)
+          throws IOException {
+        return null;
+      }
+
+      @Override
+      public boolean ack(HBQAck ack) throws IOException {
+        return false;
+      }
+
+      @Override
+      public boolean unack(HBQUnack unack) throws IOException {
+        return false;
+      }
+
+      @Override
+      public boolean finalize(HBQFinalize finalize) throws IOException {
+        return false;
+      }
+
+      @Override
+      public long getGroupID(HBQMetaOperation operation) throws IOException {
+        return 0;
+      }
+
+      @Override
+      public HBQQueueMeta getQueueMeta(HBQMetaOperation operation)
+          throws IOException {
+        return null;
+      }
     }
 
     /** The coprocessor */
Index: src/main/java/org/apache/hadoop/hbase/KeyValue.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/KeyValue.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/KeyValue.java	(working copy)
@@ -173,6 +173,7 @@
 
     Delete((byte)8),
     DeleteColumn((byte)12),
+    UndeleteColumn((byte)13),
     DeleteFamily((byte)14),
 
     // Maximum is used when searching; you look from maximum on down.
Index: src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java	(working copy)
@@ -22,8 +22,8 @@
 import java.io.ByteArrayOutputStream;
 import java.io.DataInput;
 import java.io.DataOutput;
+import java.io.IOException;
 import java.io.InputStream;
-import java.io.IOException;
 import java.io.ObjectInputStream;
 import java.io.ObjectOutputStream;
 import java.io.Serializable;
@@ -98,6 +98,24 @@
 import org.apache.hadoop.io.WritableFactories;
 import org.apache.hadoop.io.WritableUtils;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQConfig;
+import com.continuuity.hbase.ttqueue.HBQConsumer;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQEntryPointer;
+import com.continuuity.hbase.ttqueue.HBQExpirationConfig;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQShardConfig;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+import com.continuuity.hbase.ttqueue.HBReadPointer;
 import com.google.protobuf.Message;
 
 /**
@@ -266,6 +284,9 @@
     GENERIC_ARRAY_CODE = code++;
     addToMap(Array.class, GENERIC_ARRAY_CODE);
 
+    // Continuuity additions
+    code = addContinuuityClassesToMap(code);
+    
     // make sure that this is the last statement in this static block
     NEXT_CLASS_CODE = code;
   }
@@ -279,6 +300,28 @@
     super();
   }
 
+  private static int addContinuuityClassesToMap(int code) {
+    addToMap(HBQAck.class, code++);
+    addToMap(HBQConfig.class, code++);
+    addToMap(HBQConsumer.class, code++);
+    addToMap(HBQDequeue.class, code++);
+    addToMap(HBQDequeueResult.class, code++);
+    addToMap(HBQEnqueue.class, code++);
+    addToMap(HBQEnqueueResult.class, code++);
+    addToMap(HBQEntryPointer.class, code++);
+    addToMap(HBQExpirationConfig.class, code++);
+    addToMap(HBQFinalize.class, code++);
+    addToMap(HBQInvalidate.class, code++);
+    addToMap(HBQInvalidateResult.class, code++);
+    addToMap(HBQMetaOperation.class, code++);
+    addToMap(HBQOperation.class, code++);
+    addToMap(HBQQueueMeta.class, code++);
+    addToMap(HBQShardConfig.class, code++);
+    addToMap(HBQUnack.class, code++);
+    addToMap(HBReadPointer.class, code++);
+    return code;
+  }
+
   /**
    * @param instance
    */
Index: src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(revision 1383334)
+++ src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(working copy)
@@ -53,6 +53,19 @@
 import org.apache.hadoop.ipc.RemoteException;
 import org.apache.hadoop.hbase.ipc.VersionedProtocol;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQEntryPointer;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 /**
  * Clients interact with HRegionServers using a handle to the HRegionInterface.
  *
@@ -223,7 +236,7 @@
    */
   public boolean checkAndPut(final byte[] regionName, final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Put put)
+      final long readVersion, final Put put)
   throws IOException;
 
 
@@ -237,13 +250,14 @@
    * @param family column family
    * @param qualifier column qualifier
    * @param value the expected value
+   * @param readVersion 
    * @param delete data to delete if check succeeds
    * @throws IOException e
    * @return true if the new delete was execute, false otherwise
    */
   public boolean checkAndDelete(final byte[] regionName, final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Delete delete)
+      long readVersion, final Delete delete)
   throws IOException;
 
   /**
@@ -633,4 +647,37 @@
 
   @Override
   public void stop(String why);
+
+  // Continuuity Queue Methods
+
+  public HBQEnqueueResult enqueue(byte[] regionName, byte[] row,
+      HBQEnqueue enqueue) throws IOException;
+
+  public HBQEnqueueResult enqueue_data(byte[] regionName, byte[] row,
+      HBQEnqueue enqueue, HBQEnqueueResult previousResult) throws IOException;
+
+  public HBQDequeueResult dequeue(byte[] regionName, byte[] row,
+      HBQDequeue dequeue) throws IOException;
+
+  public HBQDequeueResult dequeue_continue(byte[] regionName, byte[] row,
+      HBQDequeueResult previousResult) throws IOException;
+
+  public HBQInvalidateResult invalidate(byte[] regionName, byte[] row,
+      HBQInvalidate invalidate) throws IOException;
+
+  public boolean ack(byte[] regionName, byte[] row,
+      HBQAck ack) throws IOException;
+
+  public boolean unack(byte[] regionName, byte[] row,
+      HBQUnack unack) throws IOException;
+
+  public boolean finalize(byte[] regionName, byte[] row,
+      HBQFinalize finalize) throws IOException;
+
+  public Long getGroupID(byte[] regionName, byte[] row,
+      HBQMetaOperation operation) throws IOException;
+
+  public HBQQueueMeta getQueueMeta(byte[] regionName, byte[] row,
+      HBQMetaOperation operation) throws IOException;
+  
 }
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQConfig.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQConfig.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQConfig.java	(revision 0)
@@ -0,0 +1,52 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Queue configuration settings, used by a consumer during a dequeue.
+ * 
+ * TODO: The partitioner has been removed for now until we figure out how it
+ *       will actually work
+ */
+public class HBQConfig implements Writable {
+
+  private boolean singleEntry;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQConfig() {}
+  
+  public HBQConfig(final boolean singleEntry) {
+    this.singleEntry = singleEntry;
+  }
+
+  public boolean isSingleEntry() {
+    return this.singleEntry;
+  }
+
+  public boolean isMultiEntry() {
+    return !isSingleEntry();
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.singleEntry = in.readBoolean();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeBoolean(this.singleEntry);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("singleEntry", this.singleEntry)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueueResult.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueueResult.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueueResult.java	(revision 0)
@@ -0,0 +1,120 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Result of an enqueue operation.
+ */
+public class HBQEnqueueResult implements Writable {
+
+  private boolean success;
+  private String msg;
+  private HBQEntryPointer entryPointer;
+  private boolean dataWritten = false;
+  private boolean insertShardEnd = false;
+  
+  /** Empty constructor for Writable (do not use) */
+  public HBQEnqueueResult() {}
+  
+  /**
+   * Failed dequeue constructor.  Just specify message.
+   * @param msg
+   */
+  public HBQEnqueueResult(final String msg) {
+    this.success = false;
+    this.msg = msg;
+  }
+
+  /**
+   * Constructs an enqueue result that is completely finished, that is,
+   * both meta and data have been inserted successfully.
+   * @param entryPointer
+   */
+  public HBQEnqueueResult(final HBQEntryPointer entryPointer) {
+    this.success = true;
+    this.entryPointer = entryPointer;
+    this.dataWritten = true;
+  }
+
+  /**
+   * Constructs an enqueue result when data has not yet been inserted,
+   * signaling the caller to perform the call on another region or server.
+   * @param hbqEntryPointer
+   * @param insertShardEnd
+   */
+  public HBQEnqueueResult(final HBQEntryPointer hbqEntryPointer,
+      final boolean insertShardEnd) {
+    this.success = true;
+    this.entryPointer = hbqEntryPointer;
+    this.dataWritten = false;
+    this.insertShardEnd = insertShardEnd;
+  }
+  
+  public boolean isSuccess() {
+    return this.success;
+  }
+
+  public String getFailureMessage() {
+    return this.msg;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+  
+  public boolean getInsertShardEnd() {
+    return this.insertShardEnd;
+  }
+  
+  public boolean getDataWritten() {
+    return this.dataWritten;
+  }
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.success = in.readBoolean();
+    if (this.success) {
+       this.entryPointer = new HBQEntryPointer();
+       this.entryPointer.readFields(in);
+       this.insertShardEnd = in.readBoolean();
+       this.dataWritten = in.readBoolean();
+    } else {
+      this.msg = in.readUTF();
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeBoolean(this.success);
+    if (this.success) {
+      this.entryPointer.write(out);
+      out.writeBoolean(this.insertShardEnd);
+      out.writeBoolean(this.dataWritten);
+    } else {
+      out.writeUTF(this.msg);
+    }
+  }
+
+  @Override
+  public String toString() {
+    if (success) {
+      return Objects.toStringHelper(this)
+          .add("success", this.success)
+          .add("entryPointer", this.entryPointer.toString())
+          .add("insertShardEnd", this.insertShardEnd)
+          .add("dataWritten", this.dataWritten)
+          .toString();
+    } else {
+      return Objects.toStringHelper(this)
+          .add("success", this.success)
+          .add("failureMsg", this.msg)
+          .toString();
+    }
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueue.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueue.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueue.java	(revision 0)
@@ -0,0 +1,74 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import com.google.common.base.Objects;
+
+/**
+ * Enqueue operation request for HBase Native Queues.
+ */
+public class HBQEnqueue extends HBQOperation {
+
+  private byte [] queueData;
+  
+  private HBReadPointer readPointer;
+  
+  private HBQShardConfig shardConfig;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQEnqueue() {
+    super();
+  }
+  
+  public HBQEnqueue(final byte [] queueName, final byte [] queueData,
+      final HBReadPointer readPointer, final HBQShardConfig shardConfig) {
+    super(queueName);
+    this.queueData = queueData;
+    this.readPointer = readPointer;
+    this.shardConfig = shardConfig;
+  }
+
+  public byte [] getQueueData() {
+    return this.queueData;
+  }
+
+  public HBReadPointer getReadPointer() {
+    return this.readPointer;
+  }
+
+  public HBQShardConfig getShardConfig() {
+    return this.shardConfig;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    int len = in.readInt();
+    this.queueData = new byte[len];
+    in.readFully(this.queueData);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.shardConfig = new HBQShardConfig();
+    this.shardConfig.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    out.writeInt(this.queueData.length);
+    out.write(this.queueData);
+    this.readPointer.write(out);
+    this.shardConfig.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("readPointer", this.readPointer)
+        .add("data.length", this.queueData.length)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQMetaOperation.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQMetaOperation.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQMetaOperation.java	(revision 0)
@@ -0,0 +1,67 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Meta operations.
+ */
+public class HBQMetaOperation extends HBQOperation implements Writable {
+
+  private MetaOperationType operationType;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQMetaOperation() {
+    super();
+  }
+
+  public HBQMetaOperation(final byte [] queueName,
+      final MetaOperationType operationType) {
+    super(queueName);
+    this.operationType = operationType;
+  }
+
+  public byte[] getGroupIdRow() {
+    return generateRow(HBQConstants.GLOBAL_GROUPS_HEADER);
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.operationType = MetaOperationType.fromByte(in.readByte());
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    out.writeByte(operationType.toByte());
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("operationType", operationType.name())
+        .toString();
+  }
+
+  public static enum MetaOperationType {
+    GET_GROUP_ID, GET_QUEUE_META;
+    public static MetaOperationType fromByte(byte b) {
+      switch (b) {
+        case '0': return GET_GROUP_ID;
+        case '1': return GET_QUEUE_META;
+        default: throw new RuntimeException("Fatal serialization error");
+      }
+    }
+    public byte toByte() {
+      if (this == GET_GROUP_ID) return '0';
+      else return '1';
+    }
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQExpirationConfig.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQExpirationConfig.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQExpirationConfig.java	(revision 0)
@@ -0,0 +1,56 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Expiration configuration settings.
+ */
+public class HBQExpirationConfig implements Writable {
+
+  long maxAgeBeforeExpirationInMillis;
+  long maxAgeBeforeSemiAckedToAcked;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQExpirationConfig() {}
+  
+  public HBQExpirationConfig(final long maxAgeBeforeExpirationInMillis,
+      final long maxAgeBeforeSemiAckedToAcked) {
+    this.maxAgeBeforeExpirationInMillis = maxAgeBeforeExpirationInMillis;
+    this.maxAgeBeforeSemiAckedToAcked = maxAgeBeforeSemiAckedToAcked;
+  }
+
+  public long getMaxAgeBeforeExpirationInMillis() {
+    return this.maxAgeBeforeExpirationInMillis;
+  }
+
+  public long getMaxAgeBeforeSemiAckedToAcked() {
+    return this.maxAgeBeforeSemiAckedToAcked;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.maxAgeBeforeExpirationInMillis = in.readLong();
+    this.maxAgeBeforeSemiAckedToAcked = in.readLong();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.maxAgeBeforeExpirationInMillis);
+    out.writeLong(this.maxAgeBeforeSemiAckedToAcked);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("maxAgeBeforeExpirationInMillis",
+            this.maxAgeBeforeExpirationInMillis)
+        .add("maxAgeBeforeSemiAckedToAcked", this.maxAgeBeforeSemiAckedToAcked)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQConstants.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQConstants.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQConstants.java	(revision 0)
@@ -0,0 +1,33 @@
+package com.continuuity.hbase.ttqueue;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class HBQConstants {
+
+  public static final byte [] HBQ_FAMILY = Bytes.toBytes("f");
+
+  // Row headers
+  public static final byte [] GLOBAL_META_HEADER = bytes((byte)10);
+  public static final byte [] GLOBAL_GROUPS_HEADER = bytes((byte)30);
+  public static final byte [] GLOBAL_DATA_HEADER = bytes((byte)50);
+
+  // Columns for row = GLOBAL_ENTRY_HEADER
+  public static final byte [] GLOBAL_ENTRYID_COUNTER = bytes((byte)10);
+  public static final byte [] GLOBAL_ENTRYID_WRITEPOINTER_COUNTER = bytes((byte)20);
+  public static final byte [] GLOBAL_SHARD_META = bytes((byte)30);
+
+  // Columns for row = GLOBAL_GROUPS_HEADER
+  public static final byte [] GROUP_ID_GEN = bytes((byte)10);
+  
+  // Columns for row = GLOBAL_GROUPS_HEADER+GROUP_ID
+  public static final byte [] GROUP_STATE = bytes((byte)20);
+
+  // Columns for row = GLOBAL_DATA_HEADER
+  public static final byte [] ENTRY_META = bytes((byte)10);
+  public static final byte [] ENTRY_GROUP_META = bytes((byte)20);
+  public static final byte [] ENTRY_DATA = bytes((byte)30);
+
+  private static byte [] bytes(byte b) {
+    return new byte [] { b };
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidateResult.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidateResult.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidateResult.java	(revision 0)
@@ -0,0 +1,97 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * A pointer for a single queue entry.
+ */
+public class HBQInvalidateResult implements Writable {
+
+  private boolean success;
+  private String msg;
+  private HBQEntryPointer entryPointer;
+  private byte [] queueData;
+  
+  /** Empty constructor for Writable (do not use) */
+  public HBQInvalidateResult() {}
+  
+  /**
+   * Failed dequeue constructor.  Just specify message.
+   * @param msg
+   */
+  public HBQInvalidateResult(final String msg) {
+    this.success = false;
+    this.msg = msg;
+  }
+
+  public HBQInvalidateResult(final HBQEntryPointer entryPointer,
+      final byte [] queueData) {
+    this.success = true;
+    this.entryPointer = entryPointer;
+    this.queueData = queueData;
+  }
+  
+  public boolean isSuccess() {
+    return this.success;
+  }
+
+  public String getFailureMessage() {
+    return this.msg;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+  
+  public byte [] getData() {
+    return this.queueData;
+  }
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.success = in.readBoolean();
+    if (this.success) {
+       int len = in.readInt();
+       this.queueData = new byte[len];
+       in.readFully(queueData);
+       this.entryPointer = new HBQEntryPointer();
+       this.entryPointer.readFields(in);
+    } else {
+      this.msg = in.readUTF();
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeBoolean(this.success);
+    if (this.success) {
+      out.writeInt(this.queueData.length);
+      out.write(this.queueData);
+      this.entryPointer.write(out);
+    } else {
+      out.writeUTF(this.msg);
+    }
+  }
+
+  @Override
+  public String toString() {
+    if (success) {
+      return Objects.toStringHelper(this)
+          .add("success", this.success)
+          .add("entryPointer", this.entryPointer.toString())
+          .add("data.length", this.queueData.length)
+          .toString();
+    } else {
+      return Objects.toStringHelper(this)
+          .add("success", this.success)
+          .add("failureMsg", this.msg)
+          .toString();
+    }
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidate.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidate.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidate.java	(revision 0)
@@ -0,0 +1,62 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import com.google.common.base.Objects;
+
+/**
+ * Invalidate operation request for HBase Native Queues.
+ */
+public class HBQInvalidate extends HBQOperation {
+
+  private HBQEntryPointer entryPointer;
+  
+  private HBReadPointer readPointer;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQInvalidate() {
+    super();
+  }
+  
+  public HBQInvalidate(final byte [] queueName,
+      final HBQEntryPointer entryPointer, final HBReadPointer readPointer) {
+    super(queueName);
+    this.entryPointer = entryPointer;
+    this.readPointer = readPointer;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.entryPointer = new HBQEntryPointer();
+    this.entryPointer.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.entryPointer.write(out);
+    this.readPointer.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("entryPointer", this.entryPointer)
+        .add("readPointer", this.readPointer)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQShardConfig.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQShardConfig.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQShardConfig.java	(revision 0)
@@ -0,0 +1,55 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Shard configuration settings.
+ */
+public class HBQShardConfig implements Writable {
+
+  long maxEntriesPerShard;
+  long maxBytesPerShard;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQShardConfig() {}
+  
+  public HBQShardConfig(final long maxEntriesPerShard,
+      final long maxBytesPerShard) {
+    this.maxEntriesPerShard = maxEntriesPerShard;
+    this.maxBytesPerShard = maxBytesPerShard;
+  }
+
+  public long getMaxEntriesPerShard() {
+    return this.maxEntriesPerShard;
+  }
+
+  public long getMaxBytesPerShard() {
+    return this.maxBytesPerShard;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.maxEntriesPerShard = in.readLong();
+    this.maxBytesPerShard = in.readLong();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.maxEntriesPerShard);
+    out.writeLong(this.maxBytesPerShard);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("maxEntriesPerShard", this.maxEntriesPerShard)
+        .add("maxBytesPerShard", this.maxBytesPerShard)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBReadPointer.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBReadPointer.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBReadPointer.java	(revision 0)
@@ -0,0 +1,78 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.hadoop.io.Writable;
+
+public class HBReadPointer implements Writable {
+
+  private long writePointer;
+  
+  private long readPointer;
+  
+  private Set<Long> readExcludes;
+
+  /** Writable constructor (do not use) */
+  public HBReadPointer() {}
+  
+  public HBReadPointer(long writePointer, long readPointer) {
+    this(writePointer, readPointer, new HashSet<Long>());
+  }
+  
+  public HBReadPointer(long writePointer, long readPointer,
+      Set<Long> readExcludes) {
+    this.writePointer = writePointer;
+    this.readPointer = readPointer;
+    this.readExcludes = readExcludes;
+  }
+  
+  // Note: This is the com.continuuity.data.table.ReadPointer interface but not
+  // actually defined here as we don't want HBase to depend on data-fabric
+  // (and we don't need this to actually support that interface)
+
+  public boolean isVisible(long txid) {
+    if (txid == this.writePointer) return true;
+    if (txid > this.readPointer) return false;
+    return !isExcluded(txid);
+  }
+
+  private boolean isExcluded(long txid) {
+    return this.readExcludes != null && this.readExcludes.contains(txid);
+  }
+
+  public long getReadPointer() {
+    return this.readPointer;
+  }
+  
+  public long getWritePointer() {
+    return this.writePointer;
+  }
+  
+  // Writable
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.writePointer = in.readLong();
+    this.readPointer = in.readLong();
+    int n = in.readInt();
+    this.readExcludes = new HashSet<Long>(n < 4 ? 4 : n);
+    for (int i=0; i<n; i++) this.readExcludes.add(in.readLong());
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.writePointer);
+    out.writeLong(this.readPointer);
+    if (this.readExcludes == null || this.readExcludes.isEmpty()) {
+      out.writeInt(0);
+    } else {
+      out.writeInt(this.readExcludes.size());
+      for (Long excluded : readExcludes) out.writeLong(excluded);
+    }
+  }
+
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQQueueMeta.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQQueueMeta.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQQueueMeta.java	(revision 0)
@@ -0,0 +1,73 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.continuuity.hbase.ttqueue.internal.GroupState;
+import com.google.common.base.Objects;
+
+/**
+ * Queue meta data.
+ */
+public class HBQQueueMeta implements Writable {
+
+  long globalHeadPointer;
+  long currentWritePointer;
+  GroupState [] groups;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQQueueMeta() {}
+
+  public HBQQueueMeta(long globalHeadPointer, long currentWritePointer,
+      GroupState[] groups) {
+    this.globalHeadPointer = globalHeadPointer;
+    this.currentWritePointer = currentWritePointer;
+    this.groups = groups;
+  }
+
+  public long getGlobalHeadPointer() {
+    return this.globalHeadPointer;
+  }
+
+  public long getCurrentWritePointer() {
+    return this.currentWritePointer;
+  }
+
+  public GroupState [] getGroups() {
+    return this.groups;
+  }
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.globalHeadPointer = in.readLong();
+    this.currentWritePointer = in.readLong();
+    int len = in.readInt();
+    this.groups = new GroupState[len];
+    for (int i=0; i<len; i++) {
+      this.groups[i] = new GroupState();
+      this.groups[i].readFields(in);
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.globalHeadPointer);
+    out.writeLong(this.currentWritePointer);
+    out.writeInt(this.groups.length);
+    for (GroupState groupState : groups) {
+      groupState.write(out);
+    }
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("globalHeadPointer", this.globalHeadPointer)
+        .add("currentWritePointer", this.currentWritePointer)
+        .add("groups", this.groups)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQConsumer.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQConsumer.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQConsumer.java	(revision 0)
@@ -0,0 +1,64 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Information about a single consumer from a single group.
+ */
+public class HBQConsumer implements Writable {
+
+  private int instanceId;
+  private long groupId;
+  private int groupSize;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQConsumer() {}
+  
+  public HBQConsumer(final int instanceId, final long groupId,
+      final int groupSize) {
+    this.instanceId = instanceId;
+    this.groupId = groupId;
+    this.groupSize = groupSize;
+  }
+
+  public int getInstanceId() {
+    return this.instanceId;
+  }
+  
+  public long getGroupId() {
+    return this.groupId;
+  }
+
+  public int getGroupSize() {
+    return this.groupSize;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.instanceId = in.readInt();
+    this.groupId = in.readLong();
+    this.groupSize = in.readInt();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeInt(this.instanceId);
+    out.writeLong(this.groupId);
+    out.writeInt(this.groupSize);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("instanceId", this.instanceId)
+        .add("groupId", this.groupId)
+        .add("groupSize", this.groupSize)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQDequeueResult.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQDequeueResult.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQDequeueResult.java	(revision 0)
@@ -0,0 +1,403 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+import com.continuuity.hbase.ttqueue.internal.GroupState;
+import com.google.common.base.Objects;
+
+/**
+ * The result from a dequeue operation.
+ * <p>
+ * TODO: Break this class into a second class (HBQDequeueInternalResult)
+ */
+public class HBQDequeueResult implements Writable {
+
+  // Returned values
+  private HBQDequeueStatus status = null;
+  private String msg = null;
+  private HBQEntryPointer entryPointer = null;
+  private byte [] queueData = null;
+  
+  // Variables used when dequeue request jumps regions/servers
+  private boolean groupToBeUpdated = false;
+  private boolean dataFindDone = false;
+  private HBQDequeue dequeue = null;
+  private HBQEntryPointer curPointer = null;
+  private GroupState currentGroupState = null;
+  
+  /** Empty constructor for Writable (do not use) */
+  public HBQDequeueResult() {}
+  
+  /**
+   * Failed dequeue constructor.  Just specify message.
+   * @param msg
+   */
+  public HBQDequeueResult(final String msg) {
+    this.status = HBQDequeueStatus.FAILURE;
+    this.msg = msg;
+  }
+
+  /**
+   * Successful and completed dequeue.  Contains final status, and if not empty,
+   * an entry pointer and data.
+   * @param status
+   * @param entryPointer
+   * @param queueData
+   */
+  public HBQDequeueResult(final HBQDequeueStatus status,
+      final HBQEntryPointer entryPointer, final byte [] queueData) {
+    this.status = status;
+    this.entryPointer = entryPointer;
+    this.queueData = queueData;
+  }
+  
+  /**
+   * Constructor for when queue is empty.
+   * @param status 
+   */
+  public HBQDequeueResult(HBQDequeueStatus status) {
+    assert(status == HBQDequeueStatus.EMPTY);
+    this.status = status;
+  }
+
+  /**
+   * Constructor for an in-progress dequeue that may or may not be completed
+   * because it is not yet known whether a group update is needed.  This
+   * constructor will determine if the specified initial group state is
+   * different from the current, and if so, will set it to be updated and the
+   * request will stay in progress
+   * 
+   * The data entry has already been found, or dequeue should return empty.
+   * 
+   * If dequeue should return empty, use null for queue data and entry pointer.
+   * 
+   * @param dequeue the original dequeue operation
+   * @param entryPointer pointer to the found and returned entry, null for empty
+   * @param queueData data of the found and returned entry, null for empty
+   * @param initialGroupState the initial group state
+   * @param currentGroupState the current group state
+   */
+  public HBQDequeueResult(HBQDequeue dequeue, HBQEntryPointer entryPointer,
+      byte [] queueData, GroupState initialGroupState,
+      GroupState currentGroupState) {
+    if (initialGroupState == null || currentGroupState == null ||
+        initialGroupState.equals(currentGroupState)) {
+      this.status = queueData == null ?
+          HBQDequeueStatus.EMPTY : HBQDequeueStatus.SUCCESS;
+      this.entryPointer = entryPointer;
+      this.queueData = queueData;
+    } else {
+      this.dequeue = dequeue;
+      this.entryPointer = entryPointer;
+      this.queueData = queueData;
+      this.currentGroupState = currentGroupState;
+      this.groupToBeUpdated = true;
+      this.dataFindDone = true;
+    }
+  }
+
+  /**
+   * Constructor for an in-progress dequeue that is not yet completed because
+   * the data entry has not yet been found and, if specified, the group state
+   * must be updated.  If specified group state is null, no group update will
+   * be done.
+   * 
+   * This should not be returned to the client before further processing.
+   * 
+   * @param dequeue the original dequeue operation
+   * @param curPointer pointer to the current data entry (next entry to check)
+   * @param currentGroupState the current group state
+   */
+  public HBQDequeueResult(HBQDequeue dequeue, HBQEntryPointer curPointer,
+      GroupState currentGroupState) {
+    this.dequeue = dequeue;
+    this.curPointer = curPointer;
+    this.currentGroupState = currentGroupState;
+    this.groupToBeUpdated = (currentGroupState != null);
+    this.dataFindDone = false;
+  }
+
+  /**
+   * Constructor for an in-progress dequeue that is not yet completed because
+   * the data has not yet been found.  No update to the group state is
+   * necessary.
+   * 
+   * This should not be returned to the client before further processing.
+   * 
+   * @param dequeue
+   * @param curPointer
+   */
+  public HBQDequeueResult(HBQDequeue dequeue, HBQEntryPointer curPointer) {
+    this.dequeue = dequeue;
+    this.curPointer = curPointer;
+    this.groupToBeUpdated = false;
+    this.dataFindDone = false;
+  }
+
+  /**
+   * Constructor for an in-progress dequeue that is not yet completed because
+   * the data has not yet been found.  The group state will be set to be updated
+   * if the specified initial group state is different from the specified
+   * current group state.  If either group state is null, it will also not be
+   * set to be updated.
+   * <p>
+   * This should not be returned to the client before further processing.
+   * 
+   * @param dequeue
+   * @param entryPointer
+   * @param initialGroupState
+   * @param currentGroupState
+   */
+  public HBQDequeueResult(HBQDequeue dequeue, HBQEntryPointer entryPointer,
+      GroupState initialGroupState, GroupState currentGroupState) {
+    this.dequeue = dequeue;
+    this.entryPointer = entryPointer;
+    this.dataFindDone = false;
+    if (initialGroupState == null || currentGroupState == null ||
+        initialGroupState.equals(currentGroupState)) {
+      this.groupToBeUpdated = false;
+    } else {
+      this.currentGroupState = currentGroupState;
+      this.groupToBeUpdated = true;
+    }
+  }
+
+  public String getFailureMessage() {
+    return this.msg;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  /**
+   * Checks whether search for data entry is done (either have an entry or
+   * empty).
+   * @return true if a data entry search is done, false if not
+   */
+  public boolean dataEntrySearchDone() {
+    return this.dataFindDone;
+  }
+  
+  public byte [] getData() {
+    return this.queueData;
+  }
+  
+  public HBQDequeueStatus getStatus() {
+    return this.status;
+  }
+  
+  public boolean isSuccess() {
+    return this.status == HBQDequeueStatus.SUCCESS;
+  }
+
+  public boolean isFailure() {
+    return this.status == HBQDequeueStatus.FAILURE;
+  }
+
+  public boolean isEmpty() {
+    return this.status == HBQDequeueStatus.EMPTY;
+  }
+
+  public boolean isInProgress() {
+    return this.status == HBQDequeueStatus.IN_PROGRESS;
+  }
+
+  /**
+   * Checks whether during an in-progress dequeue, the group state needs to
+   * be updated.  If this returns true, the group state should be updated to the
+   * current state available in {@link #getCurrentGroupState()}.
+   *  
+   * @return true if group state should be updated to final state, false if not
+   */
+  public boolean groupStateNeedsToBeUpdated() {
+    return this.groupToBeUpdated;
+  }
+
+  public void updateGroupState(GroupState updatedGroupState) {
+    this.currentGroupState = updatedGroupState;
+    this.groupToBeUpdated = true;
+  }
+
+  /**
+   * The group state was set to be updated and has now been completed.
+   */
+  public void groupStateUpdated() {
+    this.groupToBeUpdated = false;
+    if (dataEntrySearchDone()) {
+      if (this.queueData == null) {
+        this.status = HBQDequeueStatus.EMPTY;
+      } else {
+        this.status = HBQDequeueStatus.SUCCESS;
+      }
+    }
+  }
+
+  public GroupState getCurrentGroupState() {
+    return this.currentGroupState;
+  }
+
+  public HBQEntryPointer getCurrentEntryPointer() {
+    return this.curPointer;
+  }
+
+  public HBQDequeue getDequeue() {
+    return this.dequeue;
+  }
+
+  /**
+   * Determines and returns the next row to visit for an in-progress dequeue.
+   * 
+   * If data entry search is complete, then the group needs to be updated, and
+   * the next row will be the group meta row.
+   * 
+   * If the data entry search is not complete, the next row will be the
+   * returned shard row.
+   * 
+   * @return next row to be processed for in-progress dequeue
+   */
+  public byte[] getNextRow() {
+    if (dataEntrySearchDone()) {
+      return getDequeue().getGroupMetaRow();
+    } else {
+      return getDequeue().getDataRow(Bytes.toBytes(
+          getCurrentEntryPointer().getShardId()));
+    }
+  }
+
+  public static enum HBQDequeueStatus {
+    SUCCESS, EMPTY, FAILURE, IN_PROGRESS;
+    public byte getByte() {
+      switch (this) {
+        case SUCCESS: return 1;
+        case EMPTY:   return 2;
+        case FAILURE: return 3;
+        case IN_PROGRESS: return 4;
+        default: throw new RuntimeException("Serialization error");
+      }
+    }
+    public static HBQDequeueStatus fromByte(byte b) {
+      switch (b) {
+        case 1: return SUCCESS;
+        case 2: return EMPTY;
+        case 3: return FAILURE;
+        case 4: return IN_PROGRESS;
+        default: throw new RuntimeException("Deserialization error");
+      }
+    }
+  }
+  
+  // in progress field order:
+  // groupUpdate, dataFindDone, dequeue, curPointer, currentGroup,
+  // entryPointer, queueData
+  // all fields are first represented by a boolean representing them as null
+  // except queue data which has a length integer
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.status = HBQDequeueStatus.fromByte(in.readByte());
+    if (isSuccess()) {
+      this.entryPointer = new HBQEntryPointer();
+      this.entryPointer.readFields(in);
+      int len = in.readInt();
+      this.queueData = new byte[len];
+      in.readFully(queueData);
+    } else if (isFailure()) {
+      this.msg = in.readUTF();
+    } else if (isInProgress()) {
+      this.groupToBeUpdated = in.readBoolean();
+      this.dataFindDone = in.readBoolean();
+      if (in.readBoolean()) {
+        this.dequeue = new HBQDequeue();
+        this.dequeue.readFields(in);
+      }
+      if (in.readBoolean()) {
+        this.curPointer = new HBQEntryPointer();
+        this.curPointer.readFields(in);
+      }
+      if (in.readBoolean()) {
+        this.currentGroupState = new GroupState();
+        this.currentGroupState.readFields(in);
+      }
+      if (in.readBoolean()) {
+        this.entryPointer = new HBQEntryPointer();
+        this.entryPointer.readFields(in);
+      }
+      int len = in.readInt();
+      if (len > 0) {
+        this.queueData = new byte[len];
+        in.readFully(queueData);
+      }
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeByte(this.status.getByte());
+    if (isSuccess()) {
+      this.entryPointer.write(out);
+      out.writeInt(this.queueData.length);
+      out.write(this.queueData);
+    } else if (isFailure()) {
+      out.writeUTF(this.msg);
+    } else if (isInProgress()) {
+      out.writeBoolean(this.groupToBeUpdated);
+      out.writeBoolean(this.dataFindDone);
+      out.writeBoolean(this.dequeue != null);
+      if (this.dequeue != null) {
+        this.dequeue.write(out); 
+      }
+      out.writeBoolean(this.curPointer != null);
+      if (this.curPointer != null) {
+        this.curPointer.write(out); 
+      }
+      out.writeBoolean(this.currentGroupState != null);
+      if (this.currentGroupState != null) {
+        this.currentGroupState.write(out); 
+      }
+      out.writeBoolean(this.entryPointer != null);
+      if (this.entryPointer != null) {
+        this.entryPointer.write(out); 
+      }
+      out.writeInt(this.queueData == null ? 0 : this.queueData.length);
+      if (this.queueData != null) {
+        out.write(this.queueData);
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    if (isSuccess()) {
+      return Objects.toStringHelper(this)
+          .add("status", this.status)
+          .add("entryPointer", this.entryPointer.toString())
+          .add("data.length", this.queueData.length)
+          .toString();
+    } else if (isFailure()) {
+      return Objects.toStringHelper(this)
+          .add("status", this.status)
+          .add("failureMsg", this.msg)
+          .toString();
+    } else if (isInProgress()) {
+      return Objects.toStringHelper(this)
+          .add("status", this.status)
+          .add("dequeue", this.dequeue)
+          .add("curPointer", this.curPointer)
+          .add("currentGroupState", this.currentGroupState)
+          .add("entryPointer", this.entryPointer)
+          .add("data.length", this.queueData == null ? "null" :
+              "" + this.queueData.length)
+          .toString();
+    } else {
+      return Objects.toStringHelper(this)
+          .add("status", this.status)
+          .toString();
+    }
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQDequeue.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQDequeue.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQDequeue.java	(revision 0)
@@ -0,0 +1,97 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+import com.google.common.base.Objects;
+
+/**
+ * Dequeue operation request for HBase Native Queues.
+ */
+public class HBQDequeue extends HBQOperation {
+  
+  private HBReadPointer readPointer;
+  
+  private HBQConsumer consumer;
+  
+  private HBQConfig config;
+  
+  private HBQExpirationConfig expirationConfig;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQDequeue() {
+    super();
+  }
+  
+  public HBQDequeue(final byte [] queueName,
+      final HBQConsumer consumer, final HBQConfig config,
+      final HBReadPointer readPointer,
+      final HBQExpirationConfig expirationConfig) {
+    super(queueName);
+    this.readPointer = readPointer;
+    this.consumer = consumer;
+    this.config = config;
+    this.expirationConfig = expirationConfig;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  public HBQConsumer getConsumer() {
+    return this.consumer;
+  }
+
+  public HBQConfig getConfig() {
+    return this.config;
+  }
+
+  public HBQExpirationConfig getExpirationConfig() {
+    return this.expirationConfig;
+  }
+
+  /**
+   * Returns the group metadata row for the queue of this operation.
+   * @return group meta row for this queue
+   */
+  public byte [] getGroupMetaRow() {
+    return generateRow(HBQConstants.GLOBAL_GROUPS_HEADER,
+        Bytes.toBytes(consumer.getGroupId()));
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.consumer = new HBQConsumer();
+    this.consumer.readFields(in);
+    this.config = new HBQConfig();
+    this.config.readFields(in);
+    this.expirationConfig = new HBQExpirationConfig();
+    this.expirationConfig.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.readPointer.write(out);
+    this.consumer.write(out);
+    this.config.write(out);
+    this.expirationConfig.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("readPointer", this.readPointer)
+        .add("queueConsumer", this.consumer)
+        .add("queueConfig", this.config)
+        .add("queueExpirationConfig", this.expirationConfig)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQAck.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQAck.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQAck.java	(revision 0)
@@ -0,0 +1,74 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import com.google.common.base.Objects;
+
+/**
+ * Ack operation request for HBase Native Queues.
+ */
+public class HBQAck extends HBQOperation {
+
+  private HBQEntryPointer entryPointer;
+
+  private HBReadPointer readPointer;
+
+  private HBQConsumer consumer;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQAck() {
+    super();
+  }
+
+  public HBQAck(final byte [] queueName,
+      final HBQConsumer consumer, final HBQEntryPointer entryPointer,
+      final HBReadPointer readPointer) {
+    super(queueName);
+    this.entryPointer = entryPointer;
+    this.readPointer = readPointer;
+    this.consumer = consumer;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  public HBQConsumer getConsumer() {
+    return this.consumer;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.entryPointer = new HBQEntryPointer();
+    this.entryPointer.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.consumer = new HBQConsumer();
+    this.consumer.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.entryPointer.write(out);
+    this.readPointer.write(out);
+    this.consumer.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("entryPointer", this.entryPointer)
+        .add("readPointer", this.readPointer)
+        .add("queueConsumer", this.consumer)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQOperation.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQOperation.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQOperation.java	(revision 0)
@@ -0,0 +1,76 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+public class HBQOperation implements Writable {
+
+  protected byte [] queueName;
+  
+  protected boolean writeToWAL = true;
+
+  protected HBQOperation() {}
+  
+  protected HBQOperation(final byte [] queueName) {
+    this.queueName = queueName;
+  }
+
+  public byte [] getQueueName() {
+    return this.queueName;
+  }
+
+  /**
+   * Returns the primary metadata row for the queue of this operation.
+   * @return meta row for this queue
+   */
+  public byte [] getMetaRow() {
+    return generateRow(HBQConstants.GLOBAL_META_HEADER);
+  }
+
+  public boolean getWriteToWAL() {
+    return this.writeToWAL;
+  }
+
+  public void setWriteToWAL(boolean writeToWAL) {
+    this.writeToWAL = writeToWAL;
+  }
+
+  /**
+   * Returns the primary metadata row for the queue of this operation.
+   * @param shardid 
+   * @return row key of the data row for this queue operation and the specified
+   *         shard
+   */
+  public byte [] getDataRow(byte [] shardid) {
+    return generateRow(HBQConstants.GLOBAL_DATA_HEADER, shardid);
+  }
+
+  protected byte [] generateRow(byte [] header) {
+    return Bytes.add(this.queueName, header);
+  }
+
+  protected byte [] generateRow(byte [] headerOne, byte [] headerTwo) {
+    return Bytes.add(this.queueName, headerOne, headerTwo);
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    int len = in.readInt();
+    this.queueName = new byte[len];
+    in.readFully(this.queueName);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeInt(this.queueName.length);
+    out.write(this.queueName);
+  }
+
+  public static enum HBQOperationType {
+    ENQUEUE, DEQUEUE, INVALIDATE, ACK, UNACK, FINALIZE
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQFinalize.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQFinalize.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQFinalize.java	(revision 0)
@@ -0,0 +1,90 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import com.google.common.base.Objects;
+
+/**
+ * Ack operation request for HBase Native Queues.
+ */
+public class HBQFinalize extends HBQOperation {
+
+  private HBQEntryPointer entryPointer;
+
+  private HBQConsumer consumer;
+  
+  private int totalNumGroups;
+  
+  private HBReadPointer readPointer;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQFinalize() {
+    super();
+  }
+  
+  public HBQFinalize(final byte [] queueName,
+      final HBQConsumer consumer, final HBQEntryPointer entryPointer,
+      final HBReadPointer readPointer) {
+    this(queueName, consumer, entryPointer, readPointer, 0);
+  }
+  
+  public HBQFinalize(final byte [] queueName,
+      final HBQConsumer consumer, final HBQEntryPointer entryPointer,
+      final HBReadPointer readPointer, int totalNumGroups) {
+    super(queueName);
+    this.entryPointer = entryPointer;
+    this.readPointer = readPointer;
+    this.consumer = consumer;
+    this.totalNumGroups = totalNumGroups;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  public HBQConsumer getConsumer() {
+    return this.consumer;
+  }
+
+  public int getTotalNumGroups() {
+    return this.totalNumGroups;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.entryPointer = new HBQEntryPointer();
+    this.entryPointer.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.consumer = new HBQConsumer();
+    this.consumer.readFields(in);
+    this.totalNumGroups = in.readInt();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.entryPointer.write(out);
+    this.readPointer.write(out);
+    this.consumer.write(out);
+    out.writeInt(this.totalNumGroups);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("entryPointer", this.entryPointer)
+        .add("readPointer", this.readPointer)
+        .add("consumer", this.consumer)
+        .add("totalNumGroups", this.totalNumGroups)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQUnack.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQUnack.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQUnack.java	(revision 0)
@@ -0,0 +1,74 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import com.google.common.base.Objects;
+
+/**
+ * Ack operation request for HBase Native Queues.
+ */
+public class HBQUnack extends HBQOperation {
+
+  private HBQEntryPointer entryPointer;
+  
+  private HBReadPointer readPointer;
+
+  private HBQConsumer consumer;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQUnack() {
+    super();
+  }
+  
+  public HBQUnack(final byte [] queueName,
+      final HBQConsumer consumer, final HBQEntryPointer entryPointer,
+      final HBReadPointer readPointer) {
+    super(queueName);
+    this.entryPointer = entryPointer;
+    this.readPointer = readPointer;
+    this.consumer = consumer;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  public HBQConsumer getConsumer() {
+    return this.consumer;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.entryPointer = new HBQEntryPointer();
+    this.entryPointer.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.consumer = new HBQConsumer();
+    this.consumer.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.entryPointer.write(out);
+    this.readPointer.write(out);
+    this.consumer.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("entryPointer", this.entryPointer)
+        .add("readPointer", this.readPointer)
+        .add("queueConsumer", this.consumer)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/internal/EntryGroupMeta.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/internal/EntryGroupMeta.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/internal/EntryGroupMeta.java	(revision 0)
@@ -0,0 +1,121 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+import com.google.common.base.Objects;
+
+/**
+ * Meta data for a group about a queue entry.
+ */
+public class EntryGroupMeta {
+
+  private final EntryGroupState state;
+  private final long timestamp;
+  private final int instanceId;
+  
+  public EntryGroupMeta(final EntryGroupState state, final long timestamp,
+      final int instanceId) {
+    this.state = state;
+    this.timestamp = timestamp;
+    this.instanceId = instanceId;
+  }
+
+  public boolean isAvailable() {
+    return state == EntryGroupState.AVAILABLE;
+  }
+  
+  public boolean isAckedOrSemiAcked() {
+    return state == EntryGroupState.SEMI_ACKED ||
+        state == EntryGroupState.ACKED;
+  }
+
+  public boolean isSemiAcked() {
+    return state == EntryGroupState.SEMI_ACKED;
+  }
+  
+  public boolean isAcked() {
+    return state == EntryGroupState.ACKED;
+  }
+  
+  public boolean isDequeued() {
+    return state == EntryGroupState.DEQUEUED;
+  }
+  
+  public EntryGroupState getState() {
+    return this.state;
+  }
+
+  public long getTimestamp() {
+    return this.timestamp;
+  }
+  
+  public int getInstanceId() {
+    return this.instanceId;
+  }
+  
+  public byte [] getBytes() {
+    return Bytes.add(this.state.getBytes(), Bytes.toBytes(timestamp),
+        Bytes.toBytes(instanceId));
+  }
+  
+  public static EntryGroupMeta fromBytes(byte [] bytes) {
+    return new EntryGroupMeta(EntryGroupState.fromBytes(new byte[] {bytes[0]}),
+        Bytes.toLong(bytes, 1), Bytes.toInt(bytes, 9));
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (o == null) return false;
+    if (!(o instanceof EntryGroupMeta)) return false;
+    EntryGroupMeta egm = (EntryGroupMeta)o;
+    if (egm.getState() != this.state) return false;
+    if (egm.getInstanceId() != this.instanceId) return false;
+    if (egm.getTimestamp() != this.timestamp) return false;
+    return true;
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("state", this.state)
+        .add("timestamp", this.timestamp)
+        .add("instanceId", this.instanceId)
+        .toString();
+  }
+  
+  public static enum EntryGroupState {
+    AVAILABLE, SEMI_ACKED, ACKED, DEQUEUED;
+    
+    private static final byte [] AVAILABLE_BYTES = new byte [] { 0 };
+    private static final byte [] SEMI_ACKED_BYTES = new byte [] { 1 };
+    private static final byte [] ACKED_BYTES = new byte [] { 2 };
+    private static final byte [] DEQUEUED_BYTES = new byte [] { 3 };
+    
+    public byte [] getBytes() {
+      switch (this) {
+        case AVAILABLE: return AVAILABLE_BYTES;
+        case SEMI_ACKED:return SEMI_ACKED_BYTES;
+        case ACKED:     return ACKED_BYTES;
+        case DEQUEUED:  return DEQUEUED_BYTES;
+      }
+      return null;
+    }
+    
+    public static EntryGroupState fromBytes(byte [] bytes) {
+      if (bytes.length == 1) {
+        if (bytes[0] == AVAILABLE_BYTES[0]) return AVAILABLE;
+        if (bytes[0] == SEMI_ACKED_BYTES[0]) return SEMI_ACKED;
+        if (bytes[0] == ACKED_BYTES[0]) return ACKED;
+        if (bytes[0] == DEQUEUED_BYTES[0]) return DEQUEUED;
+      }
+      throw new RuntimeException("Invalid deserialization of EntryGroupState");
+    }
+
+    @Override
+    public String toString() {
+      return Objects.toStringHelper(this)
+          .add("state", this.name())
+          .toString();
+    }
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/internal/EntryMeta.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/internal/EntryMeta.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/internal/EntryMeta.java	(revision 0)
@@ -0,0 +1,83 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import com.google.common.base.Objects;
+
+
+/**
+ * Meta data about a queue entry.
+ */
+public class EntryMeta {
+
+  private final EntryState state;
+
+  public EntryMeta(final EntryState state) {
+    this.state = state;
+  }
+
+  public boolean isValid() {
+    return this.state == EntryState.VALID;
+  }
+
+  public boolean isInvalid() {
+    return this.state == EntryState.INVALID;
+  }
+
+  public boolean isEndOfShard() {
+    return this.state == EntryState.SHARD_END;
+  }
+
+  public boolean isEvicted() {
+    return this.state == EntryState.EVICTED;
+  }
+
+  public byte [] getBytes() {
+    return this.state.getBytes();
+  }
+
+  public static EntryMeta fromBytes(byte [] bytes) {
+    return new EntryMeta(EntryState.fromBytes(bytes));
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("state", this.state)
+        .toString();
+  }
+
+  public static enum EntryState {
+    VALID, INVALID, SHARD_END, EVICTED;
+
+    private static final byte [] VALID_BYTES = new byte [] { 0 };
+    private static final byte [] INVALID_BYTES = new byte [] { 1 };
+    private static final byte [] SHARD_END_BYTES = new byte [] { 2 };
+    private static final byte [] EVICTED_BYTES = new byte [] { 3 };
+
+    public byte [] getBytes() {
+      switch (this) {
+        case VALID:     return VALID_BYTES;
+        case INVALID:   return INVALID_BYTES;
+        case SHARD_END: return SHARD_END_BYTES;
+        case EVICTED:   return EVICTED_BYTES;
+      }
+      throw new RuntimeException("Invalid serialization of EntryState");
+    }
+
+    public static EntryState fromBytes(byte [] bytes) {
+      if (bytes.length == 1) {
+        if (bytes[0] == VALID_BYTES[0]) return VALID;
+        if (bytes[0] == INVALID_BYTES[0]) return INVALID;
+        if (bytes[0] == SHARD_END_BYTES[0]) return SHARD_END;
+        if (bytes[0] == EVICTED_BYTES[0]) return EVICTED;
+      }
+      throw new RuntimeException("Invalid deserialization of EntryState");
+    }
+
+    @Override
+    public String toString() {
+      return Objects.toStringHelper(this)
+          .add("state", this.name())
+          .toString();
+    }
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/internal/ExecutionMode.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/internal/ExecutionMode.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/internal/ExecutionMode.java	(revision 0)
@@ -0,0 +1,38 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import com.continuuity.hbase.ttqueue.HBQConfig;
+import com.google.common.base.Objects;
+
+/**
+ * The execution mode of a {@link com.continuuity.hbase.ttqueue.HBQConsumer}s within a group.
+ */
+public enum ExecutionMode {
+  SINGLE_ENTRY,
+  MULTI_ENTRY;
+
+  private static final byte [] SINGLE_BYTES = new byte [] { 0 };
+  private static final byte [] MULTI_BYTES = new byte [] { 1 };
+
+  public byte [] getBytes() {
+    return this == SINGLE_ENTRY ? SINGLE_BYTES : MULTI_BYTES;
+  }
+
+  public static ExecutionMode fromBytes(byte [] bytes) {
+    if (bytes.length == 1) {
+      if (bytes[0] == SINGLE_BYTES[0]) return SINGLE_ENTRY;
+      if (bytes[0] == MULTI_BYTES[0]) return MULTI_ENTRY;
+    }
+    throw new RuntimeException("Invalid deserialization of ExecutionMode");
+  }
+
+  public static ExecutionMode fromQueueConfig(HBQConfig config) {
+    return config.isSingleEntry() ? SINGLE_ENTRY : MULTI_ENTRY;
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("mode", name())
+        .toString();
+  }
+}
\ No newline at end of file
Index: src/main/java/com/continuuity/hbase/ttqueue/internal/ShardMeta.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/internal/ShardMeta.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/internal/ShardMeta.java	(revision 0)
@@ -0,0 +1,53 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+import com.google.common.base.Objects;
+
+/**
+ * Meta data about the current shard.
+ */
+public class ShardMeta {
+
+  private final long shardId;
+  private final long shardBytes;
+  private final long shardEntries;
+
+  public ShardMeta(final long shardId, final long shardBytes,
+      final long shardEntries) {
+    this.shardId = shardId;
+    this.shardBytes = shardBytes;
+    this.shardEntries = shardEntries;
+  }
+
+  public long getShardId() {
+    return this.shardId;
+  }
+
+  public long getShardBytes() {
+    return this.shardBytes;
+  }
+
+  public long getShardEntries() {
+    return this.shardEntries;
+  }
+
+  public byte [] getBytes() {
+    return Bytes.add(Bytes.toBytes(this.shardId),
+        Bytes.toBytes(this.shardBytes), Bytes.toBytes(this.shardEntries));
+  }
+
+  public static ShardMeta fromBytes(byte [] bytes) {
+    return new ShardMeta(Bytes.toLong(bytes, 0), Bytes.toLong(bytes, 8),
+        Bytes.toLong(bytes, 16));
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("shardId", this.shardId)
+        .add("shardBytes", this.shardBytes)
+        .add("shardEntries", this.shardEntries)
+        .toString();
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/internal/GroupState.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/internal/GroupState.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/internal/GroupState.java	(revision 0)
@@ -0,0 +1,120 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+import com.continuuity.hbase.ttqueue.HBQEntryPointer;
+import com.google.common.base.Objects;
+
+public class GroupState implements Writable {
+
+  private int groupSize;
+  private HBQEntryPointer head;
+  private ExecutionMode mode;
+
+  /** Writable Constructor - Do not use */
+  public GroupState() {}
+
+  public GroupState(final int groupSize, final HBQEntryPointer head,
+      final ExecutionMode mode) {
+    this.groupSize = groupSize;
+    this.head = head;
+    this.mode = mode;
+  }
+
+  public GroupState(GroupState groupState, HBQEntryPointer entryPointer) {
+    this(groupState.getGroupSize(), entryPointer, groupState.getMode());
+  }
+
+  public int getGroupSize() {
+    return this.groupSize;
+  }
+
+  public HBQEntryPointer getHead() {
+    return this.head;
+  }
+
+  public ExecutionMode getMode() {
+    return this.mode;
+  }
+
+  public byte [] getBytes() {
+    return Bytes.add(
+        Bytes.toBytes(this.groupSize), // 4 bytes
+        this.head.getBytes(),          // 16 bytes
+        this.mode.getBytes());         // 1 byte
+  }
+
+  public static GroupState fromBytes(byte [] bytes) {
+    return new GroupState(Bytes.toInt(bytes),
+        new HBQEntryPointer(Bytes.toLong(bytes, 4), Bytes.toLong(bytes, 12)),
+        ExecutionMode.fromBytes(Bytes.tail(bytes, 1)));
+  }
+
+  /**
+   * Checks if group size and mode are the same.
+   * @param groupState
+   * @return
+   */
+  public boolean nonPointerConfigEquals(GroupState groupState) {
+    if (groupState.getGroupSize() != getGroupSize()) return false;
+    if (groupState.getMode() != getMode()) return false;
+    return true;
+  }
+
+  /**
+   * Checks if this group state entry pointer is greater than or equal to the
+   * specified group state.
+   * @param groupState
+   * @return
+   */
+  public boolean isGreaterThanOrEqual(GroupState groupState) {
+    // check shards first
+    if (this.head.getShardId() > groupState.head.getShardId()) return true;
+    if (this.head.getShardId() < groupState.head.getShardId()) return false;
+    // same shard, check entry id
+    if (this.head.getEntryId() >= groupState.head.getEntryId()) return true;
+    return false;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (o == null) return false;
+    if (!(o instanceof GroupState)) return false;
+    GroupState gs = (GroupState)o;
+    if (gs.getGroupSize() != this.groupSize) return false;
+    if (!gs.getHead().equals(this.head)) return false;
+    if (gs.getMode() != this.mode) return false;
+    return true;
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("groupSize", this.groupSize)
+        .add("headPointer", this.head)
+        .add("execMode", this.mode.name())
+        .toString();
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.groupSize = in.readInt();
+    this.head = new HBQEntryPointer();
+    this.head.readFields(in);
+    byte [] modeBytes = new byte[1];
+    in.readFully(modeBytes);
+    this.mode = ExecutionMode.fromBytes(modeBytes);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeInt(this.groupSize);
+    this.head.write(out);
+    out.write(this.mode.getBytes());
+  }
+}
Index: src/main/java/com/continuuity/hbase/ttqueue/HBQEntryPointer.java
===================================================================
--- src/main/java/com/continuuity/hbase/ttqueue/HBQEntryPointer.java	(revision 0)
+++ src/main/java/com/continuuity/hbase/ttqueue/HBQEntryPointer.java	(revision 0)
@@ -0,0 +1,80 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * A pointer for a single queue entry.
+ */
+public class HBQEntryPointer implements Writable {
+
+  protected long entryId;
+  protected long shardId;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQEntryPointer() {}
+  
+  public HBQEntryPointer(final long entryId, final long shardId) {
+    this.entryId = entryId;
+    this.shardId = shardId;
+  }
+
+  public long getEntryId() {
+    return this.entryId;
+  }
+
+  public long getShardId() {
+    return this.shardId;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.entryId = in.readLong();
+    this.shardId = in.readLong();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.entryId);
+    out.writeLong(this.shardId);
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    return this.entryId == ((HBQEntryPointer)o).entryId &&
+        this.shardId == ((HBQEntryPointer)o).shardId;
+  }
+
+  @Override
+  public int hashCode() {
+    return Bytes.hashCode(
+        Bytes.toBytes(entryId)) ^ Bytes.hashCode(Bytes.toBytes(shardId));
+  }
+
+  public HBQEntryPointer makeCopy() {
+    return new HBQEntryPointer(this.entryId, this.shardId);
+  }
+
+  public byte [] getBytes() {
+    return Bytes.add(Bytes.toBytes(this.entryId),
+        Bytes.toBytes(this.shardId));
+  }
+
+  public static HBQEntryPointer fromBytes(byte [] bytes) {
+    return new HBQEntryPointer(Bytes.toLong(bytes, 0), Bytes.toLong(bytes, 8));
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("entryId", this.entryId)
+        .add("shardId", this.shardId)
+        .toString();
+  }
+}
Index: src/main/resources/hbase-default.xml
===================================================================
--- src/main/resources/hbase-default.xml	(revision 1383334)
+++ src/main/resources/hbase-default.xml	(working copy)
@@ -759,7 +759,7 @@
 
   <property skipInDoc="true">
     <name>hbase.defaults.for.version</name>
-    <value>@@@VERSION@@@</value>
+    <value>0.94.1-SNAPSHOT</value>
     <description>
     This defaults file was compiled for version @@@VERSION@@@. This variable is used
     to make sure that a user doesn't have an old version of hbase-default.xml on the
@@ -768,7 +768,7 @@
   </property>
   <property>
     <name>hbase.defaults.for.version.skip</name>
-    <value>false</value>
+    <value>true</value>
     <description>
     Set to true to skip the 'hbase.defaults.for.version' check.
     Setting this to true can be useful in contexts other than
Index: src/saveVersion.sh
===================================================================
--- src/saveVersion.sh	(revision 1383334)
+++ src/saveVersion.sh	(working copy)
@@ -18,30 +18,3 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-unset LANG
-unset LC_CTYPE
-version=$1
-outputDirectory=$2
-user=`whoami`
-date=`date`
-cwd=`pwd`
-if [ -d .svn ]; then
-  revision=`svn info | sed -n -e 's/Last Changed Rev: \(.*\)/\1/p'`
-  url=`svn info | sed -n -e 's/URL: \(.*\)/\1/p'`
-elif [ -d .git ]; then
-  revision=`git log -1 --pretty=format:"%H"`
-  hostname=`hostname`
-  url="git://${hostname}${cwd}"
-else
-  revision="Unknown"
-  url="file://$cwd"
-fi
-mkdir -p "$outputDirectory/org/apache/hadoop/hbase"
-cat >"$outputDirectory/org/apache/hadoop/hbase/package-info.java" <<EOF
-/*
- * Generated by src/saveVersion.sh
- */
-@VersionAnnotation(version="$version", revision="$revision",
-                         user="$user", date="$date", url="$url")
-package org.apache.hadoop.hbase;
-EOF
Index: pom.xml
===================================================================
--- pom.xml	(revision 1383334)
+++ pom.xml	(working copy)
@@ -834,9 +834,9 @@
                              package="org.apache.hadoop.hbase.generated.regionserver"
                              webxml="${build.webapps}/regionserver/WEB-INF/web.xml"/>
 
-                <exec executable="sh">
+                <!--exec executable="sh">
                   <arg line="${basedir}/src/saveVersion.sh ${project.version} ${generated.sources}/java"/>
-                </exec>
+                </exec-->
               </target>
             </configuration>
             <goals>
