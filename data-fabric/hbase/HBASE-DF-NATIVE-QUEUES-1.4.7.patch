diff --git a/pom.xml b/pom.xml
index 2f98d04..32d6f9e 100644
--- a/pom.xml
+++ b/pom.xml
@@ -36,7 +36,7 @@
   <groupId>org.apache.hbase</groupId>
   <artifactId>hbase</artifactId>
   <packaging>jar</packaging>
-  <version>0.94.2</version>
+  <version>0.94.2.1.4.7.continuuity</version>
   <name>HBase</name>
   <description>
     HBase is the &amp;lt;a href="http://hadoop.apache.org"&amp;rt;Hadoop&lt;/a&amp;rt; database. Use it when you need
@@ -837,9 +837,9 @@
                              package="org.apache.hadoop.hbase.generated.regionserver"
                              webxml="${build.webapps}/regionserver/WEB-INF/web.xml"/>
 
-                <exec executable="sh">
+                <!--exec executable="sh">
                   <arg line="${basedir}/src/saveVersion.sh ${project.version} ${generated.sources}/java"/>
-                </exec>
+                </exec-->
               </target>
             </configuration>
             <goals>
@@ -1318,7 +1318,7 @@
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
       <version>${junit.version}</version>
-      <scope>test,runtime</scope>
+      <scope>test</scope>
       <!-- FIXME: the following needs to go away once HBASE-4955 is fixed -->
       <optional>true</optional>
     </dependency>
@@ -1561,14 +1561,16 @@
       </build>
     </profile>
 
-    <!-- profile for building against Hadoop 1.0.x: This is the default. -->
+    <!-- profile for building against Hadoop 1.0.x. Activate using:
+       mvn -Dhadoop.profile=1.0 -->
     <profile>
       <id>hadoop-1.0</id>
-      <activation>
-        <property>
-          <name>!hadoop.profile</name>
-        </property>
-      </activation>
+        <activation>
+            <property>
+                <name>hadoop.profile</name>
+                <value>1.0</value>
+            </property>
+        </activation>
       <properties>
         <hadoop.version>1.0.3</hadoop.version>
         <slf4j.version>1.4.3</slf4j.version>
@@ -2040,17 +2042,15 @@
     </profile>
 
     <!--
-      profile for building against Hadoop 2.0.0-alpha. Activate using:
-       mvn -Dhadoop.profile=2.0
+      profile for building against Hadoop 2.0.0-alpha: This is the default.
     -->
     <profile>
       <id>hadoop-2.0</id>
-      <activation>
-        <property>
-          <name>hadoop.profile</name>
-          <value>2.0</value>
-        </property>
-      </activation>
+        <activation>
+            <property>
+                <name>!hadoop.profile</name>
+            </property>
+        </activation>
       <properties>
         <hadoop.version>2.0.0-alpha</hadoop.version>
         <slf4j.version>1.6.1</slf4j.version>
@@ -2098,6 +2098,9 @@
               </execution>
             </executions>
           </plugin>
+            <!-- Uncomment when need to run mapreduce unit-tests. Commented out to prevent
+                 mrapp-generated-classpath to be placed in packaged jar files -->
+<!--
           <plugin>
             <artifactId>maven-dependency-plugin</artifactId>
             <executions>
@@ -2108,15 +2111,16 @@
                   <goal>build-classpath</goal>
                 </goals>
                 <configuration>
-                  <!-- needed to run the unit test for DS to generate
+                  &lt;!&ndash; needed to run the unit test for DS to generate
                   the required classpath that is required in the env
                   of the launch container in the mini mr/yarn cluster
-                  -->
+                  &ndash;&gt;
                   <outputFile>${project.build.directory}/test-classes/mrapp-generated-classpath</outputFile>
                 </configuration>
               </execution>
             </executions>
           </plugin>
+-->
         </plugins>
       </build>
     </profile>
diff --git a/security/src/test/resources/hbase-site.xml b/security/src/test/resources/hbase-site.xml
index dcc7df2..da6ecca 100644
--- a/security/src/test/resources/hbase-site.xml
+++ b/security/src/test/resources/hbase-site.xml
@@ -23,6 +23,10 @@
 -->
 <configuration>
   <property>
+    <name>hbase.client.operation.timeout</name>
+    <value>600000</value>
+  </property>
+  <property>
     <name>hbase.regionserver.msginterval</name>
     <value>1000</value>
     <description>Interval between messages from the RegionServer to HMaster
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQAck.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQAck.java
new file mode 100644
index 0000000..2889748
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQAck.java
@@ -0,0 +1,74 @@
+package com.continuuity.hbase.ttqueue;
+
+import com.google.common.base.Objects;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+/**
+ * Ack operation request for HBase Native Queues.
+ */
+public class HBQAck extends HBQOperation {
+
+  private HBQEntryPointer entryPointer;
+
+  private HBReadPointer readPointer;
+
+  private HBQConsumer consumer;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQAck() {
+    super();
+  }
+
+  public HBQAck(final byte [] queueName,
+      final HBQConsumer consumer, final HBQEntryPointer entryPointer,
+      final HBReadPointer readPointer) {
+    super(queueName);
+    this.entryPointer = entryPointer;
+    this.readPointer = readPointer;
+    this.consumer = consumer;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  public HBQConsumer getConsumer() {
+    return this.consumer;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.entryPointer = new HBQEntryPointer();
+    this.entryPointer.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.consumer = new HBQConsumer();
+    this.consumer.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.entryPointer.write(out);
+    this.readPointer.write(out);
+    this.consumer.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("entryPointer", this.entryPointer)
+        .add("readPointer", this.readPointer)
+        .add("queueConsumer", this.consumer)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQConfig.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQConfig.java
new file mode 100644
index 0000000..69a184a
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQConfig.java
@@ -0,0 +1,63 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.continuuity.hbase.ttqueue.HBQPartitioner.HBQPartitionerType;
+import com.google.common.base.Objects;
+
+/**
+ * Queue configuration settings, used by a consumer during a dequeue.
+ * 
+ * TODO: The partitioner has been removed for now until we figure out how it
+ *       will actually work
+ */
+public class HBQConfig implements Writable {
+
+  private HBQPartitionerType partitionerType;
+  private boolean singleEntry;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQConfig() {}
+  
+  public HBQConfig(final HBQPartitionerType partitionerType,
+      final boolean singleEntry) {
+    this.partitionerType = partitionerType;
+    this.singleEntry = singleEntry;
+  }
+
+  public boolean isSingleEntry() {
+    return this.singleEntry;
+  }
+
+  public boolean isMultiEntry() {
+    return !isSingleEntry();
+  }
+
+  public HBQPartitionerType getPartitionerType() {
+    return this.partitionerType;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.partitionerType = HBQPartitionerType.fromByte(in.readByte());
+    this.singleEntry = in.readBoolean();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeByte(this.partitionerType.getByte());
+    out.writeBoolean(this.singleEntry);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("partitionerType", this.partitionerType)
+        .add("singleEntry", this.singleEntry)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQConstants.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQConstants.java
new file mode 100644
index 0000000..9193dd2
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQConstants.java
@@ -0,0 +1,34 @@
+package com.continuuity.hbase.ttqueue;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class HBQConstants {
+
+  public static final byte [] HBQ_FAMILY = Bytes.toBytes("f");
+
+  // Row headers
+  public static final byte [] GLOBAL_META_HEADER = bytes((byte)10);
+  /*
+   * JG: Removed separate row for group metadata because implementation does not
+   * currently support global meta header on different region than group header
+   * 
+   * public static final byte [] GLOBAL_GROUPS_HEADER = bytes((byte)30);
+   */
+  public static final byte [] GLOBAL_DATA_HEADER = bytes((byte)50);
+
+  // Columns for row = GLOBAL_ENTRY_HEADER
+  public static final byte [] GLOBAL_ENTRYID_COUNTER = bytes((byte)10);
+  public static final byte [] GLOBAL_SHARD_META = bytes((byte)30);
+  // Added group columns to global meta header
+  public static final byte [] GLOBAL_GROUP_PREFIX = bytes((byte)50);
+  public static final byte [] GROUP_ID_GEN = bytes((byte)70);
+
+  // Columns for row = GLOBAL_DATA_HEADER
+  public static final byte [] ENTRY_META = bytes((byte)10);
+  public static final byte [] ENTRY_GROUP_META = bytes((byte)20);
+  public static final byte [] ENTRY_DATA = bytes((byte)30);
+
+  private static byte [] bytes(byte b) {
+    return new byte [] { b };
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQConsumer.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQConsumer.java
new file mode 100644
index 0000000..8182d1a
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQConsumer.java
@@ -0,0 +1,64 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Information about a single consumer from a single group.
+ */
+public class HBQConsumer implements Writable {
+
+  private int instanceId;
+  private long groupId;
+  private int groupSize;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQConsumer() {}
+  
+  public HBQConsumer(final int instanceId, final long groupId,
+      final int groupSize) {
+    this.instanceId = instanceId;
+    this.groupId = groupId;
+    this.groupSize = groupSize;
+  }
+
+  public int getInstanceId() {
+    return this.instanceId;
+  }
+  
+  public long getGroupId() {
+    return this.groupId;
+  }
+
+  public int getGroupSize() {
+    return this.groupSize;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.instanceId = in.readInt();
+    this.groupId = in.readLong();
+    this.groupSize = in.readInt();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeInt(this.instanceId);
+    out.writeLong(this.groupId);
+    out.writeInt(this.groupSize);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("instanceId", this.instanceId)
+        .add("groupId", this.groupId)
+        .add("groupSize", this.groupSize)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQDequeue.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQDequeue.java
new file mode 100644
index 0000000..aa82ca0
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQDequeue.java
@@ -0,0 +1,105 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+import com.google.common.base.Objects;
+
+/**
+ * Dequeue operation request for HBase Native Queues.
+ */
+public class HBQDequeue extends HBQOperation {
+  
+  private HBReadPointer readPointer;
+  
+  private HBQConsumer consumer;
+  
+  private HBQConfig config;
+  
+  private HBQExpirationConfig expirationConfig;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQDequeue() {
+    super();
+  }
+  
+  public HBQDequeue(final byte [] queueName,
+      final HBQConsumer consumer, final HBQConfig config,
+      final HBReadPointer readPointer,
+      final HBQExpirationConfig expirationConfig) {
+    super(queueName);
+    this.readPointer = readPointer;
+    this.consumer = consumer;
+    this.config = config;
+    this.expirationConfig = expirationConfig;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  public HBQConsumer getConsumer() {
+    return this.consumer;
+  }
+
+  public HBQConfig getConfig() {
+    return this.config;
+  }
+
+  public HBQExpirationConfig getExpirationConfig() {
+    return this.expirationConfig;
+  }
+
+  /**
+   * Returns the group metadata row for the queue of this operation.
+   * @return group meta row for this queue
+   */
+  public byte [] getGroupMetaRow() {
+    return generateRow(HBQConstants.GLOBAL_META_HEADER);
+  }
+
+  public byte [] getGroupMetaColumn() {
+    return HBQDequeue.getGroupMetaColumn(consumer.getGroupId());
+  }
+
+  public static byte [] getGroupMetaColumn(long groupId) {
+    return Bytes.add(HBQConstants.GLOBAL_GROUP_PREFIX,
+        Bytes.toBytes(groupId));
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.consumer = new HBQConsumer();
+    this.consumer.readFields(in);
+    this.config = new HBQConfig();
+    this.config.readFields(in);
+    this.expirationConfig = new HBQExpirationConfig();
+    this.expirationConfig.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.readPointer.write(out);
+    this.consumer.write(out);
+    this.config.write(out);
+    this.expirationConfig.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("readPointer", this.readPointer)
+        .add("queueConsumer", this.consumer)
+        .add("queueConfig", this.config)
+        .add("queueExpirationConfig", this.expirationConfig)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQDequeueResult.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQDequeueResult.java
new file mode 100644
index 0000000..101a8e3
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQDequeueResult.java
@@ -0,0 +1,405 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+import com.continuuity.hbase.ttqueue.internal.GroupState;
+import com.google.common.base.Objects;
+
+/**
+ * The result from a dequeue operation.
+ * <p>
+ * TODO: Break this class into a second class (HBQDequeueInternalResult)
+ */
+public class HBQDequeueResult implements Writable {
+
+  // Returned values
+  private HBQDequeueStatus status = null;
+  private String msg = null;
+  private HBQEntryPointer entryPointer = null;
+  private byte [] queueData = null;
+  
+  // Variables used when dequeue request jumps regions/servers
+  private boolean pendingEntryCheck = false;
+  private boolean groupToBeUpdated = false;
+  private boolean dataFindDone = false;
+  private HBQDequeue dequeue = null;
+  private HBQEntryPointer curPointer = null;
+  private GroupState currentGroupState = null;
+  
+  /** Empty constructor for Writable (do not use) */
+  public HBQDequeueResult() {}
+  
+  /**
+   * Failed dequeue constructor.  Just specify message.
+   * @param msg
+   */
+  public HBQDequeueResult(final String msg) {
+    this.status = HBQDequeueStatus.FAILURE;
+    this.msg = msg;
+  }
+
+  /**
+   * Successful and completed dequeue.  Contains final status, and if not empty,
+   * an entry pointer and data.
+   * @param status
+   * @param entryPointer
+   * @param queueData
+   */
+  public HBQDequeueResult(final HBQDequeueStatus status,
+      final HBQEntryPointer entryPointer, final byte [] queueData) {
+    this.status = status;
+    this.entryPointer = entryPointer;
+    this.queueData = queueData;
+  }
+  
+  /**
+   * Constructor for when queue is empty.
+   * @param status 
+   */
+  public HBQDequeueResult(HBQDequeueStatus status) {
+    assert(status == HBQDequeueStatus.EMPTY);
+    this.status = status;
+  }
+
+  /**
+   * Constructor for an in-progress dequeue that is currently performing a check
+   * for dynamic queue reconfiguration (whether there are pending entries).
+   * @param dequeue the original dequeue operation
+   * @param currentEntryPointer the pointer to the current entry to be checked
+   * @param existingGroupState the original group state
+   * @param pendingEntryCheck true if this is an in-progress pending entry
+   *                          check, otherwise it will be treated as a normal
+   *                          in-progress data entry search
+   */
+  public HBQDequeueResult(HBQDequeue dequeue,
+      HBQEntryPointer currentEntryPointer, GroupState existingGroupState,
+      boolean pendingEntryCheck) {
+    this.dequeue = dequeue;
+    this.status = HBQDequeueStatus.IN_PROGRESS;
+    this.curPointer = currentEntryPointer;
+    this.currentGroupState = existingGroupState;
+    this.pendingEntryCheck = pendingEntryCheck;
+  }
+
+  /**
+   * Constructor for an in-progress dequeue that may or may not be completed
+   * because it is not yet known whether a group update is needed.  This
+   * constructor will determine if the specified initial group state is
+   * different from the current, and if so, will set it to be updated and the
+   * request will stay in progress
+   * 
+   * The data entry has already been found, or dequeue should return empty.
+   * 
+   * If dequeue should return empty, use null for queue data and entry pointer.
+   * 
+   * @param dequeue the original dequeue operation
+   * @param entryPointer pointer to the found and returned entry, null for empty
+   * @param queueData data of the found and returned entry, null for empty
+   * @param initialGroupState the initial group state
+   * @param currentGroupState the current group state
+   */
+  public HBQDequeueResult(HBQDequeue dequeue, HBQEntryPointer entryPointer,
+      byte [] queueData, GroupState initialGroupState,
+      GroupState currentGroupState) {
+    if (initialGroupState == null || currentGroupState == null ||
+        initialGroupState.equals(currentGroupState)) {
+      this.status = queueData == null ?
+          HBQDequeueStatus.EMPTY : HBQDequeueStatus.SUCCESS;
+      this.entryPointer = entryPointer;
+      this.queueData = queueData;
+      this.dequeue = dequeue;
+    } else {
+      this.status = HBQDequeueStatus.IN_PROGRESS;
+      this.dequeue = dequeue;
+      this.entryPointer = entryPointer;
+      this.queueData = queueData;
+      this.currentGroupState = currentGroupState;
+      this.groupToBeUpdated = true;
+      this.dataFindDone = true;
+    }
+  }
+
+  /**
+   * Constructor for an in-progress dequeue that is not yet completed because
+   * the data has not yet been found.  The group state will be set to be updated
+   * if the specified initial group state is different from the specified
+   * current group state.  If either group state is null, it will also not be
+   * set to be updated.
+   * <p>
+   * This should not be returned to the client before further processing.
+   * 
+   * @param dequeue
+   * @param entryPointer
+   * @param initialGroupState
+   * @param currentGroupState
+   */
+  public HBQDequeueResult(HBQDequeue dequeue, HBQEntryPointer entryPointer,
+      GroupState initialGroupState, GroupState currentGroupState) {
+    this.status = HBQDequeueStatus.IN_PROGRESS;
+    this.dequeue = dequeue;
+    this.curPointer = entryPointer;
+    this.dataFindDone = false;
+    if (initialGroupState == null || currentGroupState == null) {
+      this.groupToBeUpdated = false;
+    } else {
+      this.currentGroupState = currentGroupState;
+      if (!initialGroupState.equals(currentGroupState)) {
+        this.groupToBeUpdated = true;
+      }
+    }
+  }
+
+  public String getFailureMessage() {
+    return this.msg;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  /**
+   * Checks whether this in-progress dequeue operation is currently doing
+   * a pending entry check during a group reconfiguration.
+   * @return true if dequeue is in-progress and doing a pending entry check,
+   *         false if not
+   */
+  public boolean checkingPendingEntries() {
+    return this.pendingEntryCheck;
+  }
+
+  /**
+   * Checks whether search for data entry is done (either have an entry or
+   * empty).
+   * @return true if a data entry search is done, false if not
+   */
+  public boolean dataEntrySearchDone() {
+    return this.dataFindDone;
+  }
+
+  /**
+   * Checks whether during an in-progress dequeue, the group state needs to
+   * be updated.  If this returns true, the group state should be updated to the
+   * current state available in {@link #getCurrentGroupState()}.
+   *  
+   * @return true if group state should be updated to final state, false if not
+   */
+  public boolean groupStateNeedsToBeUpdated() {
+    return this.groupToBeUpdated;
+  }
+  
+  public byte [] getData() {
+    return this.queueData;
+  }
+  
+  public HBQDequeueStatus getStatus() {
+    return this.status;
+  }
+  
+  public boolean isSuccess() {
+    return this.status == HBQDequeueStatus.SUCCESS;
+  }
+
+  public boolean isFailure() {
+    return this.status == HBQDequeueStatus.FAILURE;
+  }
+
+  public boolean isEmpty() {
+    return this.status == HBQDequeueStatus.EMPTY;
+  }
+
+  public boolean isInProgress() {
+    return this.status == HBQDequeueStatus.IN_PROGRESS;
+  }
+
+  public void updateGroupState(GroupState updatedGroupState) {
+    this.currentGroupState = updatedGroupState;
+    this.groupToBeUpdated = true;
+  }
+
+  /**
+   * The group state was set to be updated and has now been completed.
+   */
+  public void groupStateUpdated() {
+    this.groupToBeUpdated = false;
+    if (dataEntrySearchDone()) {
+      if (this.queueData == null) {
+        this.status = HBQDequeueStatus.EMPTY;
+      } else {
+        this.status = HBQDequeueStatus.SUCCESS;
+      }
+    }
+  }
+
+  public GroupState getCurrentGroupState() {
+    return this.currentGroupState;
+  }
+
+  public HBQEntryPointer getCurrentEntryPointer() {
+    return this.curPointer;
+  }
+
+  public HBQDequeue getDequeue() {
+    return this.dequeue;
+  }
+
+  /**
+   * Determines and returns the next row to visit for an in-progress dequeue.
+   * 
+   * If data entry search is complete, then the group needs to be updated, and
+   * the next row will be the group meta row.
+   * 
+   * If the data entry search is not complete, the next row will be the
+   * returned shard row.
+   * 
+   * @return next row to be processed for in-progress dequeue
+   */
+  public byte[] getNextRow() {
+    if (dataEntrySearchDone()) {
+      return getDequeue().getGroupMetaRow();
+    } else {
+      return getDequeue().getDataRow(Bytes.toBytes(
+          getCurrentEntryPointer().getShardId()));
+    }
+  }
+
+  public static enum HBQDequeueStatus {
+    SUCCESS, EMPTY, FAILURE, IN_PROGRESS;
+    public byte getByte() {
+      switch (this) {
+        case SUCCESS: return 1;
+        case EMPTY:   return 2;
+        case FAILURE: return 3;
+        case IN_PROGRESS: return 4;
+        default: throw new RuntimeException("Serialization error");
+      }
+    }
+    public static HBQDequeueStatus fromByte(byte b) {
+      switch (b) {
+        case 1: return SUCCESS;
+        case 2: return EMPTY;
+        case 3: return FAILURE;
+        case 4: return IN_PROGRESS;
+        default: throw new RuntimeException("Deserialization error");
+      }
+    }
+  }
+  
+  // in progress field order:
+  // groupUpdate, dataFindDone, dequeue, curPointer, currentGroup,
+  // entryPointer, queueData
+  // all fields are first represented by a boolean representing them as null
+  // except queue data which has a length integer
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.status = HBQDequeueStatus.fromByte(in.readByte());
+    if (isSuccess()) {
+      this.entryPointer = new HBQEntryPointer();
+      this.entryPointer.readFields(in);
+      int len = in.readInt();
+      this.queueData = new byte[len];
+      in.readFully(queueData);
+    } else if (isFailure()) {
+      this.msg = in.readUTF();
+    } else if (isInProgress()) {
+      this.pendingEntryCheck = in.readBoolean();
+      this.groupToBeUpdated = in.readBoolean();
+      this.dataFindDone = in.readBoolean();
+      if (in.readBoolean()) {
+        this.dequeue = new HBQDequeue();
+        this.dequeue.readFields(in);
+      }
+      if (in.readBoolean()) {
+        this.curPointer = new HBQEntryPointer();
+        this.curPointer.readFields(in);
+      }
+      if (in.readBoolean()) {
+        this.currentGroupState = new GroupState();
+        this.currentGroupState.readFields(in);
+      }
+      if (in.readBoolean()) {
+        this.entryPointer = new HBQEntryPointer();
+        this.entryPointer.readFields(in);
+      }
+      int len = in.readInt();
+      if (len > 0) {
+        this.queueData = new byte[len];
+        in.readFully(queueData);
+      }
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeByte(this.status.getByte());
+    if (isSuccess()) {
+      this.entryPointer.write(out);
+      out.writeInt(this.queueData.length);
+      out.write(this.queueData);
+    } else if (isFailure()) {
+      out.writeUTF(this.msg);
+    } else if (isInProgress()) {
+      out.writeBoolean(this.pendingEntryCheck);
+      out.writeBoolean(this.groupToBeUpdated);
+      out.writeBoolean(this.dataFindDone);
+      out.writeBoolean(this.dequeue != null);
+      if (this.dequeue != null) {
+        this.dequeue.write(out); 
+      }
+      out.writeBoolean(this.curPointer != null);
+      if (this.curPointer != null) {
+        this.curPointer.write(out); 
+      }
+      out.writeBoolean(this.currentGroupState != null);
+      if (this.currentGroupState != null) {
+        this.currentGroupState.write(out); 
+      }
+      out.writeBoolean(this.entryPointer != null);
+      if (this.entryPointer != null) {
+        this.entryPointer.write(out); 
+      }
+      out.writeInt(this.queueData == null ? 0 : this.queueData.length);
+      if (this.queueData != null) {
+        out.write(this.queueData);
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    if (isSuccess()) {
+      return Objects.toStringHelper(this)
+          .add("status", this.status)
+          .add("entryPointer", this.entryPointer.toString())
+          .add("data.length", this.queueData.length)
+          .toString();
+    } else if (isFailure()) {
+      return Objects.toStringHelper(this)
+          .add("status", this.status)
+          .add("failureMsg", this.msg)
+          .toString();
+    } else if (isInProgress()) {
+      return Objects.toStringHelper(this)
+          .add("status", this.status)
+          .add("dequeue", this.dequeue)
+          .add("pendingEntryCheck", this.pendingEntryCheck)
+          .add("dataFindDone", this.dataFindDone)
+          .add("groupToBeUpdated", this.groupToBeUpdated)
+          .add("curPointer", this.curPointer)
+          .add("currentGroupState", this.currentGroupState)
+          .add("entryPointer", this.entryPointer)
+          .add("data.length", this.queueData == null ? "null" :
+              "" + this.queueData.length)
+          .toString();
+    } else {
+      return Objects.toStringHelper(this)
+          .add("status", this.status)
+          .toString();
+    }
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueue.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueue.java
new file mode 100644
index 0000000..f5e4c79
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueue.java
@@ -0,0 +1,74 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import com.google.common.base.Objects;
+
+/**
+ * Enqueue operation request for HBase Native Queues.
+ */
+public class HBQEnqueue extends HBQOperation {
+
+  private byte [] queueData;
+  
+  private HBReadPointer readPointer;
+  
+  private HBQShardConfig shardConfig;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQEnqueue() {
+    super();
+  }
+  
+  public HBQEnqueue(final byte [] queueName, final byte [] queueData,
+      final HBReadPointer readPointer, final HBQShardConfig shardConfig) {
+    super(queueName);
+    this.queueData = queueData;
+    this.readPointer = readPointer;
+    this.shardConfig = shardConfig;
+  }
+
+  public byte [] getQueueData() {
+    return this.queueData;
+  }
+
+  public HBReadPointer getReadPointer() {
+    return this.readPointer;
+  }
+
+  public HBQShardConfig getShardConfig() {
+    return this.shardConfig;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    int len = in.readInt();
+    this.queueData = new byte[len];
+    in.readFully(this.queueData);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.shardConfig = new HBQShardConfig();
+    this.shardConfig.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    out.writeInt(this.queueData.length);
+    out.write(this.queueData);
+    this.readPointer.write(out);
+    this.shardConfig.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("readPointer", this.readPointer)
+        .add("data.length", this.queueData.length)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueueResult.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueueResult.java
new file mode 100644
index 0000000..97ae462
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQEnqueueResult.java
@@ -0,0 +1,120 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Result of an enqueue operation.
+ */
+public class HBQEnqueueResult implements Writable {
+
+  private boolean success;
+  private String msg;
+  private HBQEntryPointer entryPointer;
+  private boolean dataWritten = false;
+  private boolean insertShardEnd = false;
+  
+  /** Empty constructor for Writable (do not use) */
+  public HBQEnqueueResult() {}
+  
+  /**
+   * Failed dequeue constructor.  Just specify message.
+   * @param msg
+   */
+  public HBQEnqueueResult(final String msg) {
+    this.success = false;
+    this.msg = msg;
+  }
+
+  /**
+   * Constructs an enqueue result that is completely finished, that is,
+   * both meta and data have been inserted successfully.
+   * @param entryPointer
+   */
+  public HBQEnqueueResult(final HBQEntryPointer entryPointer) {
+    this.success = true;
+    this.entryPointer = entryPointer;
+    this.dataWritten = true;
+  }
+
+  /**
+   * Constructs an enqueue result when data has not yet been inserted,
+   * signaling the caller to perform the call on another region or server.
+   * @param hbqEntryPointer
+   * @param insertShardEnd
+   */
+  public HBQEnqueueResult(final HBQEntryPointer hbqEntryPointer,
+      final boolean insertShardEnd) {
+    this.success = true;
+    this.entryPointer = hbqEntryPointer;
+    this.dataWritten = false;
+    this.insertShardEnd = insertShardEnd;
+  }
+  
+  public boolean isSuccess() {
+    return this.success;
+  }
+
+  public String getFailureMessage() {
+    return this.msg;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+  
+  public boolean getInsertShardEnd() {
+    return this.insertShardEnd;
+  }
+  
+  public boolean getDataWritten() {
+    return this.dataWritten;
+  }
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.success = in.readBoolean();
+    if (this.success) {
+       this.entryPointer = new HBQEntryPointer();
+       this.entryPointer.readFields(in);
+       this.insertShardEnd = in.readBoolean();
+       this.dataWritten = in.readBoolean();
+    } else {
+      this.msg = in.readUTF();
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeBoolean(this.success);
+    if (this.success) {
+      this.entryPointer.write(out);
+      out.writeBoolean(this.insertShardEnd);
+      out.writeBoolean(this.dataWritten);
+    } else {
+      out.writeUTF(this.msg);
+    }
+  }
+
+  @Override
+  public String toString() {
+    if (success) {
+      return Objects.toStringHelper(this)
+          .add("success", this.success)
+          .add("entryPointer", this.entryPointer.toString())
+          .add("insertShardEnd", this.insertShardEnd)
+          .add("dataWritten", this.dataWritten)
+          .toString();
+    } else {
+      return Objects.toStringHelper(this)
+          .add("success", this.success)
+          .add("failureMsg", this.msg)
+          .toString();
+    }
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQEntryPointer.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQEntryPointer.java
new file mode 100644
index 0000000..a4fb860
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQEntryPointer.java
@@ -0,0 +1,80 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * A pointer for a single queue entry.
+ */
+public class HBQEntryPointer implements Writable {
+
+  protected long entryId;
+  protected long shardId;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQEntryPointer() {}
+  
+  public HBQEntryPointer(final long entryId, final long shardId) {
+    this.entryId = entryId;
+    this.shardId = shardId;
+  }
+
+  public long getEntryId() {
+    return this.entryId;
+  }
+
+  public long getShardId() {
+    return this.shardId;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.entryId = in.readLong();
+    this.shardId = in.readLong();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.entryId);
+    out.writeLong(this.shardId);
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    return this.entryId == ((HBQEntryPointer)o).entryId &&
+        this.shardId == ((HBQEntryPointer)o).shardId;
+  }
+
+  @Override
+  public int hashCode() {
+    return Bytes.hashCode(
+        Bytes.toBytes(entryId)) ^ Bytes.hashCode(Bytes.toBytes(shardId));
+  }
+
+  public HBQEntryPointer makeCopy() {
+    return new HBQEntryPointer(this.entryId, this.shardId);
+  }
+
+  public byte [] getBytes() {
+    return Bytes.add(Bytes.toBytes(this.entryId),
+        Bytes.toBytes(this.shardId));
+  }
+
+  public static HBQEntryPointer fromBytes(byte [] bytes) {
+    return new HBQEntryPointer(Bytes.toLong(bytes, 0), Bytes.toLong(bytes, 8));
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("entryId", this.entryId)
+        .add("shardId", this.shardId)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQExpirationConfig.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQExpirationConfig.java
new file mode 100644
index 0000000..8d294d9
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQExpirationConfig.java
@@ -0,0 +1,60 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Expiration configuration settings.
+ */
+public class HBQExpirationConfig implements Writable {
+
+  long maxAgeBeforeExpirationInMillis;
+  long maxAgeBeforeSemiAckedToAcked;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQExpirationConfig() {}
+  
+  public HBQExpirationConfig(final long maxAgeBeforeExpirationInMillis,
+      final long maxAgeBeforeSemiAckedToAcked) {
+    this.maxAgeBeforeExpirationInMillis = maxAgeBeforeExpirationInMillis;
+    this.maxAgeBeforeSemiAckedToAcked = maxAgeBeforeSemiAckedToAcked;
+  }
+
+  public void setMaxAgeBeforeExpirationInMillis(long expirationInMillis) {
+    this.maxAgeBeforeExpirationInMillis = expirationInMillis;
+  }
+
+  public long getMaxAgeBeforeExpirationInMillis() {
+    return this.maxAgeBeforeExpirationInMillis;
+  }
+
+  public long getMaxAgeBeforeSemiAckedToAcked() {
+    return this.maxAgeBeforeSemiAckedToAcked;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.maxAgeBeforeExpirationInMillis = in.readLong();
+    this.maxAgeBeforeSemiAckedToAcked = in.readLong();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.maxAgeBeforeExpirationInMillis);
+    out.writeLong(this.maxAgeBeforeSemiAckedToAcked);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("maxAgeBeforeExpirationInMillis",
+            this.maxAgeBeforeExpirationInMillis)
+        .add("maxAgeBeforeSemiAckedToAcked", this.maxAgeBeforeSemiAckedToAcked)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQFinalize.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQFinalize.java
new file mode 100644
index 0000000..1e56fd9
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQFinalize.java
@@ -0,0 +1,90 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import com.google.common.base.Objects;
+
+/**
+ * Ack operation request for HBase Native Queues.
+ */
+public class HBQFinalize extends HBQOperation {
+
+  private HBQEntryPointer entryPointer;
+
+  private HBQConsumer consumer;
+  
+  private int totalNumGroups;
+  
+  private HBReadPointer readPointer;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQFinalize() {
+    super();
+  }
+  
+  public HBQFinalize(final byte [] queueName,
+      final HBQConsumer consumer, final HBQEntryPointer entryPointer,
+      final HBReadPointer readPointer) {
+    this(queueName, consumer, entryPointer, readPointer, 0);
+  }
+  
+  public HBQFinalize(final byte [] queueName,
+      final HBQConsumer consumer, final HBQEntryPointer entryPointer,
+      final HBReadPointer readPointer, int totalNumGroups) {
+    super(queueName);
+    this.entryPointer = entryPointer;
+    this.readPointer = readPointer;
+    this.consumer = consumer;
+    this.totalNumGroups = totalNumGroups;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  public HBQConsumer getConsumer() {
+    return this.consumer;
+  }
+
+  public int getTotalNumGroups() {
+    return this.totalNumGroups;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.entryPointer = new HBQEntryPointer();
+    this.entryPointer.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.consumer = new HBQConsumer();
+    this.consumer.readFields(in);
+    this.totalNumGroups = in.readInt();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.entryPointer.write(out);
+    this.readPointer.write(out);
+    this.consumer.write(out);
+    out.writeInt(this.totalNumGroups);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("entryPointer", this.entryPointer)
+        .add("readPointer", this.readPointer)
+        .add("consumer", this.consumer)
+        .add("totalNumGroups", this.totalNumGroups)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidate.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidate.java
new file mode 100644
index 0000000..d702966
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidate.java
@@ -0,0 +1,62 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import com.google.common.base.Objects;
+
+/**
+ * Invalidate operation request for HBase Native Queues.
+ */
+public class HBQInvalidate extends HBQOperation {
+
+  private HBQEntryPointer entryPointer;
+  
+  private HBReadPointer readPointer;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQInvalidate() {
+    super();
+  }
+  
+  public HBQInvalidate(final byte [] queueName,
+      final HBQEntryPointer entryPointer, final HBReadPointer readPointer) {
+    super(queueName);
+    this.entryPointer = entryPointer;
+    this.readPointer = readPointer;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.entryPointer = new HBQEntryPointer();
+    this.entryPointer.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.entryPointer.write(out);
+    this.readPointer.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("entryPointer", this.entryPointer)
+        .add("readPointer", this.readPointer)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidateResult.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidateResult.java
new file mode 100644
index 0000000..1b54cdf
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQInvalidateResult.java
@@ -0,0 +1,97 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * A pointer for a single queue entry.
+ */
+public class HBQInvalidateResult implements Writable {
+
+  private boolean success;
+  private String msg;
+  private HBQEntryPointer entryPointer;
+  private byte [] queueData;
+  
+  /** Empty constructor for Writable (do not use) */
+  public HBQInvalidateResult() {}
+  
+  /**
+   * Failed dequeue constructor.  Just specify message.
+   * @param msg
+   */
+  public HBQInvalidateResult(final String msg) {
+    this.success = false;
+    this.msg = msg;
+  }
+
+  public HBQInvalidateResult(final HBQEntryPointer entryPointer,
+      final byte [] queueData) {
+    this.success = true;
+    this.entryPointer = entryPointer;
+    this.queueData = queueData;
+  }
+  
+  public boolean isSuccess() {
+    return this.success;
+  }
+
+  public String getFailureMessage() {
+    return this.msg;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+  
+  public byte [] getData() {
+    return this.queueData;
+  }
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.success = in.readBoolean();
+    if (this.success) {
+       int len = in.readInt();
+       this.queueData = new byte[len];
+       in.readFully(queueData);
+       this.entryPointer = new HBQEntryPointer();
+       this.entryPointer.readFields(in);
+    } else {
+      this.msg = in.readUTF();
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeBoolean(this.success);
+    if (this.success) {
+      out.writeInt(this.queueData.length);
+      out.write(this.queueData);
+      this.entryPointer.write(out);
+    } else {
+      out.writeUTF(this.msg);
+    }
+  }
+
+  @Override
+  public String toString() {
+    if (success) {
+      return Objects.toStringHelper(this)
+          .add("success", this.success)
+          .add("entryPointer", this.entryPointer.toString())
+          .add("data.length", this.queueData.length)
+          .toString();
+    } else {
+      return Objects.toStringHelper(this)
+          .add("success", this.success)
+          .add("failureMsg", this.msg)
+          .toString();
+    }
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQMetaOperation.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQMetaOperation.java
new file mode 100644
index 0000000..6413b6b
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQMetaOperation.java
@@ -0,0 +1,67 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Meta operations.
+ */
+public class HBQMetaOperation extends HBQOperation implements Writable {
+
+  private MetaOperationType operationType;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQMetaOperation() {
+    super();
+  }
+
+  public HBQMetaOperation(final byte [] queueName,
+      final MetaOperationType operationType) {
+    super(queueName);
+    this.operationType = operationType;
+  }
+
+  public byte[] getGroupIdRow() {
+    return generateRow(HBQConstants.GLOBAL_META_HEADER);
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.operationType = MetaOperationType.fromByte(in.readByte());
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    out.writeByte(operationType.toByte());
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("operationType", operationType.name())
+        .toString();
+  }
+
+  public static enum MetaOperationType {
+    GET_GROUP_ID, GET_QUEUE_META;
+    public static MetaOperationType fromByte(byte b) {
+      switch (b) {
+        case '0': return GET_GROUP_ID;
+        case '1': return GET_QUEUE_META;
+        default: throw new RuntimeException("Fatal serialization error");
+      }
+    }
+    public byte toByte() {
+      if (this == GET_GROUP_ID) return '0';
+      else return '1';
+    }
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQOperation.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQOperation.java
new file mode 100644
index 0000000..401bd59
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQOperation.java
@@ -0,0 +1,81 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+public class HBQOperation implements Writable {
+
+  protected byte [] queueName;
+  
+  protected boolean writeToWAL = true;
+
+  protected HBQOperation() {}
+  
+  protected HBQOperation(final byte [] queueName) {
+    this.queueName = queueName;
+  }
+
+  public byte [] getQueueName() {
+    return this.queueName;
+  }
+
+  /**
+   * Returns the primary metadata row for the queue of this operation.
+   * @return meta row for this queue
+   */
+  public byte [] getMetaRow() {
+    return generateRow(HBQConstants.GLOBAL_META_HEADER);
+  }
+
+  public boolean getWriteToWAL() {
+    return this.writeToWAL;
+  }
+
+  public void setWriteToWAL(boolean writeToWAL) {
+    this.writeToWAL = writeToWAL;
+  }
+
+  /**
+   * Returns the primary metadata row for the queue of this operation.
+   * @param shardid 
+   * @return row key of the data row for this queue operation and the specified
+   *         shard
+   */
+  public byte [] getDataRow(byte [] shardid) {
+    return generateRow(HBQConstants.GLOBAL_DATA_HEADER, shardid);
+  }
+
+  protected byte [] generateRow(byte [] header) {
+    return Bytes.add(this.queueName, header);
+  }
+
+  protected byte [] generateRow(byte [] headerOne, byte [] headerTwo) {
+    return Bytes.add(this.queueName, headerOne, headerTwo);
+  }
+
+  protected byte [] generateRow(byte [] headerOne, byte [] headerTwo,
+      byte [] headerThree) {
+    return Bytes.add(generateRow(headerOne, headerTwo), headerThree);
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    int len = in.readInt();
+    this.queueName = new byte[len];
+    in.readFully(this.queueName);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeInt(this.queueName.length);
+    out.write(this.queueName);
+  }
+
+  public static enum HBQOperationType {
+    ENQUEUE, DEQUEUE, DEQUEUE_CONTINUE, INVALIDATE, ACK, UNACK, FINALIZE
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQPartitioner.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQPartitioner.java
new file mode 100644
index 0000000..35f24c4
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQPartitioner.java
@@ -0,0 +1,95 @@
+package com.continuuity.hbase.ttqueue;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+import com.google.common.base.Objects;
+
+/**
+ * Interface used to determine whether a queue entry should be returned to a
+ * given consumer.
+ */
+public interface HBQPartitioner {
+
+  /**
+   * Returns true if the specified entry should be emitted to the specified
+   * consumer.
+   * @param consumer
+   * @param entryId
+   * @param value
+   * @return true if entry should be emitted to consumer, false if not
+   */
+  public boolean shouldEmit(HBQConsumer consumer, long entryId,
+      byte [] value);
+
+  public static enum HBQPartitionerType {
+    RANDOM, HASH_ON_VALUE, MODULO_LONG_VALUE;
+
+    private static final HBQPartitioner PARTITIONER_RANDOM =
+        new RandomPartitioner();
+    private static final HBQPartitioner PARTITIONER_HASH =
+        new HashPartitioner();
+    private static final HBQPartitioner PARTITIONER_LONG_MOD =
+        new LongValueHashPartitioner();
+    
+    public HBQPartitioner getPartitioner() {
+      switch (this) {
+        case RANDOM: return PARTITIONER_RANDOM;
+        case HASH_ON_VALUE: return PARTITIONER_HASH;
+        case MODULO_LONG_VALUE: return PARTITIONER_LONG_MOD;
+        default: return new RandomPartitioner();
+      }
+    }
+    public byte getByte() {
+      switch (this) {
+        case RANDOM: return '0';
+        case HASH_ON_VALUE: return '1';
+        case MODULO_LONG_VALUE: return '2';
+        default: return '0';
+      }
+    }
+    public static HBQPartitionerType fromByte(byte b) {
+      switch (b) {
+        case '0': return RANDOM;
+        case '1': return HASH_ON_VALUE;
+        case '2': return MODULO_LONG_VALUE;
+        default: return RANDOM;
+      }
+    }
+  }
+  
+  public static class RandomPartitioner implements HBQPartitioner {
+    @Override
+    public boolean shouldEmit(HBQConsumer consumer, long entryId,
+        byte [] value) {
+      return true;
+    }
+
+    @Override
+    public String toString() {
+      return Objects.toStringHelper(this).toString();
+    }
+  }
+
+  public static class HashPartitioner implements HBQPartitioner {
+    @Override
+    public boolean shouldEmit(HBQConsumer consumer, long entryId,
+        byte [] value) {
+      int hash = Bytes.hashCode(value);
+      return (hash % consumer.getGroupSize() == consumer.getInstanceId());
+    }
+
+    @Override
+    public String toString() {
+      return Objects.toStringHelper(this).toString();
+    }
+  }
+
+  public static class LongValueHashPartitioner implements HBQPartitioner {
+    @Override
+    public boolean shouldEmit(HBQConsumer consumer, long entryId,
+        byte [] value) {
+      long val = Bytes.toLong(value);
+      return (val % consumer.getGroupSize()) == consumer.getInstanceId();
+    }
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQQueueMeta.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQQueueMeta.java
new file mode 100644
index 0000000..4f0fa09
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQQueueMeta.java
@@ -0,0 +1,164 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+
+import com.continuuity.hbase.ttqueue.internal.GroupState;
+import com.continuuity.hbase.ttqueue.internal.QueueStats;
+import com.continuuity.hbase.ttqueue.internal.ShardMeta;
+import com.google.common.base.Objects;
+
+/**
+ * Queue meta data.
+ */
+public class HBQQueueMeta implements Writable {
+
+  long entryId;
+  ShardMeta shardMeta;
+  Map<Long,GroupState> groups;
+  QueueStats stats;
+  HBQEntryPointer currentPointer = null;
+  HBQMetaOperation inProgressOperation = null;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQQueueMeta() {}
+
+  public HBQQueueMeta(long entryId, ShardMeta shardMeta,
+      Map<Long,GroupState> groups) {
+    this.entryId = entryId;
+    this.shardMeta = shardMeta;
+    this.groups = groups;
+    this.stats = new QueueStats();
+  }
+
+  public long getEntryId() {
+    return this.entryId;
+  }
+
+  public ShardMeta getShardMeta() {
+    return this.shardMeta;
+  }
+
+  public Map<Long,GroupState> getGroups() {
+    return this.groups;
+  }
+  
+  public QueueStats getQueueStats() {
+    return this.stats;
+  }
+
+  public HBQEntryPointer getPointer() {
+    return this.currentPointer;
+  }
+
+  public void setAsCompleted() {
+    this.inProgressOperation = null;
+    this.currentPointer = null;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.entryId = in.readLong();
+    this.shardMeta = new ShardMeta();
+    this.shardMeta.readFields(in);
+    int len = in.readInt();
+    this.groups = new HashMap<Long,GroupState>();
+    for (int i=0; i<len; i++) {
+      Long groupid = in.readLong();
+      GroupState groupState = new GroupState();
+      groupState.readFields(in);
+      this.groups.put(groupid, groupState);
+    }
+    this.stats = new QueueStats();
+    this.stats.readFields(in);
+    boolean inProgress = in.readBoolean();
+    if (inProgress) {
+      this.inProgressOperation = new HBQMetaOperation();
+      this.inProgressOperation.readFields(in);
+      this.currentPointer = new HBQEntryPointer();
+      this.currentPointer.readFields(in);
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.entryId);
+    this.shardMeta.write(out);
+    out.writeInt(this.groups.size());
+    for (Map.Entry<Long,GroupState> entry : this.groups.entrySet()) {
+      out.writeLong(entry.getKey());
+      entry.getValue().write(out);
+    }
+    this.stats.write(out);
+    out.writeBoolean(this.inProgressOperation != null);
+    if (this.inProgressOperation != null) {
+      this.inProgressOperation.write(out);
+      this.currentPointer.write(out);
+    }
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("entryId", this.entryId)
+        .add("shardMeta", this.shardMeta)
+        .add("groups", this.groups)
+        .add("stats", this.stats)
+        .toString();
+  }
+
+  public String toJSON() throws JSONException {
+    return getJSONObject().toString();
+  }
+
+  public String toJSONPretty() throws JSONException {
+    return getJSONObject().toString(2);
+  }
+  
+  public JSONObject getJSONObject() throws JSONException {
+    JSONObject outer = new JSONObject();
+    outer.put("entryid", this.entryId);
+    outer.put("shardmeta", this.shardMeta.getJSONObject());
+    JSONArray groupArray = new JSONArray();
+    for (Map.Entry<Long,GroupState> entry : this.groups.entrySet()) {
+      JSONObject inner = new JSONObject();
+      inner.put("groupid", entry.getKey());
+      inner.put("groupstats", entry.getValue().getJSONObject());
+      groupArray.put(inner);
+    }
+    outer.put("groups", groupArray);
+    outer.put("stats", this.stats.getJSONObject());
+    return outer;
+  }
+
+  public boolean isInProgress() {
+    return this.inProgressOperation != null;
+  }
+
+  public byte[] getNextRow() {
+    assert isInProgress();
+    assert this.currentPointer != null;
+    return this.inProgressOperation.getDataRow(
+        Bytes.toBytes(this.currentPointer.getShardId()));
+  }
+
+  public void updateInProgress(HBQMetaOperation operation,
+      HBQEntryPointer hbqEntryPointer) {
+    this.inProgressOperation = operation;
+    this.currentPointer = hbqEntryPointer;
+  }
+
+  public void updateInProgress(HBQEntryPointer hbqEntryPointer) {
+    assert isInProgress();
+    this.currentPointer = hbqEntryPointer;
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQShardConfig.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQShardConfig.java
new file mode 100644
index 0000000..de81758
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQShardConfig.java
@@ -0,0 +1,55 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.base.Objects;
+
+/**
+ * Shard configuration settings.
+ */
+public class HBQShardConfig implements Writable {
+
+  long maxEntriesPerShard;
+  long maxBytesPerShard;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQShardConfig() {}
+  
+  public HBQShardConfig(final long maxEntriesPerShard,
+      final long maxBytesPerShard) {
+    this.maxEntriesPerShard = maxEntriesPerShard;
+    this.maxBytesPerShard = maxBytesPerShard;
+  }
+
+  public long getMaxEntriesPerShard() {
+    return this.maxEntriesPerShard;
+  }
+
+  public long getMaxBytesPerShard() {
+    return this.maxBytesPerShard;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.maxEntriesPerShard = in.readLong();
+    this.maxBytesPerShard = in.readLong();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.maxEntriesPerShard);
+    out.writeLong(this.maxBytesPerShard);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("maxEntriesPerShard", this.maxEntriesPerShard)
+        .add("maxBytesPerShard", this.maxBytesPerShard)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBQUnack.java b/src/main/java/com/continuuity/hbase/ttqueue/HBQUnack.java
new file mode 100644
index 0000000..039e1f3
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBQUnack.java
@@ -0,0 +1,74 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import com.google.common.base.Objects;
+
+/**
+ * Ack operation request for HBase Native Queues.
+ */
+public class HBQUnack extends HBQOperation {
+
+  private HBQEntryPointer entryPointer;
+  
+  private HBReadPointer readPointer;
+
+  private HBQConsumer consumer;
+
+  /** Empty constructor for Writable (do not use) */
+  public HBQUnack() {
+    super();
+  }
+  
+  public HBQUnack(final byte [] queueName,
+      final HBQConsumer consumer, final HBQEntryPointer entryPointer,
+      final HBReadPointer readPointer) {
+    super(queueName);
+    this.entryPointer = entryPointer;
+    this.readPointer = readPointer;
+    this.consumer = consumer;
+  }
+
+  public HBQEntryPointer getEntryPointer() {
+    return this.entryPointer;
+  }
+
+  public HBReadPointer getPointer() {
+    return this.readPointer;
+  }
+
+  public HBQConsumer getConsumer() {
+    return this.consumer;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    this.entryPointer = new HBQEntryPointer();
+    this.entryPointer.readFields(in);
+    this.readPointer = new HBReadPointer();
+    this.readPointer.readFields(in);
+    this.consumer = new HBQConsumer();
+    this.consumer.readFields(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    this.entryPointer.write(out);
+    this.readPointer.write(out);
+    this.consumer.write(out);
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("hbqOperation", super.toString())
+        .add("entryPointer", this.entryPointer)
+        .add("readPointer", this.readPointer)
+        .add("queueConsumer", this.consumer)
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/HBReadPointer.java b/src/main/java/com/continuuity/hbase/ttqueue/HBReadPointer.java
new file mode 100644
index 0000000..a5eb4fb
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/HBReadPointer.java
@@ -0,0 +1,80 @@
+package com.continuuity.hbase.ttqueue;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.io.Writable;
+
+public class HBReadPointer implements Writable {
+
+  private long writePointer;
+  
+  private long readPointer;
+  
+  private Set<Long> readExcludes;
+
+  /** Writable constructor (do not use) */
+  public HBReadPointer() {}
+  
+  public HBReadPointer(long writePointer, long readPointer) {
+    this(writePointer, readPointer, new HashSet<Long>());
+  }
+  
+  public HBReadPointer(long writePointer, long readPointer,
+      Set<Long> readExcludes) {
+    this.writePointer = writePointer;
+    this.readPointer = readPointer;
+    this.readExcludes = readExcludes;
+  }
+  
+  // Note: This is the com.continuuity.data.table.ReadPointer interface but not
+  // actually defined here as we don't want HBase to depend on data-fabric
+  // (and we don't need this to actually support that interface)
+
+  public boolean isVisible(long txid) {
+    if (txid == this.writePointer) return true;
+    if (txid > this.readPointer) return false;
+    return !isExcluded(txid);
+  }
+
+  private boolean isExcluded(long txid) {
+    return this.readExcludes != null && this.readExcludes.contains(txid);
+  }
+
+  public long getReadPointer() {
+    return this.readPointer;
+  }
+  
+  public long getWritePointer() {
+    return this.writePointer < 0 ?
+        EnvironmentEdgeManager.currentTimeMillis() : this.writePointer;
+  }
+  
+  // Writable
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.writePointer = in.readLong();
+    this.readPointer = in.readLong();
+    int n = in.readInt();
+    this.readExcludes = new HashSet<Long>(n < 4 ? 4 : n);
+    for (int i=0; i<n; i++) this.readExcludes.add(in.readLong());
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.writePointer);
+    out.writeLong(this.readPointer);
+    if (this.readExcludes == null || this.readExcludes.isEmpty()) {
+      out.writeInt(0);
+    } else {
+      out.writeInt(this.readExcludes.size());
+      for (Long excluded : readExcludes) out.writeLong(excluded);
+    }
+  }
+
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/internal/EntryGroupMeta.java b/src/main/java/com/continuuity/hbase/ttqueue/internal/EntryGroupMeta.java
new file mode 100644
index 0000000..b0abd61
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/internal/EntryGroupMeta.java
@@ -0,0 +1,121 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+import com.google.common.base.Objects;
+
+/**
+ * Meta data for a group about a queue entry.
+ */
+public class EntryGroupMeta {
+
+  private final EntryGroupState state;
+  private final long timestamp;
+  private final int instanceId;
+  
+  public EntryGroupMeta(final EntryGroupState state, final long timestamp,
+      final int instanceId) {
+    this.state = state;
+    this.timestamp = timestamp;
+    this.instanceId = instanceId;
+  }
+
+  public boolean isAvailable() {
+    return state == EntryGroupState.AVAILABLE;
+  }
+  
+  public boolean isAckedOrSemiAcked() {
+    return state == EntryGroupState.SEMI_ACKED ||
+        state == EntryGroupState.ACKED;
+  }
+
+  public boolean isSemiAcked() {
+    return state == EntryGroupState.SEMI_ACKED;
+  }
+  
+  public boolean isAcked() {
+    return state == EntryGroupState.ACKED;
+  }
+  
+  public boolean isDequeued() {
+    return state == EntryGroupState.DEQUEUED;
+  }
+  
+  public EntryGroupState getState() {
+    return this.state;
+  }
+
+  public long getTimestamp() {
+    return this.timestamp;
+  }
+  
+  public int getInstanceId() {
+    return this.instanceId;
+  }
+  
+  public byte [] getBytes() {
+    return Bytes.add(this.state.getBytes(), Bytes.toBytes(timestamp),
+        Bytes.toBytes(instanceId));
+  }
+  
+  public static EntryGroupMeta fromBytes(byte [] bytes) {
+    return new EntryGroupMeta(EntryGroupState.fromBytes(new byte[] {bytes[0]}),
+        Bytes.toLong(bytes, 1), Bytes.toInt(bytes, 9));
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (o == null) return false;
+    if (!(o instanceof EntryGroupMeta)) return false;
+    EntryGroupMeta egm = (EntryGroupMeta)o;
+    if (egm.getState() != this.state) return false;
+    if (egm.getInstanceId() != this.instanceId) return false;
+    if (egm.getTimestamp() != this.timestamp) return false;
+    return true;
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("state", this.state)
+        .add("timestamp", this.timestamp)
+        .add("instanceId", this.instanceId)
+        .toString();
+  }
+  
+  public static enum EntryGroupState {
+    AVAILABLE, SEMI_ACKED, ACKED, DEQUEUED;
+    
+    private static final byte [] AVAILABLE_BYTES = new byte [] { 0 };
+    private static final byte [] SEMI_ACKED_BYTES = new byte [] { 1 };
+    private static final byte [] ACKED_BYTES = new byte [] { 2 };
+    private static final byte [] DEQUEUED_BYTES = new byte [] { 3 };
+    
+    public byte [] getBytes() {
+      switch (this) {
+        case AVAILABLE: return AVAILABLE_BYTES;
+        case SEMI_ACKED:return SEMI_ACKED_BYTES;
+        case ACKED:     return ACKED_BYTES;
+        case DEQUEUED:  return DEQUEUED_BYTES;
+      }
+      return null;
+    }
+    
+    public static EntryGroupState fromBytes(byte [] bytes) {
+      if (bytes.length == 1) {
+        if (bytes[0] == AVAILABLE_BYTES[0]) return AVAILABLE;
+        if (bytes[0] == SEMI_ACKED_BYTES[0]) return SEMI_ACKED;
+        if (bytes[0] == ACKED_BYTES[0]) return ACKED;
+        if (bytes[0] == DEQUEUED_BYTES[0]) return DEQUEUED;
+      }
+      throw new RuntimeException("Invalid deserialization of EntryGroupState");
+    }
+
+    @Override
+    public String toString() {
+      return Objects.toStringHelper(this)
+          .add("state", this.name())
+          .toString();
+    }
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/internal/EntryMeta.java b/src/main/java/com/continuuity/hbase/ttqueue/internal/EntryMeta.java
new file mode 100644
index 0000000..11ed4d2
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/internal/EntryMeta.java
@@ -0,0 +1,87 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import com.google.common.base.Objects;
+
+
+/**
+ * Meta data about a queue entry.
+ */
+public class EntryMeta {
+
+  private final EntryState state;
+
+  public EntryMeta(final EntryState state) {
+    this.state = state;
+  }
+
+  public boolean isValid() {
+    return this.state == EntryState.VALID;
+  }
+
+  public boolean isInvalid() {
+    return this.state == EntryState.INVALID;
+  }
+
+  public boolean isEndOfShard() {
+    return this.state == EntryState.SHARD_END;
+  }
+
+  public boolean isEvicted() {
+    return this.state == EntryState.EVICTED;
+  }
+
+  public byte [] getBytes() {
+    return this.state.getBytes();
+  }
+
+  public EntryState getState() {
+    return this.state;
+  }
+
+  public static EntryMeta fromBytes(byte [] bytes) {
+    return new EntryMeta(EntryState.fromBytes(bytes));
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("state", this.state)
+        .toString();
+  }
+
+  public static enum EntryState {
+    VALID, INVALID, SHARD_END, EVICTED;
+
+    private static final byte [] VALID_BYTES = new byte [] { 0 };
+    private static final byte [] INVALID_BYTES = new byte [] { 1 };
+    private static final byte [] SHARD_END_BYTES = new byte [] { 2 };
+    private static final byte [] EVICTED_BYTES = new byte [] { 3 };
+
+    public byte [] getBytes() {
+      switch (this) {
+        case VALID:     return VALID_BYTES;
+        case INVALID:   return INVALID_BYTES;
+        case SHARD_END: return SHARD_END_BYTES;
+        case EVICTED:   return EVICTED_BYTES;
+      }
+      throw new RuntimeException("Invalid serialization of EntryState");
+    }
+
+    public static EntryState fromBytes(byte [] bytes) {
+      if (bytes.length == 1) {
+        if (bytes[0] == VALID_BYTES[0]) return VALID;
+        if (bytes[0] == INVALID_BYTES[0]) return INVALID;
+        if (bytes[0] == SHARD_END_BYTES[0]) return SHARD_END;
+        if (bytes[0] == EVICTED_BYTES[0]) return EVICTED;
+      }
+      throw new RuntimeException("Invalid deserialization of EntryState");
+    }
+
+    @Override
+    public String toString() {
+      return Objects.toStringHelper(this)
+          .add("state", this.name())
+          .toString();
+    }
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/internal/ExecutionMode.java b/src/main/java/com/continuuity/hbase/ttqueue/internal/ExecutionMode.java
new file mode 100644
index 0000000..a66cb75
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/internal/ExecutionMode.java
@@ -0,0 +1,38 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import com.continuuity.hbase.ttqueue.HBQConfig;
+import com.google.common.base.Objects;
+
+/**
+ * The execution mode of a {@link com.continuuity.hbase.ttqueue.HBQConsumer}s within a group.
+ */
+public enum ExecutionMode {
+  SINGLE_ENTRY,
+  MULTI_ENTRY;
+
+  private static final byte [] SINGLE_BYTES = new byte [] { 0 };
+  private static final byte [] MULTI_BYTES = new byte [] { 1 };
+
+  public byte [] getBytes() {
+    return this == SINGLE_ENTRY ? SINGLE_BYTES : MULTI_BYTES;
+  }
+
+  public static ExecutionMode fromBytes(byte [] bytes) {
+    if (bytes.length == 1) {
+      if (bytes[0] == SINGLE_BYTES[0]) return SINGLE_ENTRY;
+      if (bytes[0] == MULTI_BYTES[0]) return MULTI_ENTRY;
+    }
+    throw new RuntimeException("Invalid deserialization of ExecutionMode");
+  }
+
+  public static ExecutionMode fromQueueConfig(HBQConfig config) {
+    return config.isSingleEntry() ? SINGLE_ENTRY : MULTI_ENTRY;
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("mode", name())
+        .toString();
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/internal/GroupState.java b/src/main/java/com/continuuity/hbase/ttqueue/internal/GroupState.java
new file mode 100644
index 0000000..c3da124
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/internal/GroupState.java
@@ -0,0 +1,127 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+
+import com.continuuity.hbase.ttqueue.HBQEntryPointer;
+import com.google.common.base.Objects;
+
+public class GroupState implements Writable {
+
+  private int groupSize;
+  private HBQEntryPointer head;
+  private ExecutionMode mode;
+
+  /** Writable Constructor - Do not use */
+  public GroupState() {}
+
+  public GroupState(final int groupSize, final HBQEntryPointer head,
+      final ExecutionMode mode) {
+    this.groupSize = groupSize;
+    this.head = head;
+    this.mode = mode;
+  }
+
+  public GroupState(GroupState groupState, HBQEntryPointer entryPointer) {
+    this(groupState.getGroupSize(), entryPointer, groupState.getMode());
+  }
+
+  public int getGroupSize() {
+    return this.groupSize;
+  }
+
+  public HBQEntryPointer getHead() {
+    return this.head;
+  }
+
+  public ExecutionMode getMode() {
+    return this.mode;
+  }
+
+  public byte [] getBytes() {
+    return Bytes.add(
+        Bytes.toBytes(this.groupSize), // 4 bytes
+        this.head.getBytes(),          // 16 bytes
+        this.mode.getBytes());         // 1 byte
+  }
+
+  public static GroupState fromBytes(byte [] bytes) {
+    return new GroupState(Bytes.toInt(bytes),
+        new HBQEntryPointer(Bytes.toLong(bytes, 4), Bytes.toLong(bytes, 12)),
+        ExecutionMode.fromBytes(Bytes.tail(bytes, 1)));
+  }
+
+  /**
+   * Checks if group size and mode are the same.
+   * @param groupState
+   * @return true if non-pointer related configs are same (group size and mode)
+   */
+  public boolean nonPointerConfigEquals(GroupState groupState) {
+    if (groupState.getGroupSize() != getGroupSize()) return false;
+    if (groupState.getMode() != getMode()) return false;
+    return true;
+  }
+
+  /**
+   * Checks if this group state entry pointer is greater than or equal to the
+   * specified group state.
+   * @param groupState
+   * @return true if entry pointer is greater than or equal to that specified
+   */
+  public boolean isGreaterThanOrEqual(GroupState groupState) {
+    if (this.head.getEntryId() >= groupState.head.getEntryId()) return true;
+    return false;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (o == null) return false;
+    if (!(o instanceof GroupState)) return false;
+    GroupState gs = (GroupState)o;
+    if (gs.getGroupSize() != this.groupSize) return false;
+    if (!gs.getHead().equals(this.head)) return false;
+    if (gs.getMode() != this.mode) return false;
+    return true;
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("groupSize", this.groupSize)
+        .add("headPointer", this.head)
+        .add("execMode", this.mode.name())
+        .toString();
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.groupSize = in.readInt();
+    this.head = new HBQEntryPointer();
+    this.head.readFields(in);
+    byte [] modeBytes = new byte[1];
+    in.readFully(modeBytes);
+    this.mode = ExecutionMode.fromBytes(modeBytes);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeInt(this.groupSize);
+    this.head.write(out);
+    out.write(this.mode.getBytes());
+  }
+
+  public JSONObject getJSONObject() throws JSONException {
+    JSONObject outer = new JSONObject();
+    outer.put("groupSize", this.groupSize);
+    outer.put("headEntryId", this.head.getEntryId());
+    outer.put("headShardId", this.head.getShardId());
+    outer.put("execMode", this.mode.name());
+    return outer;
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/internal/QueueStats.java b/src/main/java/com/continuuity/hbase/ttqueue/internal/QueueStats.java
new file mode 100644
index 0000000..241ecf9
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/internal/QueueStats.java
@@ -0,0 +1,150 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.hadoop.io.Writable;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+
+import com.continuuity.hbase.ttqueue.HBQEntryPointer;
+
+public class QueueStats implements Writable {
+
+  private long totalEntriesVisited = 0L;
+  
+  // global entry meta stats
+  public long validEntries = 0L;
+  public long invalidEntries = 0L;
+  public long evictedEntries = 0L;
+  public long shardEndEntries = 0L;
+  
+  // group meta stats
+  private Map<Long,QueueGroupStats> groupStats =
+      new HashMap<Long,QueueGroupStats>();
+  
+  public QueueStats() {}
+
+  public QueueGroupStats getOrCreateGroup(long id) {
+    QueueGroupStats groupStats = this.groupStats.get(id);
+    if (groupStats == null) {
+      groupStats = new QueueGroupStats(id);
+      this.groupStats.put(id, groupStats);
+    }
+    return groupStats;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    // global stats
+    this.totalEntriesVisited = in.readLong();
+    this.validEntries = in.readLong();
+    this.invalidEntries = in.readLong();
+    this.evictedEntries = in.readLong();
+    this.shardEndEntries = in.readLong();
+    // group stats
+    int len = in.readInt();
+    for (int i=0; i<len; i++) {
+      QueueGroupStats stats = new QueueGroupStats();
+      stats.readFields(in);
+      this.groupStats.put(stats.id, stats);
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    // global stats
+    out.writeLong(this.totalEntriesVisited);
+    out.writeLong(this.validEntries);
+    out.writeLong(this.invalidEntries);
+    out.writeLong(this.evictedEntries);
+    out.writeLong(this.shardEndEntries);
+    // group stats
+    out.writeInt(this.groupStats.size());
+    for (QueueGroupStats stats : this.groupStats.values()) {
+      stats.write(out);
+    }
+  }
+
+  public JSONObject getJSONObject() throws JSONException {
+    JSONObject outer = new JSONObject();
+    outer.put("totalEntries", this.totalEntriesVisited);
+    outer.put("validEntries", this.validEntries);
+    outer.put("evictedEntries", this.evictedEntries);
+    outer.put("shardEndEntries", this.shardEndEntries);
+    JSONArray groupArray = new JSONArray();
+    for (QueueGroupStats groupStat : this.groupStats.values()) {
+      groupArray.put(groupStat.getJSONObject());
+    }
+    outer.put("groupStats", groupArray);
+    return outer;
+  }
+
+  public static class QueueGroupStats implements Writable {
+
+    private long id;
+    public long availableEntries = 0L;
+    public long semiAckedEntries = 0L;
+    public long ackedEntries = 0L;
+    public long dequeuedEntries = 0L;
+    public long entriesAfterNonAcked = 0L;
+    public HBQEntryPointer firstNonAckedEntry = null;
+    
+    /** Writable constructor (do not use) */
+    public QueueGroupStats() {}
+    
+    public QueueGroupStats(long groupId) {
+      this.id = groupId;
+    }
+
+    @Override
+    public void readFields(DataInput in) throws IOException {
+      this.id = in.readLong();
+      this.availableEntries = in.readLong();
+      this.semiAckedEntries = in.readLong();
+      this.ackedEntries = in.readLong();
+      this.dequeuedEntries = in.readLong();
+      this.entriesAfterNonAcked = in.readLong();
+      boolean nonAckedExists = in.readBoolean();
+      if (nonAckedExists) {
+        this.firstNonAckedEntry = new HBQEntryPointer();
+        this.firstNonAckedEntry.readFields(in);
+      }
+    }
+
+    @Override
+    public void write(DataOutput out) throws IOException {
+      out.writeLong(this.id);
+      out.writeLong(this.availableEntries);
+      out.writeLong(this.semiAckedEntries);
+      out.writeLong(this.ackedEntries);
+      out.writeLong(this.dequeuedEntries);
+      out.writeLong(this.entriesAfterNonAcked);
+      out.writeBoolean(firstNonAckedEntry != null);
+      if (firstNonAckedEntry != null) {
+        this.firstNonAckedEntry.write(out);
+      }
+    }
+
+    public JSONObject getJSONObject() throws JSONException {
+      JSONObject outer = new JSONObject();
+      outer.put("groupid", this.id);
+      outer.put("availableEntries", this.availableEntries);
+      outer.put("semiAckedEntries", this.semiAckedEntries);
+      outer.put("ackedEntries", this.ackedEntries);
+      outer.put("dequeuedEntries", this.dequeuedEntries);
+      outer.put("entriesAfterNonAcked", this.entriesAfterNonAcked);
+      if (firstNonAckedEntry != null) {
+        JSONObject inner = new JSONObject();
+        inner.put("entryid", firstNonAckedEntry.getEntryId());
+        inner.put("shardid", firstNonAckedEntry.getShardId());
+        outer.put("firstNonAckedEntry", inner);
+      }
+      return outer;
+    }
+  }
+}
diff --git a/src/main/java/com/continuuity/hbase/ttqueue/internal/ShardMeta.java b/src/main/java/com/continuuity/hbase/ttqueue/internal/ShardMeta.java
new file mode 100644
index 0000000..bdb9b42
--- /dev/null
+++ b/src/main/java/com/continuuity/hbase/ttqueue/internal/ShardMeta.java
@@ -0,0 +1,85 @@
+package com.continuuity.hbase.ttqueue.internal;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+
+import com.google.common.base.Objects;
+
+/**
+ * Meta data about the current shard.
+ */
+public class ShardMeta implements Writable {
+
+  private long shardId;
+  private long shardBytes;
+  private long shardEntries;
+
+  /** Writable constructor (do not use) */
+  public ShardMeta() {}
+
+  public ShardMeta(final long shardId, final long shardBytes,
+      final long shardEntries) {
+    this.shardId = shardId;
+    this.shardBytes = shardBytes;
+    this.shardEntries = shardEntries;
+  }
+
+  public long getShardId() {
+    return this.shardId;
+  }
+
+  public long getShardBytes() {
+    return this.shardBytes;
+  }
+
+  public long getShardEntries() {
+    return this.shardEntries;
+  }
+
+  public byte [] getBytes() {
+    return Bytes.add(Bytes.toBytes(this.shardId),
+        Bytes.toBytes(this.shardBytes), Bytes.toBytes(this.shardEntries));
+  }
+
+  public static ShardMeta fromBytes(byte [] bytes) {
+    return new ShardMeta(Bytes.toLong(bytes, 0), Bytes.toLong(bytes, 8),
+        Bytes.toLong(bytes, 16));
+  }
+
+  @Override
+  public String toString() {
+    return Objects.toStringHelper(this)
+        .add("shardId", this.shardId)
+        .add("shardBytes", this.shardBytes)
+        .add("shardEntries", this.shardEntries)
+        .toString();
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.shardId = in.readLong();
+    this.shardBytes = in.readLong();
+    this.shardEntries = in.readLong();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeLong(this.shardId);
+    out.writeLong(this.shardBytes);
+    out.writeLong(this.shardEntries);
+  }
+
+  public JSONObject getJSONObject() throws JSONException {
+    JSONObject outer = new JSONObject();
+    outer.put("shardId", this.shardId);
+    outer.put("shardBytes", this.shardBytes);
+    outer.put("shardEntries", this.shardEntries);
+    return outer;
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/KeyValue.java b/src/main/java/org/apache/hadoop/hbase/KeyValue.java
index bd87c34..ed0fff8 100644
--- a/src/main/java/org/apache/hadoop/hbase/KeyValue.java
+++ b/src/main/java/org/apache/hadoop/hbase/KeyValue.java
@@ -173,6 +173,7 @@ public class KeyValue implements Writable, HeapSize {
 
     Delete((byte)8),
     DeleteColumn((byte)12),
+    UndeleteColumn((byte)13),
     DeleteFamily((byte)14),
 
     // Maximum is used when searching; you look from maximum on down.
diff --git a/src/main/java/org/apache/hadoop/hbase/client/Delete.java b/src/main/java/org/apache/hadoop/hbase/client/Delete.java
index a806f8a..1ec3162 100644
--- a/src/main/java/org/apache/hadoop/hbase/client/Delete.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/Delete.java
@@ -212,6 +212,17 @@ public class Delete extends Mutation
     return this;
   }
 
+  public Delete undeleteColumns(byte [] family, byte [] qualifier, long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if (list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(new KeyValue(this.row, family, qualifier, timestamp,
+      KeyValue.Type.UndeleteColumn));
+    familyMap.put(family, list);
+    return this;
+  }
+
   /**
    * Delete the latest version of the specified column.
    * This is an expensive call in that on the server-side, it first does a
diff --git a/src/main/java/org/apache/hadoop/hbase/client/HTable.java b/src/main/java/org/apache/hadoop/hbase/client/HTable.java
index 9bea85f..ea2ba2d 100644
--- a/src/main/java/org/apache/hadoop/hbase/client/HTable.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/HTable.java
@@ -26,17 +26,15 @@ import java.io.IOException;
 import java.lang.reflect.Proxy;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Collections;
 import java.util.NavigableMap;
 import java.util.TreeMap;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.SynchronousQueue;
-import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -58,6 +56,18 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Threads;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 /**
  * <p>Used to communicate with a single HBase table.
  *
@@ -862,15 +872,25 @@ public class HTable implements HTableInterface {
   /**
    * {@inheritDoc}
    */
+  public boolean checkAndPut(final byte [] row,
+                             final byte [] family, final byte [] qualifier, final byte [] value,
+                             final Put put)
+  throws IOException {
+    return this.checkAndPut(row, family, qualifier, value, Long.MAX_VALUE, put);
+  }
+
+  /**
+   * {@inheritDoc}
+   */
   @Override
   public boolean checkAndPut(final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Put put)
+      final long readVersion, final Put put)
   throws IOException {
     return new ServerCallable<Boolean>(connection, tableName, row, operationTimeout) {
           public Boolean call() throws IOException {
             return server.checkAndPut(location.getRegionInfo().getRegionName(),
-                row, family, qualifier, value, put) ? Boolean.TRUE : Boolean.FALSE;
+                row, family, qualifier, value, readVersion, put) ? Boolean.TRUE : Boolean.FALSE;
           }
         }.withRetries();
   }
@@ -882,13 +902,13 @@ public class HTable implements HTableInterface {
   @Override
   public boolean checkAndDelete(final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Delete delete)
+      final long readVersion, final Delete delete)
   throws IOException {
     return new ServerCallable<Boolean>(connection, tableName, row, operationTimeout) {
           public Boolean call() throws IOException {
             return server.checkAndDelete(
                 location.getRegionInfo().getRegionName(),
-                row, family, qualifier, value, delete)
+                row, family, qualifier, value, readVersion, delete)
             ? Boolean.TRUE : Boolean.FALSE;
           }
         }.withRetries();
@@ -1276,4 +1296,207 @@ public class HTable implements HTableInterface {
     return operationTimeout;
   }
 
+  // Continuuity - HBase Native TTQueue Operations
+
+  @Override
+  public HBQEnqueueResult enqueue(final HBQEnqueue enqueue) throws IOException {
+    byte [] metaRow = enqueue.getMetaRow();
+    final HBQEnqueueResult resultOne = new ServerCallable<HBQEnqueueResult>(
+        connection, tableName, metaRow, operationTimeout) {
+          @Override
+          public HBQEnqueueResult call() throws IOException {
+            return server.enqueue(location.getRegionInfo().getRegionName(),
+                row, enqueue);
+          }
+        }.withRetries();
+    // if failure or data is written, operation is complete
+    if (!resultOne.isSuccess() || resultOne.getDataWritten()) return resultOne;
+    // need to continue operation on another server
+    byte [] shardRow = enqueue.getDataRow(
+        Bytes.toBytes(resultOne.getEntryPointer().getShardId() -
+            (resultOne.getInsertShardEnd() ? 1 : 0)));
+    final HBQEnqueueResult resultTwo = new ServerCallable<HBQEnqueueResult>(
+        connection, tableName, shardRow, operationTimeout) {
+          @Override
+          public HBQEnqueueResult call() throws IOException {
+            return server.enqueue_data(location.getRegionInfo().getRegionName(),
+                row, enqueue, resultOne);
+          }
+        }.withRetries();
+    // if failure or data is written, operation is complete
+    if (!resultTwo.isSuccess() || resultTwo.getDataWritten()) return resultTwo;
+    // the only valid case for this should be that the first result returned a
+    // shard end and the second result didn't
+    if (!resultOne.getInsertShardEnd() || resultTwo.getInsertShardEnd()) {
+      LOG.warn("[HBQ] Fell through to third request but didn't have to write " +
+          "a shard end (resultOne=" + resultOne + ") " +
+          "(resultTwo=" + resultTwo + ")");
+      throw new IOException("Invalid state, three requests required without " +
+          "writing a shard end marker");
+    }
+    // finish operation on another server
+    shardRow = enqueue.getDataRow(
+        Bytes.toBytes(resultTwo.getEntryPointer().getShardId()));
+    HBQEnqueueResult resultThree = new ServerCallable<HBQEnqueueResult>(
+        connection, tableName, shardRow, operationTimeout) {
+          @Override
+          public HBQEnqueueResult call() throws IOException {
+            return server.enqueue_data(location.getRegionInfo().getRegionName(),
+                row, enqueue, resultTwo);
+          }
+        }.withRetries();
+    // regardless of the status of the return, it is the final result
+    return resultThree;
+  }
+
+  @Override
+  public HBQDequeueResult dequeue(final HBQDequeue dequeue) throws IOException {
+    byte [] metaRow = dequeue.getMetaRow();
+    HBQDequeueResult result = new ServerCallable<HBQDequeueResult>(
+        connection, tableName, metaRow, operationTimeout) {
+          @Override
+          public HBQDequeueResult call() throws IOException {
+            return server.dequeue(location.getRegionInfo().getRegionName(),
+                row, dequeue);
+          }
+        }.withRetries();
+    // TODO: determine if we should put a maximum on this loop
+    //       (doing so will change behavior by enforcing a max number of hops)
+    // Keep looping as long as the dequeue is still in progress
+    while (result.isInProgress()) {
+      // need to run dequeue_continue on next row (a shard row or group meta)
+      byte [] nextRow = result.getNextRow();
+      final HBQDequeueResult previousResult = result;
+      // Execute dequeue_continue on region server, passing previous result
+      result = new ServerCallable<HBQDequeueResult>(
+          connection, tableName, nextRow, operationTimeout) {
+            @Override
+            public HBQDequeueResult call() throws IOException {
+              return server.dequeue_continue(
+                  location.getRegionInfo().getRegionName(), row,
+                  previousResult);
+            }
+          }.withRetries();
+    }
+    // TODO: insert some logging and metrics to expose loop behavior
+    return result;
+  }
+
+  @Override
+  public void invalidate(final HBQInvalidate invalidate)
+      throws IOException {
+    byte [] row = invalidate.getDataRow(
+        Bytes.toBytes(invalidate.getEntryPointer().getShardId()));
+    new ServerCallable<Object>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public Object call() throws IOException {
+            server.invalidate(location.getRegionInfo().getRegionName(),
+                row, invalidate);
+            return null;
+          }
+        }.withRetries();
+  }
+
+  @Override
+  public boolean ack(final HBQAck ack) throws IOException {
+    byte [] row = ack.getDataRow(
+        Bytes.toBytes(ack.getEntryPointer().getShardId()));
+    return new ServerCallable<Boolean>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public Boolean call() throws IOException {
+            return server.ack(location.getRegionInfo().getRegionName(), row,
+                ack)
+            ? Boolean.TRUE : Boolean.FALSE;
+          }
+        }.withRetries();
+  }
+
+  @Override
+  public boolean finalize(final HBQFinalize finalize) throws IOException {
+    byte [] row = finalize.getDataRow(
+        Bytes.toBytes(finalize.getEntryPointer().getShardId()));
+    return new ServerCallable<Boolean>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public Boolean call() throws IOException {
+            return server.finalize(location.getRegionInfo().getRegionName(), row,
+                finalize)
+            ? Boolean.TRUE : Boolean.FALSE;
+          }
+        }.withRetries();
+  }
+
+  @Override
+  public boolean unack(final HBQUnack unack) throws IOException {
+    byte [] row = unack.getDataRow(
+        Bytes.toBytes(unack.getEntryPointer().getShardId()));
+    return new ServerCallable<Boolean>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public Boolean call() throws IOException {
+            return server.unack(location.getRegionInfo().getRegionName(), row,
+                unack)
+            ? Boolean.TRUE : Boolean.FALSE;
+          }
+        }.withRetries();
+  }
+
+  /**
+   * Generates and returns a unique group id for this queue.
+   *
+   * Note: uniqueness only guaranteed if you always use this call to generate
+   * groups ids.
+   *
+   * @return a unique group id for this queue
+   */
+  @Override
+  public long getGroupID(final HBQMetaOperation operation) throws IOException {
+    byte [] row = operation.getMetaRow();
+    return new ServerCallable<Long>(connection, tableName, row,
+        operationTimeout) {
+          @Override
+          public Long call() throws IOException {
+            return server.getGroupID(location.getRegionInfo().getRegionName(), row,
+                operation);
+          }
+        }.withRetries();
+  }
+
+  /**
+   * Gets the meta information for this queue.  This includes all meta
+   * data available without walking the entire queue.
+   * @return global meta information for this queue and its groups
+   */
+  @Override
+  public HBQQueueMeta getQueueMeta(final HBQMetaOperation operation)
+      throws IOException {
+    byte [] row = operation.getMetaRow();
+    HBQQueueMeta queueMeta = new ServerCallable<HBQQueueMeta>(connection,
+        tableName, row, operationTimeout) {
+          @Override
+          public HBQQueueMeta call() throws IOException {
+            return server.getQueueMeta(location.getRegionInfo().getRegionName(),
+                row, operation, null);
+          }
+        }.withRetries();
+    while (queueMeta.isInProgress()) {
+      // need to run dequeue_continue on next row (a shard row or group meta)
+      byte [] nextRow = queueMeta.getNextRow();
+      final HBQQueueMeta previousResult = queueMeta;
+      // Execute dequeue_continue on region server, passing previous result
+      queueMeta = new ServerCallable<HBQQueueMeta>(
+          connection, tableName, nextRow, operationTimeout) {
+            @Override
+            public HBQQueueMeta call() throws IOException {
+              return server.getQueueMeta(
+                  location.getRegionInfo().getRegionName(), row,
+                  operation, previousResult);
+            }
+          }.withRetries();
+    }
+    return queueMeta;
+  }
+
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java b/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
index 007f0a3..d59b969 100644
--- a/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
@@ -30,6 +30,18 @@ import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.coprocessor.Batch;
 import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 /**
  * Used to communicate with a single HBase table.
  *
@@ -223,7 +235,7 @@ public interface HTableInterface extends Closeable {
    * @return true if the new put was executed, false otherwise
    */
   boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Put put) throws IOException;
+      byte[] value, long readVersion, Put put) throws IOException;
 
   /**
    * Deletes the specified cells/row.
@@ -261,7 +273,7 @@ public interface HTableInterface extends Closeable {
    * @return true if the new delete was executed, false otherwise
    */
   boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Delete delete) throws IOException;
+      byte[] value, long readVersion, Delete delete) throws IOException;
 
   /**
    * Performs multiple mutations atomically on a single row. Currently
@@ -527,4 +539,22 @@ public interface HTableInterface extends Closeable {
    * @throws IOException if a remote or network exception occurs.
    */
   public void setWriteBufferSize(long writeBufferSize) throws IOException;
+
+  // Continuuity Queue Methods
+
+  HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException;
+
+  HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException;
+
+  void invalidate(HBQInvalidate invalidate) throws IOException;
+
+  boolean ack(HBQAck ack) throws IOException;
+
+  boolean unack(HBQUnack unack) throws IOException;
+
+  boolean finalize(HBQFinalize finalize) throws IOException;
+
+  long getGroupID(HBQMetaOperation operation) throws IOException;
+
+  HBQQueueMeta getQueueMeta(HBQMetaOperation operation) throws IOException;
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java b/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
index 398dc52..2ce2791 100755
--- a/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
@@ -19,22 +19,31 @@
  */
 package org.apache.hadoop.hbase.client;
 
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.List;
-import java.util.Map;
-
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.client.coprocessor.Batch;
-import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
 import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.PoolMap;
 import org.apache.hadoop.hbase.util.PoolMap.PoolType;
 
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+
 /**
  * A simple pool of HTable instances.
  * 
@@ -401,8 +410,8 @@ public class HTablePool implements Closeable {
 
     @Override
     public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-        byte[] value, Put put) throws IOException {
-      return table.checkAndPut(row, family, qualifier, value, put);
+        byte[] value, long readVersion, Put put) throws IOException {
+      return table.checkAndPut(row, family, qualifier, value, readVersion, put);
     }
 
     @Override
@@ -417,8 +426,8 @@ public class HTablePool implements Closeable {
 
     @Override
     public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-        byte[] value, Delete delete) throws IOException {
-      return table.checkAndDelete(row, family, qualifier, value, delete);
+        byte[] value, long readVersion, Delete delete) throws IOException {
+      return table.checkAndDelete(row, family, qualifier, value, readVersion, delete);
     }
 
     @Override
@@ -532,5 +541,48 @@ public class HTablePool implements Closeable {
     public void setWriteBufferSize(long writeBufferSize) throws IOException {
       table.setWriteBufferSize(writeBufferSize);
     }
+
+    // Continuuity Methods (unimplemented)
+
+    @Override
+    public HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException {
+      return null;
+    }
+
+    @Override
+    public HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException {
+      return null;
+    }
+
+    @Override
+    public void invalidate(HBQInvalidate invalidate)
+        throws IOException {
+    }
+
+    @Override
+    public boolean ack(HBQAck ack) throws IOException {
+      return false;
+    }
+
+    @Override
+    public boolean unack(HBQUnack unack) throws IOException {
+      return false;
+    }
+
+    @Override
+    public boolean finalize(HBQFinalize finalize) throws IOException {
+      return false;
+    }
+
+    @Override
+    public long getGroupID(HBQMetaOperation operation) throws IOException {
+      return 0;
+    }
+
+    @Override
+    public HBQQueueMeta getQueueMeta(HBQMetaOperation operation)
+        throws IOException {
+      return null;
+    }
   }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/client/Increment.java b/src/main/java/org/apache/hadoop/hbase/client/Increment.java
index ef0b654..5fdf3ba 100644
--- a/src/main/java/org/apache/hadoop/hbase/client/Increment.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/Increment.java
@@ -27,6 +27,7 @@ import java.util.NavigableMap;
 import java.util.Set;
 import java.util.TreeMap;
 
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.Writable;
@@ -52,6 +53,7 @@ public class Increment implements Row {
   private TimeRange tr = new TimeRange();
   private Map<byte [], NavigableMap<byte [], Long>> familyMap =
     new TreeMap<byte [], NavigableMap<byte [], Long>>(Bytes.BYTES_COMPARATOR);
+  private long writeVersion = HConstants.LATEST_TIMESTAMP;
 
   /** Constructor for Writable.  DO NOT USE */
   public Increment() {}
@@ -101,6 +103,24 @@ public class Increment implements Row {
     return this;
   }
 
+  /**
+   * Sets the write version to use for this increment.
+   * @param writeVersion
+   * @return this Increment object
+   */
+  public Increment setWriteVersion(long writeVersion) {
+    this.writeVersion = writeVersion;
+    return this;
+  }
+
+  /**
+   * Returns the write version to use for this increment.
+   * @return the write version to use
+   */
+  public long getWriteVersion() {
+    return this.writeVersion;
+  }
+
   /* Accessors */
 
   /**
@@ -274,6 +294,7 @@ public class Increment implements Row {
     this.tr = new TimeRange();
     tr.readFields(in);
     this.lockId = in.readLong();
+    this.writeVersion = in.readLong();
     int numFamilies = in.readInt();
     if (numFamilies == 0) {
       throw new IOException("At least one column required");
@@ -307,6 +328,7 @@ public class Increment implements Row {
     Bytes.writeByteArray(out, this.row);
     tr.write(out);
     out.writeLong(this.lockId);
+    out.writeLong(this.writeVersion);
     if (familyMap.size() == 0) {
       throw new IOException("At least one column required");
     }
diff --git a/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java b/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
index 2fdaf6f..6081caf 100644
--- a/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
+++ b/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
@@ -39,6 +39,18 @@ import org.apache.hadoop.hbase.util.VersionInfo;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.io.IOUtils;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 import java.io.File;
 import java.io.FileOutputStream;
 import java.io.IOException;
@@ -415,13 +427,13 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
       }
 
       public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-          byte[] value, Put put) throws IOException {
-        return table.checkAndPut(row, family, qualifier, value, put);
+          byte[] value, long readVersion, Put put) throws IOException {
+        return table.checkAndPut(row, family, qualifier, value, readVersion, put);
       }
 
       public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-          byte[] value, Delete delete) throws IOException {
-        return table.checkAndDelete(row, family, qualifier, value, delete);
+          byte[] value, long readVersion, Delete delete) throws IOException {
+        return table.checkAndDelete(row, family, qualifier, value, readVersion, delete);
       }
 
       public long incrementColumnValue(byte[] row, byte[] family,
@@ -546,6 +558,49 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
       public void setWriteBufferSize(long writeBufferSize) throws IOException {
         table.setWriteBufferSize(writeBufferSize);
       }
+
+      // Continuuity methods (unimplemented)
+
+      @Override
+      public HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException {
+        return null;
+      }
+
+      @Override
+      public HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException {
+        return null;
+      }
+
+      @Override
+      public void invalidate(HBQInvalidate invalidate)
+          throws IOException {
+      }
+
+      @Override
+      public boolean ack(HBQAck ack) throws IOException {
+        return false;
+      }
+
+      @Override
+      public boolean unack(HBQUnack unack) throws IOException {
+        return false;
+      }
+
+      @Override
+      public boolean finalize(HBQFinalize finalize) throws IOException {
+        return false;
+      }
+
+      @Override
+      public long getGroupID(HBQMetaOperation operation) throws IOException {
+        return 0;
+      }
+
+      @Override
+      public HBQQueueMeta getQueueMeta(HBQMetaOperation operation)
+          throws IOException {
+        return null;
+      }
     }
 
     /** The coprocessor */
diff --git a/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java b/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
index d1d1990..0748c01 100644
--- a/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
+++ b/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
@@ -22,8 +22,8 @@ import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
 import java.io.DataInput;
 import java.io.DataOutput;
-import java.io.InputStream;
 import java.io.IOException;
+import java.io.InputStream;
 import java.io.ObjectInputStream;
 import java.io.ObjectOutputStream;
 import java.io.Serializable;
@@ -98,6 +98,24 @@ import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableFactories;
 import org.apache.hadoop.io.WritableUtils;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQConfig;
+import com.continuuity.hbase.ttqueue.HBQConsumer;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQEntryPointer;
+import com.continuuity.hbase.ttqueue.HBQExpirationConfig;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQShardConfig;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+import com.continuuity.hbase.ttqueue.HBReadPointer;
 import com.google.protobuf.Message;
 
 /**
@@ -266,6 +284,9 @@ public class HbaseObjectWritable implements Writable, WritableWithSize, Configur
     GENERIC_ARRAY_CODE = code++;
     addToMap(Array.class, GENERIC_ARRAY_CODE);
 
+    // Continuuity additions
+    code = addContinuuityClassesToMap(code);
+
     // make sure that this is the last statement in this static block
     NEXT_CLASS_CODE = code;
   }
@@ -279,6 +300,28 @@ public class HbaseObjectWritable implements Writable, WritableWithSize, Configur
     super();
   }
 
+  private static int addContinuuityClassesToMap(int code) {
+    addToMap(HBQAck.class, code++);
+    addToMap(HBQConfig.class, code++);
+    addToMap(HBQConsumer.class, code++);
+    addToMap(HBQDequeue.class, code++);
+    addToMap(HBQDequeueResult.class, code++);
+    addToMap(HBQEnqueue.class, code++);
+    addToMap(HBQEnqueueResult.class, code++);
+    addToMap(HBQEntryPointer.class, code++);
+    addToMap(HBQExpirationConfig.class, code++);
+    addToMap(HBQFinalize.class, code++);
+    addToMap(HBQInvalidate.class, code++);
+    addToMap(HBQInvalidateResult.class, code++);
+    addToMap(HBQMetaOperation.class, code++);
+    addToMap(HBQOperation.class, code++);
+    addToMap(HBQQueueMeta.class, code++);
+    addToMap(HBQShardConfig.class, code++);
+    addToMap(HBQUnack.class, code++);
+    addToMap(HBReadPointer.class, code++);
+    return code;
+  }
+
   /**
    * @param instance
    */
diff --git a/src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java b/src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
index d5e329e..57087c7 100644
--- a/src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
+++ b/src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
@@ -53,6 +53,19 @@ import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.ipc.RemoteException;
 import org.apache.hadoop.hbase.ipc.VersionedProtocol;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQEntryPointer;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 /**
  * Clients interact with HRegionServers using a handle to the HRegionInterface.
  *
@@ -223,7 +236,7 @@ public interface HRegionInterface extends VersionedProtocol, Stoppable, Abortabl
    */
   public boolean checkAndPut(final byte[] regionName, final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Put put)
+      final long readVersion, final Put put)
   throws IOException;
 
 
@@ -237,13 +250,14 @@ public interface HRegionInterface extends VersionedProtocol, Stoppable, Abortabl
    * @param family column family
    * @param qualifier column qualifier
    * @param value the expected value
+   * @param readVersion
    * @param delete data to delete if check succeeds
    * @throws IOException e
    * @return true if the new delete was execute, false otherwise
    */
   public boolean checkAndDelete(final byte[] regionName, final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Delete delete)
+      long readVersion, final Delete delete)
   throws IOException;
 
   /**
@@ -633,4 +647,36 @@ public interface HRegionInterface extends VersionedProtocol, Stoppable, Abortabl
 
   @Override
   public void stop(String why);
+
+  // Continuuity Queue Methods
+
+  public HBQEnqueueResult enqueue(byte[] regionName, byte[] row,
+      HBQEnqueue enqueue) throws IOException;
+
+  public HBQEnqueueResult enqueue_data(byte[] regionName, byte[] row,
+      HBQEnqueue enqueue, HBQEnqueueResult previousResult) throws IOException;
+
+  public HBQDequeueResult dequeue(byte[] regionName, byte[] row,
+      HBQDequeue dequeue) throws IOException;
+
+  public HBQDequeueResult dequeue_continue(byte[] regionName, byte[] row,
+      HBQDequeueResult previousResult) throws IOException;
+
+  public void invalidate(byte[] regionName, byte[] row,
+      HBQInvalidate invalidate) throws IOException;
+
+  public boolean ack(byte[] regionName, byte[] row,
+      HBQAck ack) throws IOException;
+
+  public boolean unack(byte[] regionName, byte[] row,
+      HBQUnack unack) throws IOException;
+
+  public boolean finalize(byte[] regionName, byte[] row,
+      HBQFinalize finalize) throws IOException;
+
+  public Long getGroupID(byte[] regionName, byte[] row,
+      HBQMetaOperation operation) throws IOException;
+
+  public HBQQueueMeta getQueueMeta(byte[] regionName, byte[] row,
+      HBQMetaOperation operation, HBQQueueMeta existingMeta) throws IOException;
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 7204a8b..e5db126 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -19,44 +19,14 @@
  */
 package org.apache.hadoop.hbase.regionserver;
 
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.InterruptedIOException;
-import java.io.UnsupportedEncodingException;
-import java.lang.reflect.Constructor;
-import java.lang.reflect.InvocationTargetException;
-import java.lang.reflect.Method;
-import java.text.ParseException;
-import java.util.AbstractList;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableMap;
-import java.util.NavigableSet;
-import java.util.Random;
-import java.util.Set;
-import java.util.TreeMap;
-import java.util.UUID;
-import java.util.concurrent.Callable;
-import java.util.concurrent.CompletionService;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentSkipListMap;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.ExecutorCompletionService;
-import java.util.concurrent.Future;
-import java.util.concurrent.ThreadFactory;
-import java.util.concurrent.ThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicLong;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
+import com.continuuity.hbase.ttqueue.*;
+import com.continuuity.hbase.ttqueue.HBQOperation.HBQOperationType;
+import com.continuuity.hbase.ttqueue.internal.*;
+import com.continuuity.hbase.ttqueue.internal.EntryGroupMeta.EntryGroupState;
+import com.continuuity.hbase.ttqueue.internal.EntryMeta.EntryState;
+import com.continuuity.hbase.ttqueue.internal.QueueStats.QueueGroupStats;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.*;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -91,6 +61,7 @@ import org.apache.hadoop.hbase.client.RowLock;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.coprocessor.Exec;
 import org.apache.hadoop.hbase.client.coprocessor.ExecResult;
+import org.apache.hadoop.hbase.filter.ColumnPrefixFilter;
 import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
 import org.apache.hadoop.hbase.filter.Filter;
 import org.apache.hadoop.hbase.filter.IncompatibleFilterException;
@@ -107,31 +78,31 @@ import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
 import org.apache.hadoop.hbase.regionserver.metrics.OperationMetrics;
+import org.apache.hadoop.hbase.regionserver.metrics.RegionMetricsStorage;
 import org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
 import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.CancelableProgressable;
-import org.apache.hadoop.hbase.util.ClassSize;
-import org.apache.hadoop.hbase.util.CompressionTest;
-import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.apache.hadoop.hbase.util.HashedBytes;
-import org.apache.hadoop.hbase.util.Pair;
-import org.apache.hadoop.hbase.util.Threads;
-import org.apache.hadoop.hbase.util.Writables;
+import org.apache.hadoop.hbase.util.*;
 import org.apache.hadoop.io.MultipleIOException;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.util.StringUtils;
 import org.cliffc.high_scale_lib.Counter;
 
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ClassToInstanceMap;
-import com.google.common.collect.ImmutableList;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
-import com.google.common.collect.MutableClassToInstanceMap;
+import java.io.EOFException;
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.io.UnsupportedEncodingException;
+import java.lang.reflect.Constructor;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.text.ParseException;
+import java.util.*;
+import java.util.concurrent.*;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
 
 /**
  * HRegion stores data for a certain region of a table.  It stores all columns
@@ -1921,6 +1892,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @throws IOException
    * @deprecated Instead use {@link HRegion#batchMutate(Pair[])}
    */
+  @SuppressWarnings("unchecked")
   @Deprecated
   public OperationStatus[] put(Pair<Put, Integer>[] putsAndLocks) throws IOException {
     Pair<Mutation, Integer>[] mutationsAndLocks = new Pair[putsAndLocks.length];
@@ -2322,14 +2294,15 @@ public class HRegion implements HeapSize { // , Writable{
    * @param qualifier
    * @param compareOp
    * @param comparator
+   * @param readVersion
    * @param lockId
    * @param writeToWAL
    * @throws IOException
    * @return true if the new put was execute, false otherwise
    */
   public boolean checkAndMutate(byte [] row, byte [] family, byte [] qualifier,
-      CompareOp compareOp, WritableByteArrayComparable comparator, Writable w,
-      Integer lockId, boolean writeToWAL)
+      CompareOp compareOp, WritableByteArrayComparable comparator,
+      long readVersion, Writable w, Integer lockId, boolean writeToWAL)
   throws IOException{
     checkReadOnly();
     //TODO, add check for value length or maybe even better move this to the
@@ -2350,6 +2323,8 @@ public class HRegion implements HeapSize { // , Writable{
       Get get = new Get(row, lock);
       checkFamily(family);
       get.addColumn(family, qualifier);
+      get.setTimeRange(0, readVersion <= 0 || readVersion == Long.MAX_VALUE ?
+          Long.MAX_VALUE : readVersion + 1);
 
       // Lock row
       Integer lid = getLock(lockId, get.getRow(), true);
@@ -4579,7 +4554,9 @@ public class HRegion implements HeapSize { // , Writable{
       Integer lid = getLock(lockid, row, true);
       this.updatesLock.readLock().lock();
       try {
-        long now = EnvironmentEdgeManager.currentTimeMillis();
+        long now = increment.getWriteVersion() == HConstants.LATEST_TIMESTAMP ?
+            EnvironmentEdgeManager.currentTimeMillis() :
+              increment.getWriteVersion();
         // Process each family
         for (Map.Entry<byte [], NavigableMap<byte [], Long>> family :
           increment.getFamilyMap().entrySet()) {
@@ -5242,4 +5219,2242 @@ public class HRegion implements HeapSize { // , Writable{
        if (bc != null) bc.shutdown();
      }
   }
+
+  // Continuuity Queue Implementation
+
+  private static final String HBQ_METRIC_PREFIX = "hbq_";
+
+  private static final long DIRTY_WRITE = 1L;
+
+  private static final long DIRTY_READ = Long.MAX_VALUE;
+
+  public HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException {
+    byte [] metaRow = enqueue.getMetaRow();
+    checkRow(metaRow, "enqueue");
+    boolean flush = false;
+    boolean metaEnqueued = false;
+    ShardMeta shardMeta = null;
+    boolean moveShard = false;
+    long entryId = -1L;
+    boolean writeToWAL = enqueue.getWriteToWAL();
+    byte [] data = enqueue.getQueueData();
+    WALEdit walEdits = new WALEdit();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    HBQEnqueueResult result = null;
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "enqueue_count", 1);
+
+    // Lock row
+    startRegionOperation();
+    try {
+      Integer metaRowLock = getLock(null, metaRow, true);
+      this.updatesLock.readLock().lock();
+      try {
+
+        long ts_now = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_lock", ts_now - ts_start);
+
+        // Get a unique entry id using existing meta row lock
+        entryId = queueIncrement_existingLock(metaRowLock, metaRow,
+            HBQConstants.GLOBAL_ENTRYID_COUNTER, DIRTY_WRITE, writeToWAL,
+            walEdits, size);
+
+        long ts_entryIdInc = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_entryid_incr", ts_entryIdInc - ts_now);
+
+        // Determine shard state (initialize or read existing)
+        if (entryId == 1) {
+          // This is the first entry in the queue, initialize
+          shardMeta = new ShardMeta(1, data.length, 1);
+        } else {
+          // Read existing, determine updated shard state
+          shardMeta = ShardMeta.fromBytes(queueGet_noLock(metaRow,
+              HBQConstants.GLOBAL_SHARD_META, DIRTY_READ));
+          // Check if we need to move to next shard (max bytes or max entries)
+          if ((shardMeta.getShardBytes() + data.length >
+          enqueue.getShardConfig().getMaxBytesPerShard() &&
+          shardMeta.getShardEntries() > 1) ||
+          shardMeta.getShardEntries() ==
+          enqueue.getShardConfig().getMaxEntriesPerShard()) {
+            // Move to next shard
+            moveShard = true;
+            shardMeta = new ShardMeta(shardMeta.getShardId() + 1,
+                data.length, 1);
+          } else {
+            // Update current shard sizing
+            shardMeta = new ShardMeta(shardMeta.getShardId(),
+                shardMeta.getShardBytes() + data.length,
+                shardMeta.getShardEntries() + 1);
+          }
+        }
+
+        long ts_shardMetaRead = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_shardmeta_read",
+            ts_shardMetaRead - ts_entryIdInc);
+
+        // Write updated shard meta
+        queuePut_existingLock(metaRowLock, metaRow,
+            HBQConstants.GLOBAL_SHARD_META, shardMeta.getBytes(), DIRTY_WRITE,
+            writeToWAL, walEdits, size);
+
+        long ts_shardMetaWrite = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_shardmeta_write",
+            ts_shardMetaWrite - ts_shardMetaRead);
+
+        // Done with processing on global meta row
+        // Finalize and move to data rows
+
+        // Write out to WAL if enabled
+        long ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+        if (writeToWAL) {
+          txid = this.log.appendNoSync(this.regionInfo,
+              this.htableDescriptor.getName(), walEdits,
+              HConstants.DEFAULT_CLUSTER_ID,
+              EnvironmentEdgeManager.currentTimeMillis(),
+              this.htableDescriptor);
+          ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+          RegionMetricsStorage.incrTimeVaryingMetric(
+              HBQ_METRIC_PREFIX + "enqueue_wal_append",
+              ts_walAppend - ts_shardMetaWrite);
+        }
+        metaEnqueued = true;
+      } finally {
+        // Release meta row lock
+        this.updatesLock.readLock().unlock();
+        releaseRowLock(metaRowLock);
+      }
+      long ts_unlock = EnvironmentEdgeManager.currentTimeMillis();
+
+      // Only proceed if we finished with meta enqueue operations
+      if (!metaEnqueued) {
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "enqueue_meta_errors", 1);
+        return new HBQEnqueueResult("Error performing meta operations");
+      }
+      boolean rowMiss = false;
+
+      // If we moved shards, first we need to update previous data shard
+      if (moveShard) {
+        byte [] previousShardRow = enqueue.getDataRow(
+            Bytes.toBytes(shardMeta.getShardId() - 1));
+        if (rowIsInRange(this.regionInfo, previousShardRow)) {
+          // Row of previous data shard is in this region
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_same", 1);
+          TxidAndSize internalResult =
+              enqueue_data_internal(previousShardRow, entryId, writeToWAL,
+                  enqueue.getReadPointer().getWritePointer(),
+                  new EntryMeta(EntryMeta.EntryState.SHARD_END),
+                  new EntryMeta(EntryMeta.EntryState.SHARD_END).getBytes());
+          txid = internalResult.updateTxid(txid);
+          size = internalResult.updateSize(size);
+          long ts_postShardEnd = EnvironmentEdgeManager.currentTimeMillis();
+          RegionMetricsStorage.incrTimeVaryingMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_end_write",
+              ts_postShardEnd - ts_unlock);
+          ts_unlock = ts_postShardEnd;
+        } else {
+          // Row of previous data shard is NOT in this region
+          rowMiss = true;
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_diff", 1);
+          result = new HBQEnqueueResult(
+              new HBQEntryPointer(entryId, shardMeta.getShardId()), true);
+        }
+      }
+
+      // If we haven't missed a row yet, attempt to enqueue in this region
+      if (!rowMiss) {
+        byte [] shardRow = enqueue.getDataRow(
+            Bytes.toBytes(shardMeta.getShardId()));
+        if (rowIsInRange(this.regionInfo, shardRow)) {
+          // Row of data shard is in this region
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_same", 1);
+          TxidAndSize internalResult =
+              enqueue_data_internal(shardRow, entryId, writeToWAL,
+                  enqueue.getReadPointer().getWritePointer(),
+                  new EntryMeta(EntryMeta.EntryState.VALID),
+                  enqueue.getQueueData());
+          txid = internalResult.updateTxid(txid);
+          size = internalResult.updateSize(size);
+          result = new HBQEnqueueResult(
+              new HBQEntryPointer(entryId, shardMeta.getShardId()));
+        } else {
+          // Row of previous data shard is NOT in this region
+          rowMiss = true;
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_diff", 1);
+          result = new HBQEnqueueResult(
+              new HBQEntryPointer(entryId, shardMeta.getShardId()), false);
+        }
+      }
+      // Data block write was done
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "enqueue_data", ts_preSync - ts_unlock);
+
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_walsync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.ENQUEUE,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+
+    return result;
+  }
+
+  private static class TxidAndSize {
+    long txid;
+    long size;
+    TxidAndSize(long txid, long size) {
+      this.txid = txid;
+      this.size = size;
+    }
+    public TxidAndSize() {
+      this(0L, 0L);
+    }
+    long updateTxid(long currentTxid) {
+      if (this.txid > currentTxid) return this.txid;
+      return currentTxid;
+    }
+    public long updateSize(long currentSize) {
+      return currentSize + this.size;
+    }
+    public AtomicLong updateSize(AtomicLong currentSize) {
+      currentSize.addAndGet(this.size);
+      return currentSize;
+    }
+  }
+
+  public HBQEnqueueResult enqueue_data(HBQEnqueue enqueue,
+      HBQEnqueueResult previousResult)
+          throws IOException {
+    // TODO: Add metrics and logging
+    HBQEntryPointer entryPointer = previousResult.getEntryPointer();
+    byte [] shardRow = enqueue.getDataRow(
+        Bytes.toBytes(previousResult.getEntryPointer().getShardId() -
+            (previousResult.getInsertShardEnd() ? 1 : 0)));
+    checkRow(shardRow, "enqueue_data");
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    long size = 0;
+    long txid = 0;
+    boolean flush = false;
+    HBQEnqueueResult result = null;
+
+    startRegionOperation();
+    try {
+
+      // If we moved shards, first we need to update previous data shard
+      if (previousResult.getInsertShardEnd()) {
+        TxidAndSize internalResult =
+            enqueue_data_internal(shardRow, entryPointer.getEntryId(),
+                enqueue.getWriteToWAL(),
+                enqueue.getReadPointer().getWritePointer(),
+                new EntryMeta(EntryMeta.EntryState.SHARD_END),
+                new EntryMeta(EntryMeta.EntryState.SHARD_END).getBytes());
+        txid = internalResult.updateTxid(txid);
+        size = internalResult.updateSize(size);
+      }
+
+      // Attempt to enqueue in this region
+      shardRow = enqueue.getDataRow(Bytes.toBytes(entryPointer.getShardId()));
+      if (rowIsInRange(this.regionInfo, shardRow)) {
+        if (previousResult.getInsertShardEnd()) {
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "enqueue_shard_region_same", 1);
+        }
+        TxidAndSize internalResult =
+            enqueue_data_internal(shardRow, entryPointer.getEntryId(),
+                enqueue.getWriteToWAL(), enqueue.getReadPointer().getWritePointer(),
+                new EntryMeta(EntryMeta.EntryState.VALID),
+                enqueue.getQueueData());
+        txid = internalResult.updateTxid(txid);
+        size = internalResult.updateSize(size);
+        result = new HBQEnqueueResult(entryPointer);
+      } else {
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "enqueue_shard_region_diff", 1);
+        result = new HBQEnqueueResult(entryPointer, false);
+      }
+
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (enqueue.getWriteToWAL() && txid > 0) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_data_walsync",
+            ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.ENQUEUE,
+        ts_end - ts_start);
+
+    if (flush) {
+      requestFlush();
+    }
+    return result;
+  }
+
+  private TxidAndSize enqueue_data_internal(byte[] shardRow,
+      long entryId, boolean writeToWAL, long writePointer, EntryMeta entryMeta,
+      byte[] queueData)
+          throws IOException {
+    checkRow(shardRow, "enqueue_data_internal");
+    WALEdit walEdits = writeToWAL ? new WALEdit() : null;
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+
+    // Lock data row
+    Integer shardRowLock = getLock(null, shardRow, true);
+    this.updatesLock.readLock().lock();
+    try {
+
+      long ts_now = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "enqueue_data_lock", ts_now - ts_start);
+
+      if (entryMeta.isEndOfShard()) {
+        // Write end of shard meta only
+        queuePut_existingLock(shardRowLock, shardRow, dataMetaColumn(entryId),
+            entryMeta.getBytes(), writePointer,writeToWAL, walEdits, size);
+      } else {
+        // Write shard meta and actual data
+        queuePut_existingLock(shardRowLock, shardRow,
+            new byte [][] { dataMetaColumn(entryId), dataDataColumn(entryId) },
+            new byte [][] { entryMeta.getBytes(), queueData }, writePointer,
+            writeToWAL, walEdits, size);
+      }
+
+      long ts_dataWrite = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "enqueue_data_write",
+          ts_dataWrite - ts_now);
+
+      // Write out to WAL if enabled
+      long ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        txid = this.log.appendNoSync(this.regionInfo,
+            this.htableDescriptor.getName(), walEdits,
+            HConstants.DEFAULT_CLUSTER_ID,
+            EnvironmentEdgeManager.currentTimeMillis(),
+            this.htableDescriptor);
+        ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "enqueue_data_wal_append",
+            ts_walAppend - ts_dataWrite);
+      }
+    } finally {
+      // Release meta row lock
+      this.updatesLock.readLock().unlock();
+      releaseRowLock(shardRowLock);
+    }
+    return new TxidAndSize(txid, size.get());
+  }
+
+  /**
+   * Performs a dequeue operation
+   * @param dequeue
+   * @return
+   * @throws IOException
+   */
+  public HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException {
+    byte [] groupMetaRow = dequeue.getGroupMetaRow();
+    checkRow(groupMetaRow, "dequeue");
+    boolean flush = false;
+    boolean writeToWAL = dequeue.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    long size = 0;
+    long txid = 0;
+    boolean groupReconfigure = false;
+    HBQConsumer consumer = dequeue.getConsumer();
+    HBQConfig config = dequeue.getConfig();
+    HBQDequeueResult result = null;
+    GroupState groupState = null;
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "dequeue_count", 1);
+
+    // Lock region
+    startRegionOperation();
+    try {
+      // Need to determine the group state for the dequeueing group
+      byte [] groupStateColumn = dequeue.getGroupMetaColumn();
+
+      // Before locking, be optimistic and do a dirty read of group meta
+      // to see if it already matches the current consumers configuration
+      byte [] existingValue = queueGet_noLock(groupMetaRow, groupStateColumn,
+          DIRTY_READ);
+
+      if (existingValue == null) {
+        // Group meta does not exist, initialize with new group state
+        groupState = new GroupState(consumer.getGroupSize(),
+            new HBQEntryPointer(1, 1), ExecutionMode.fromQueueConfig(config));
+        // TODO: For now, this will throw an exception if the compare-and-swap
+        //       fails and the value that appears is not the state we are
+        //       trying to change it to
+        TxidAndSize txidAndSize = updateGroupState(dequeue, null, groupState,
+            writeToWAL);
+        txid = txidAndSize.updateTxid(txid);
+        size = txidAndSize.updateSize(size);
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "dequeue_init_group", 1);
+      } else {
+        // Group meta exists, see if it's what we expect it to be
+        groupState = GroupState.fromBytes(existingValue);
+        if (groupState.getGroupSize() != consumer.getGroupSize() ||
+            groupState.getMode() != ExecutionMode.fromQueueConfig(config)) {
+          // Group configuration has changed!  Need to perform group reconfig
+          groupReconfigure = true;
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "dequeue_group_reconfig", 1);
+        }
+        // Group size and execution mode are the same so no changes needed
+      }
+
+      // Existing group checking done, now proceed with reconfig then dequeue
+      // or straight to data dequeue
+
+      Pair<HBQDequeueResult, TxidAndSize> resultAndStats = null;
+
+      if (groupReconfigure) {
+
+        // Group reconfiguration
+
+        // Need to see if there are pending entries.  Group configuration
+        // changes are only permitted when there are no queue entries
+        // currently dequeued and un-acked.  If this completes, it will continue
+        // to a normal dequeue operation.
+
+        resultAndStats = attemptGroupReconfigure(dequeue,
+            groupState,groupState.getHead(), writeToWAL);
+
+      } else {
+
+        // GroupState is ready to go
+
+        // Starting from group head pointer, iterate through entries and shards
+        // to find the first available entry for the dequeueing consumer, or
+        // reach the end of the queue and return as empty
+
+        resultAndStats = dequeue_data_internal(dequeue, groupState,
+            groupState.getHead(), writeToWAL);
+
+      }
+
+      // Set the result and update txid and size of any writes performed
+      result = resultAndStats.getFirst();
+      TxidAndSize txidAndSize = resultAndStats.getSecond();
+      txidAndSize.updateSize(size);
+      txidAndSize.updateTxid(txid);
+
+      // Check if group state needs to be updated (row is in this region so
+      // might as well just do it now, whether or not dequeue is done)
+      if (result.groupStateNeedsToBeUpdated()) {
+        txidAndSize = updateGroupState(dequeue,
+            result.getCurrentGroupState(), writeToWAL);
+        txidAndSize.updateSize(size);
+        txidAndSize.updateTxid(txid);
+        // TODO: should the update return the current group state?  would be
+        //       more optimal to always have the most up-to-date pointer
+        result.groupStateUpdated();
+      }
+
+      // Sync the WAL to highest txid of wal edits written in this dequeue
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL && txid > 0) {
+        this.log.sync(txid); // sync the transaction log up to highest txid
+        long ts_sync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "dequeue_walsync",
+            ts_sync - ts_preSync);
+      }
+
+      // Check if we need to flush the region
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.DEQUEUE,
+        ts_end - ts_start);
+
+    // TODO: Insert statistical metrics around whether data was found in this
+    //       request, group was updated, more hops necessary, etc
+
+    if (flush) {
+      requestFlush();
+    }
+    return result;
+  }
+
+  /**
+   *
+   * @param previousResult
+   * @return
+   * @throws IOException
+   */
+  public HBQDequeueResult dequeue_continue(HBQDequeueResult previousResult)
+      throws IOException {
+    HBQDequeue dequeue = previousResult.getDequeue();
+    HBQDequeueResult result = null;
+    boolean writeToWAL = dequeue.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    long size = 0;
+    long txid = 0;
+    boolean flush = false;
+    GroupState updatedGroupState = null;
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "dequeue_count", 1);
+
+    // Only in-progress dequeues should be called here
+    if (!previousResult.isInProgress()) {
+      throw new IOException("Received not-in-progress dequeue in " +
+          "dequeue_continue operation");
+    }
+
+    // Lock region
+    startRegionOperation();
+    try {
+
+      // Check if data already found and this is just a group state update
+      if (previousResult.dataEntrySearchDone()) {
+
+        // The data entry has already been found, this request is just to
+        // update the group state which is expected to be on this region
+
+        // Required to need a group state update, otherwise invalid request
+        if (!previousResult.groupStateNeedsToBeUpdated()) {
+          throw new IOException("Received in-progress dequeue with data entry " +
+              "found but group does not need to be updated, invalid state");
+        }
+
+        // Verify the group state row is on this region
+        byte [] groupMetaRow = dequeue.getGroupMetaRow();
+        checkRow(groupMetaRow, "dequeue_continue");
+
+        // Update the group state
+        updatedGroupState = previousResult.getCurrentGroupState();
+        result = previousResult;
+
+      } else if (previousResult.checkingPendingEntries()) {
+
+        // Request is a group reconfiguration check
+        Pair<HBQDequeueResult, TxidAndSize> resultAndStats =
+            attemptGroupReconfigure(dequeue,
+                previousResult.getCurrentGroupState(),
+                previousResult.getCurrentEntryPointer(), writeToWAL);
+
+        result = resultAndStats.getFirst();
+        TxidAndSize txidAndSize = resultAndStats.getSecond();
+        txidAndSize.updateSize(size);
+        txidAndSize.updateTxid(txid);
+
+        // Check if group state needs to be updated and the group meta row
+        // is on this region, and if so, update group state here
+        if (result.groupStateNeedsToBeUpdated() &&
+            rowIsInRange(this.regionInfo, dequeue.getGroupMetaRow())) {
+          updatedGroupState = result.getCurrentGroupState();
+        }
+
+
+      } else {
+
+        // Request is to continue the search for a queue data entry
+
+        // Starting from current pointer in previous result, iterate through
+        // entries and shards to find the first available entry for the
+        // dequeueing consumer, or reach the end of the queue and return as
+        // empty
+
+        // The current group state for this dequeue call will be the current
+        // state from any prior processing
+        GroupState currentGroupState = previousResult.getCurrentGroupState();
+
+        // Perform dequeue, following all shards that fall within this region
+        Pair<HBQDequeueResult, TxidAndSize> resultAndStats =
+            dequeue_data_internal(dequeue, currentGroupState,
+                previousResult.getCurrentEntryPointer(), writeToWAL);
+
+        result = resultAndStats.getFirst();
+        TxidAndSize txidAndSize = resultAndStats.getSecond();
+        txidAndSize.updateSize(size);
+        txidAndSize.updateTxid(txid);
+
+        // Check if group state needs to be updated and the group meta row
+        // is on this region, and if so, update group state here
+        if (result.groupStateNeedsToBeUpdated() &&
+            rowIsInRange(this.regionInfo, dequeue.getGroupMetaRow())) {
+          updatedGroupState = result.getCurrentGroupState();
+        }
+
+      }
+
+      // If the group state was set to be updated, do it here (region boundary
+      // checks have already been done)
+      if (updatedGroupState != null) {
+        TxidAndSize txidAndSize = updateGroupState(dequeue, updatedGroupState,
+            writeToWAL);
+        txidAndSize.updateSize(size);
+        txidAndSize.updateTxid(txid);
+        // TODO: should the update return the current group state?  would be
+        //       more optimal to always have the most up-to-date pointer
+        result.groupStateUpdated();
+      }
+
+      // Sync the WAL to highest txid of wal edits written, if enabled
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL && txid > 0) {
+        this.log.sync(txid); // sync the transaction log up to highest txid
+        long ts_sync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "dequeue_continue_walsync",
+            ts_sync - ts_preSync);
+      }
+
+      // Check if we need to flush the region
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.DEQUEUE_CONTINUE,
+        ts_end - ts_start);
+
+    // TODO: Insert statistical metrics around whether data was found in this
+    //       request, group was updated, more hops necessary, etc
+
+    if (flush) {
+      requestFlush();
+    }
+    return result;
+  }
+
+  /**
+   * Attempts a dequeue given the specified dequeue operation and previously
+   * determined group state.
+   *
+   * Starting from the specified entry pointer, iterates through entries and
+   * shards to find the first available entry for the dequeueing consumer, or
+   * until it reaches the end of the queue and return as empty.
+   *
+   * This method also determines if the group head pointer needs to be updated,
+   * and if so, will store the final group state in the result, and the caller
+   * will be responsible for performing the update.  It is impossible
+   * for this update to hard-fail (if it does, there is a bug or invalid
+   * behavior by consumers).
+   *
+   * This method should be called while holding the overall region operation
+   * lock, but no other locks (row lock, updates lock).  Row locks will be
+   * acquired within this method on shard rows that are written to.
+   *
+   * All writes (marking of the data block meta) will be performed according to
+   * the specified write-to-wal flag and written to the wal under a row lock and
+   * with append-no-sync.  The txid and aggregate size of writes will be
+   * returned.  It is the responsibility of the caller to synchronize and check
+   * if a region flush should be performed.
+   *
+   * @param dequeue
+   * @param groupState
+   * @param groupHeadPointer
+   * @return
+   * @throws IOException
+   */
+  private Pair<HBQDequeueResult, TxidAndSize> dequeue_data_internal(
+      HBQDequeue dequeue, GroupState currentGroupState,
+      HBQEntryPointer currentEntryPointer, boolean writeToWAL)
+          throws IOException {
+    HBQConsumer consumer = dequeue.getConsumer();
+    HBQConfig config = dequeue.getConfig();
+    HBQPartitioner partitioner = config.getPartitionerType().getPartitioner();
+    GroupState initialGroupState = currentGroupState;
+    boolean skippedEntry = false;
+    long visitedEntries = 0L;
+
+    EntryMeta entryMeta = null;
+    EntryGroupMeta entryGroupMeta = null;
+    HBQEntryPointer entryPointer = currentEntryPointer;
+    long previousShardId = -1;
+    // TODO: Should there be a max retry on here?  This would effectively
+    //       change behavior and cap the maximum number of unacked entries
+    //       (but would you return empty or some other code like "full/max"?)
+    while (entryPointer != null) {
+
+      // We are pointed at {entryPointer}=(shardid,entryid) and we are either
+      // at the head of this group or we have skipped everything between where
+      // we are and the head.
+      visitedEntries++;
+
+      byte [] shardRow = dequeue.getDataRow(
+          Bytes.toBytes(entryPointer.getShardId()));
+
+      if (previousShardId != entryPointer.getShardId()) {
+        // Moved to a new shard
+
+        // Check if shard is in the same region we are currently in
+        if (!rowIsInRange(this.regionInfo, shardRow)) {
+          // Row of the current shard is not on this region, need to return
+          // back to region server to continue dequeue
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "dequeue_region_hop", 1);
+          return new Pair<HBQDequeueResult, TxidAndSize>(
+              new HBQDequeueResult(dequeue, entryPointer, initialGroupState,
+                  currentGroupState),
+                  new TxidAndSize());
+        } else {
+          previousShardId = entryPointer.getShardId();
+        }
+      }
+
+      byte [] entryMetaColumn = dataMetaColumn(entryPointer.getEntryId());
+      byte [] entryDataColumn = dataDataColumn(entryPointer.getEntryId());
+
+      // Do a dirty read of the current entry meta data
+      byte [] entryMetaBytes = queueGet_noLock(shardRow, entryMetaColumn,
+          DIRTY_READ);
+
+      if (entryMetaBytes == null) {
+        // This entry doesn't exist or is not visible
+        // so queue is empty for this consumer
+
+        // Note, this is a slight change in behavior from the existing TTQ
+        // implementation which would return a RETRY if it found uncommitted
+        // data rather than just returning EMPTY now which seems like correct
+        // behavior
+
+        // Return empty with both group states (constructor detects if change)
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "dequeue_visited_empty", visitedEntries);
+        return new Pair<HBQDequeueResult, TxidAndSize>(
+            new HBQDequeueResult(dequeue, null, null, initialGroupState,
+                currentGroupState),
+                new TxidAndSize());
+      }
+
+      // Queue entry exists and is visible, check the global state of it
+      entryMeta = EntryMeta.fromBytes(entryMetaBytes);
+
+      // Check if this entry is not a valid data entry
+      if (!entryMeta.isValid()) {
+        // Entry is invalid in some way.  We will skip it and move to next.
+        HBQEntryPointer nextEntryPointer = null;
+
+        if (entryMeta.isEndOfShard()) {
+          // If entry is an end-of-shard marker, move to next shard
+          nextEntryPointer = nextShard(entryPointer);
+        } else {
+          // Otherwise for invalidated/evicted, just move to next entry
+          nextEntryPointer = nextEntry(entryPointer);
+        }
+
+        // Before moving to next entry, check if group state can be updated
+        currentGroupState = attemptGroupPointerUpdate(skippedEntry,
+            currentGroupState, nextEntryPointer);
+
+        // Move on to the next entry
+        entryPointer = nextEntryPointer;
+        continue;
+      }
+
+      // Entry is visible and valid
+
+      // Do a dirty read of entry group meta data
+      byte [] entryGroupMetaColumn = dataGroupMetaColumn(
+          entryPointer.getEntryId(), consumer.getGroupId());
+      byte [] entryGroupMetaBytes = queueGet_noLock(shardRow,
+          entryGroupMetaColumn, DIRTY_READ);
+
+      if (entryGroupMetaBytes == null) {
+        // Group has not processed entry, consider available
+        entryGroupMeta = null;
+
+      } else {
+        // Group has already processed entry, check status
+        entryGroupMeta = EntryGroupMeta.fromBytes(entryGroupMetaBytes);
+
+        HBQEntryPointer nextEntryPointer = null;
+
+        // First check if entry is already acked/semi-acked (can be skipped)
+
+        if (entryGroupMeta.isAckedOrSemiAcked()) {
+
+          // Group has already acked this entry, move to next entry
+          nextEntryPointer = nextEntry(entryPointer);
+
+          // Before moving to next entry, if acked or semi-acked+timed_out,
+          // check for group state update
+          if (ackedOrTimedOutSemiAcked(entryGroupMeta,
+              dequeue.getExpirationConfig().getMaxAgeBeforeSemiAckedToAcked()))
+          {
+            currentGroupState = attemptGroupPointerUpdate(skippedEntry,
+                currentGroupState, nextEntryPointer);
+          } else {
+            // There is a semi-acked entry, we cannot safely move head pointer
+            // at this point or beyond
+            skippedEntry = true;
+          }
+
+          // Entry is acked, moved to next
+          entryPointer = nextEntryPointer;
+          continue;
+        }
+
+        // Second check if entry is already dequeued by current group
+        // (skipped if not timed out and either a different consumer or in
+        // multi-entry mode, or returned if same consumer and single entry mode
+        // or if timed out)
+
+        if (entryGroupMeta.isDequeued()) {
+
+          // Check if current consumer is in single entry mode and is the
+          // instance that already dequeued this entry
+          if (config.isSingleEntry() &&
+              entryGroupMeta.getInstanceId() == consumer.getInstanceId()) {
+            // Same consumer, give back the already dequeued but not acked entry
+            TxidAndSize txidAndSize = attemptToClaimEntry(dequeue, entryPointer,
+                entryGroupMeta, new EntryGroupMeta(EntryGroupState.DEQUEUED,
+                    now(), consumer.getInstanceId()));
+            if (txidAndSize == null) {
+              // Failed to claim entry, another consumer won,
+              // skip and move to next entry
+              skippedEntry = true;
+              entryPointer = nextEntry(entryPointer);
+              continue;
+            } else {
+              // Entry claimed.  Read and return entry.
+              byte [] queueData = queueGet_noLock(shardRow,
+                  dataDataColumn(entryPointer.getEntryId()), DIRTY_READ);
+              return new Pair<HBQDequeueResult, TxidAndSize>(
+                  new HBQDequeueResult(dequeue, entryPointer, queueData,
+                      initialGroupState, currentGroupState), txidAndSize);
+            }
+          }
+
+          // We should move to the next entry unless the dequeue is timed out
+          if (entryGroupMeta.getTimestamp() +
+              dequeue.getExpirationConfig().getMaxAgeBeforeExpirationInMillis()
+              >= now()) {
+            // Dequeue is not timed out, skip and move to next entry
+            skippedEntry = true;
+            entryPointer = nextEntry(entryPointer);
+            continue;
+          }
+
+          // Dequeued and timed-out, treat as available
+          RegionMetricsStorage.incrNumericMetric(
+              HBQ_METRIC_PREFIX + "dequeue_timed_out", 1);
+        }
+
+        // If we get here, the entry is available
+      }
+
+      // Entry is available for this consumer and group
+
+      // Do a dirty read of the data
+      byte [] queueData = queueGet_noLock(shardRow, entryDataColumn, DIRTY_READ);
+
+      // Check against partitioner
+      if (!partitioner.shouldEmit(consumer, entryPointer.getEntryId(),
+          queueData)) {
+        // Entry not valid for this consumer, skip to next entry
+        skippedEntry = true;
+        entryPointer = nextEntry(entryPointer);
+        continue;
+      }
+
+      // Attempt to update group meta to be dequeued by this consumer
+      TxidAndSize txidAndSize = attemptToClaimEntry(dequeue, entryPointer,
+          entryGroupMeta, new EntryGroupMeta(EntryGroupState.DEQUEUED,
+              now(), consumer.getInstanceId()));
+      if (txidAndSize == null) {
+        // Failed to claim entry, another consumer won,
+        // skip and move to next entry
+        skippedEntry = true;
+        entryPointer = nextEntry(entryPointer);
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "dequeue_failed_claim", 1);
+        continue;
+      } else {
+        // Entry claimed, return entry.
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "dequeue_visited", visitedEntries);
+        return new Pair<HBQDequeueResult, TxidAndSize>(
+            new HBQDequeueResult(dequeue, entryPointer, queueData,
+                initialGroupState, currentGroupState), txidAndSize);
+      }
+    }
+    // Logic should never fall through here
+    throw new IOException("Reached invalid state in dequeue, fell out of loop");
+  }
+
+  public void invalidate(HBQInvalidate invalidate) throws IOException {
+    byte [] shardRow = invalidate.getDataRow(
+        Bytes.toBytes(invalidate.getEntryPointer().getShardId()));
+    checkRow(shardRow, "invalidate");
+    boolean flush = false;
+    boolean writeToWAL = invalidate.getWriteToWAL();
+    WALEdit walEdits = writeToWAL ? new WALEdit() : null;
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    long entryId = invalidate.getEntryPointer().getEntryId();
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "invalidate_count", 1);
+
+    // Lock row
+    startRegionOperation();
+    try {
+      Integer shardRowLock = getLock(null, shardRow, true);
+      this.updatesLock.readLock().lock();
+      try {
+
+        long ts_now = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "invalidate_lock", ts_now - ts_start);
+
+        // Update meta data for data entry to INVALID
+        queuePut_existingLock(shardRowLock, shardRow, dataMetaColumn(entryId),
+            new EntryMeta(EntryState.INVALID).getBytes(),
+            invalidate.getPointer().getWritePointer(), writeToWAL, walEdits,
+            size);
+
+        // Delete data column since it's invalidated
+        queueDelete_existingLock(shardRowLock, shardRow,
+            dataDataColumn(entryId), invalidate.getPointer().getWritePointer(),
+            writeToWAL, walEdits, size);
+
+        long ts_dataWrite = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "invalidate_put-and-delete",
+            ts_dataWrite - ts_now);
+
+        // Write out to WAL if enabled
+        long ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+        if (writeToWAL) {
+          txid = this.log.appendNoSync(this.regionInfo,
+              this.htableDescriptor.getName(), walEdits,
+              HConstants.DEFAULT_CLUSTER_ID,
+              EnvironmentEdgeManager.currentTimeMillis(),
+              this.htableDescriptor);
+          ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+          RegionMetricsStorage.incrTimeVaryingMetric(
+              HBQ_METRIC_PREFIX + "invalidate_wal-append",
+              ts_walAppend - ts_dataWrite);
+        }
+      } finally {
+        // Release meta row lock
+        this.updatesLock.readLock().unlock();
+        releaseRowLock(shardRowLock);
+      }
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "invalidate_wal-sync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.INVALIDATE,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+  }
+
+  public boolean ack(HBQAck ack) throws IOException {
+    HBQEntryPointer entryPointer = ack.getEntryPointer();
+    byte [] shardRow = ack.getDataRow(Bytes.toBytes(entryPointer.getShardId()));
+    checkRow(shardRow, "ack");
+    boolean flush = false;
+    boolean writeToWAL = ack.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    HBQConsumer consumer = ack.getConsumer();
+
+    RegionMetricsStorage.incrNumericMetric(HBQ_METRIC_PREFIX + "ack_count", 1);
+
+    startRegionOperation();
+    try {
+
+      // Perform dirty read of entry group meta for this entry
+      byte [] entryGroupMetaColumn = dataGroupMetaColumn(
+          entryPointer.getEntryId(), consumer.getGroupId());
+      byte [] entryGroupMetaBytes = queueGet_noLock(shardRow,
+          entryGroupMetaColumn, DIRTY_READ);
+
+      EntryGroupMeta groupMeta = EntryGroupMeta.fromBytes(entryGroupMetaBytes);
+
+      // Check if instance id matches
+      if (groupMeta.getInstanceId() != consumer.getInstanceId()) {
+        LOG.warn("Attempt to ack an entry of a different consumer instance.");
+        return false;
+      }
+
+      // Instance ids match, check if in an invalid state for ack'ing
+      if (groupMeta.isAvailable() || groupMeta.isAckedOrSemiAcked()) {
+        LOG.warn("Attempt to ack an entry that is not in ack'able state.");
+        return false;
+      }
+
+      // It is in the right state, attempt atomic semi_ack
+      // (ack passed if this CAS works, fails if this CAS fails)
+      byte [] newValue = new EntryGroupMeta(EntryGroupState.SEMI_ACKED,
+          now(), consumer.getInstanceId()).getBytes();
+
+      TxidAndSize txidAndSize = queueCAS(shardRow, entryGroupMetaColumn,
+          entryGroupMetaBytes, newValue, writeToWAL, null);
+
+      // If cas failed, fail ack
+      if (txidAndSize == null) {
+        LOG.warn("Atomic update from dequeued to semi-acked failed");
+        return false;
+      }
+
+      // CAS was successful, update txid and size, sync log, and finish
+
+      txid = txidAndSize.updateTxid(txid);
+      size = txidAndSize.updateSize(size);
+
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "ack_walsync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.ACK,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+
+    return true;
+  }
+
+  public boolean finalize(HBQFinalize finalize) throws IOException {
+    HBQEntryPointer entryPointer = finalize.getEntryPointer();
+    byte [] shardRow = finalize.getDataRow(
+        Bytes.toBytes(entryPointer.getShardId()));
+    checkRow(shardRow, "finalize");
+    boolean flush = false;
+    boolean writeToWAL = finalize.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    HBQConsumer consumer = finalize.getConsumer();
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "finalize_count", 1);
+
+    startRegionOperation();
+    try {
+
+      // Perform dirty read of entry group meta for this entry
+      byte [] entryGroupMetaColumn = dataGroupMetaColumn(
+          entryPointer.getEntryId(), consumer.getGroupId());
+      byte [] entryGroupMetaBytes = queueGet_noLock(shardRow,
+          entryGroupMetaColumn, DIRTY_READ);
+
+      if (entryGroupMetaBytes == null) {
+        LOG.warn("No existing entry group meta found on finalize");
+        return false;
+      }
+
+      EntryGroupMeta groupMeta = EntryGroupMeta.fromBytes(entryGroupMetaBytes);
+
+      // Should only be in semi-acked state
+      if (!groupMeta.isSemiAcked()) {
+        LOG.warn("Attempt to finalize an entry that is not in semi-acked " +
+            "state");
+        return false;
+      }
+
+      // It is in the right state, attempt atomic semi_ack to ack transition
+      // (finalize passed if this CAS works, fails if this CAS fails)
+      byte [] newValue = new EntryGroupMeta(EntryGroupState.ACKED,
+          now(), consumer.getInstanceId()).getBytes();
+
+      TxidAndSize txidAndSize = queueCAS(shardRow, entryGroupMetaColumn,
+          entryGroupMetaBytes, newValue, writeToWAL, null);
+
+      // If cas failed, fail finalize
+      if (txidAndSize == null) {
+        LOG.warn("Atomic update from semi-acked to acked failed");
+        return false;
+      }
+
+      // CAS successful, update txid and size
+
+      txid = txidAndSize.updateTxid(txid);
+      size = txidAndSize.updateSize(size);
+
+      // Perform evict on ack if all groups done
+      int numGroups = finalize.getTotalNumGroups();
+      if (numGroups == 1 ||
+          (numGroups > 0 && allOtherGroupsFinalized(shardRow, entryPointer,
+              numGroups, consumer.getGroupId()))) {
+        // All groups are finalized, evict data block!
+        txidAndSize = evictDataBlock(shardRow, entryPointer, writeToWAL);
+        txid = txidAndSize.updateTxid(txid);
+        size = txidAndSize.updateSize(size);
+      }
+
+      // Sync log, and finish
+
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "finalize_walsync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.FINALIZE,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+
+    return true;
+  }
+
+  public boolean unack(HBQUnack unack) throws IOException {
+    HBQEntryPointer entryPointer = unack.getEntryPointer();
+    byte [] shardRow = unack.getDataRow(
+        Bytes.toBytes(entryPointer.getShardId()));
+    checkRow(shardRow, "unack");
+    boolean flush = false;
+    boolean writeToWAL = unack.getWriteToWAL();
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+    HBQConsumer consumer = unack.getConsumer();
+
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "unack_count", 1);
+
+    startRegionOperation();
+    try {
+
+      // Perform dirty read of entry group meta for this entry
+      byte [] entryGroupMetaColumn = dataGroupMetaColumn(
+          entryPointer.getEntryId(), consumer.getGroupId());
+      byte [] entryGroupMetaBytes = queueGet_noLock(shardRow,
+          entryGroupMetaColumn, DIRTY_READ);
+
+      if (entryGroupMetaBytes == null) {
+        throw new IOException("No existing entry group meta found on unack");
+      }
+
+      EntryGroupMeta groupMeta = EntryGroupMeta.fromBytes(entryGroupMetaBytes);
+
+      // Should only be in semi-acked state
+      if (!groupMeta.isSemiAcked()) {
+        LOG.warn("Attempt to unack an entry that is not in semi-acked state.");
+        return false;
+      }
+
+      // It is in the right state, attempt atomic semi_ack to dequeued transition
+      // (finalize passed if this CAS works, fails if this CAS fails)
+      byte [] newValue = new EntryGroupMeta(EntryGroupState.DEQUEUED,
+          now(), consumer.getInstanceId()).getBytes();
+
+      TxidAndSize txidAndSize = queueCAS(shardRow, entryGroupMetaColumn,
+          entryGroupMetaBytes, newValue, writeToWAL, null);
+
+      // If cas failed, fail unack
+      if (txidAndSize == null) {
+        LOG.warn("Atomic update from semi-acked to dequeued failed");
+        return false;
+      }
+
+      // CAS was successful, update txid and size, sync log, and finish
+
+      txid = txidAndSize.updateTxid(txid);
+      size = txidAndSize.updateSize(size);
+
+      long ts_preSync = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        this.log.sync(txid); // sync the transaction log outside the rowlock
+        long ts_writeSync = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "unack_walsync", ts_writeSync - ts_preSync);
+      }
+      flush = isFlushSize(addAndGetGlobalMemstoreSize(size.get()));
+    } finally {
+      closeRegionOperation();
+    }
+
+    long ts_end = EnvironmentEdgeManager.currentTimeMillis();
+    this.opMetrics.updateQueueMetrics(HBQOperationType.UNACK,
+        ts_end - ts_start);
+
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
+
+    return true;
+  }
+
+  public Long getGroupID(HBQMetaOperation operation) throws IOException {
+    Increment increment = new Increment(operation.getGroupIdRow());
+    increment.addColumn(HBQConstants.HBQ_FAMILY, HBQConstants.GROUP_ID_GEN, 1L);
+    Result result = increment(increment, null, true);
+    return Bytes.toLong(result.raw()[0].getValue());
+  }
+
+  public HBQQueueMeta getQueueMeta(HBQMetaOperation operation,
+      HBQQueueMeta queueMeta) throws IOException {
+    startRegionOperation();
+    try {
+      if (queueMeta == null) {
+        // Start of queue meta operation, grab global state from headers
+        byte [] metaRow = operation.getMetaRow();
+        checkRow(metaRow, "queueMeta");
+
+        // Read entry id
+        byte [] entryIdBytes = queueGet_noLock(metaRow,
+            HBQConstants.GLOBAL_ENTRYID_COUNTER, DIRTY_READ);
+        if (entryIdBytes == null || entryIdBytes.length == 0) {
+          throw new IOException("Queue not initialized / does not exist");
+        }
+        long entryId = Bytes.toLong(entryIdBytes);
+
+        // Read shard meta
+        ShardMeta shardMeta = ShardMeta.fromBytes(queueGet_noLock(
+            metaRow, HBQConstants.GLOBAL_SHARD_META, DIRTY_READ));
+
+        // Read group states
+        byte [] groupStateColumnPrefix = HBQConstants.GLOBAL_GROUP_PREFIX;
+        Map<byte[],byte[]> groupMap =
+            queueGet_columnPrefix_noLock(metaRow, groupStateColumnPrefix,
+                DIRTY_READ);
+        Map<Long,GroupState> groupStates = new HashMap<Long,GroupState>();
+        for (Map.Entry<byte[],byte[]> groupEntry : groupMap.entrySet()) {
+          byte [] column = groupEntry.getKey();
+          byte [] value = groupEntry.getValue();
+          long groupid = Bytes.toLong(column, groupStateColumnPrefix.length);
+          GroupState groupState = GroupState.fromBytes(value);
+          groupStates.put(groupid, groupState);
+        }
+
+        // Construct initial in-progress queue meta
+        queueMeta = new HBQQueueMeta(entryId, shardMeta, groupStates);
+        queueMeta.updateInProgress(operation,
+            new HBQEntryPointer(1L, 1L));
+      }
+
+      // Either continuing existing operation or just finished global header
+      QueueStats stats = queueMeta.getQueueStats();
+      HBQEntryPointer entryPointer = queueMeta.getPointer();
+      Set<Long> groupIds = queueMeta.getGroups().keySet();
+
+      // Loop until meta operation is complete or we break to jump regions
+      long previousShardId = -1L;
+      while (queueMeta.isInProgress() && entryPointer != null) {
+
+        // Check shard row is still on this region
+
+        byte [] shardRow = operation.getDataRow(
+            Bytes.toBytes(entryPointer.getShardId()));
+        if (previousShardId != entryPointer.getShardId()) {
+          // Moved to a new shard
+
+          // Check if shard is in the same region we are currently in
+          if (!rowIsInRange(this.regionInfo, shardRow)) {
+            // Row of the current shard is not on this region, need to return
+            // back to region server to continue queue meta scan
+            queueMeta.updateInProgress(entryPointer);
+            return queueMeta;
+          } else {
+            previousShardId = entryPointer.getShardId();
+          }
+        }
+        checkRow(shardRow, "queueMeta");
+
+        // Shard row is on this region, process data and group meta
+
+        byte [] entryMetaColumn = dataMetaColumn(entryPointer.getEntryId());
+        byte [] entryMetaBytes = queueGet_noLock(shardRow, entryMetaColumn,
+            DIRTY_READ);
+        if (entryMetaBytes == null) {
+          // Entry does not exist, have reached the end of this queue
+          queueMeta.setAsCompleted();
+          return queueMeta;
+        }
+        // Increment global entry metadata counters
+        EntryMeta entryMeta = EntryMeta.fromBytes(entryMetaBytes);
+        switch (entryMeta.getState()) {
+          case VALID: stats.validEntries++; break;
+          case INVALID: stats.invalidEntries++; break;
+          case SHARD_END: stats.shardEndEntries++; break;
+          case EVICTED: stats.evictedEntries++; break;
+        }
+
+        // If entry is not valid, move to next entry, group metadata is invalid
+        if (!entryMeta.isValid()) {
+          HBQEntryPointer nextPointer = null;
+          if (entryMeta.isEndOfShard()) {
+            // If entry is an end-of-shard marker, move to next shard
+            nextPointer = nextShard(entryPointer);
+          } else {
+            // Otherwise for invalidated/evicted, just move to next entry
+            nextPointer = nextEntry(entryPointer);
+          }
+          entryPointer = nextPointer;
+          continue;
+        }
+
+        // Build array of columns to read group meta entries (if any)
+        if (!groupIds.isEmpty()) {
+          byte [][] columns = new byte[groupIds.size()][];
+          int idx = 0;
+          for (Long groupId : groupIds) {
+            columns[idx++] = dataGroupMetaColumn(entryPointer.getEntryId(),
+                groupId);
+          }
+          // Do a dirty read of group meta data entries
+          Map<byte[],byte[]> readValues = queueGet_noLock(shardRow, columns,
+              DIRTY_READ);
+          for (Map.Entry<byte[],byte[]> entry : readValues.entrySet()) {
+            Long groupid = getGroupFromDataGroupMetaColumn(entry.getKey());
+            EntryGroupMeta groupMeta = EntryGroupMeta.fromBytes(entry.getValue());
+            QueueGroupStats groupStats =
+                queueMeta.getQueueStats().getOrCreateGroup(groupid);
+            // Process according to the current group state
+            // Increment counter according to state
+            switch (groupMeta.getState()) {
+              case AVAILABLE: groupStats.availableEntries++; break;
+              case SEMI_ACKED: groupStats.semiAckedEntries++; break;
+              case ACKED: groupStats.ackedEntries++; break;
+              case DEQUEUED: groupStats.dequeuedEntries++; break;
+            }
+            // If state is not acked, attempt to mark as first non-acked
+            if (!groupMeta.isAcked() && groupStats.firstNonAckedEntry == null) {
+              groupStats.firstNonAckedEntry = entryPointer;
+            } else if (groupStats.firstNonAckedEntry != null) {
+              groupStats.entriesAfterNonAcked++;
+            }
+          }
+        }
+
+        // Finished stats for this entry, move to next and repeat
+        entryPointer = nextEntry(entryPointer);
+      }
+
+      return queueMeta;
+    } finally {
+      closeRegionOperation();
+    }
+  }
+
+  // Simple private helper methods
+
+  private long now() {
+    return EnvironmentEdgeManager.currentTimeMillis();
+  }
+
+  private HBQEntryPointer nextEntry(HBQEntryPointer entryPointer) {
+    return new HBQEntryPointer(entryPointer.getEntryId() + 1,
+        entryPointer.getShardId());
+  }
+
+  private HBQEntryPointer nextShard(HBQEntryPointer entryPointer) {
+    return new HBQEntryPointer(entryPointer.getEntryId(),
+        entryPointer.getShardId() + 1);
+  }
+
+  // Row and column makers
+
+  private byte[] dataDataColumn(long entryId) {
+    return Bytes.add(Bytes.toBytes(entryId), HBQConstants.ENTRY_DATA);
+  }
+
+  private byte[] dataMetaColumn(long entryId) {
+    return Bytes.add(Bytes.toBytes(entryId), HBQConstants.ENTRY_META);
+  }
+
+  private byte[] dataGroupMetaColumn(long entryId, long groupId) {
+    return Bytes.add(Bytes.toBytes(entryId), HBQConstants.ENTRY_GROUP_META,
+        Bytes.toBytes(groupId));
+  }
+
+  private long getGroupFromDataGroupMetaColumn(byte [] dataGroupMetaColumn) {
+    return Bytes.toLong(dataGroupMetaColumn, 9);
+  }
+
+  // Core database operation private helper methods
+
+  /**
+   * Writes the specified value to the specified row and column and with the
+   * specified write version.  Utilizes an existing row lock, wal edits, and
+   * size tracker.
+   *
+   * The actual writing to the HLog and syncing before returning are the
+   * responsibility of the caller, this method only adds KVs to the walEdits
+   * list if enabled writeToWAL is enabled.
+   *
+   * @param rowLock
+   * @param row
+   * @param column
+   * @param value
+   * @param writeVersion
+   * @param writeToWAL
+   * @param walEdits
+   * @param size
+   */
+  private void queuePut_existingLock(Integer rowLock, byte [] row,
+      byte [] column, byte [] value, long writeVersion,
+      boolean writeToWAL, WALEdit walEdits, AtomicLong size) {
+    queuePut_existingLock(rowLock, row,
+        new byte [][]  { column }, new byte [][] { value }, writeVersion,
+        writeToWAL, walEdits, size);
+  }
+
+  /**
+   * Writes the specified values to the specified row and columns and with the
+   * specified write version.  Utilizes an existing row lock, wal edits, and
+   * size tracker.
+   *
+   * The actual writing to the HLog and syncing before returning are the
+   * responsibility of the caller, this method only adds KVs to the walEdits
+   * list if enabled writeToWAL is enabled.
+   *
+   * @param rowLock
+   * @param row
+   * @param columns
+   * @param values
+   * @param writeVersion
+   * @param writeToWAL
+   * @param walEdits
+   * @param size
+   */
+  private void queuePut_existingLock(Integer rowLock, byte[] row,
+      byte[][] columns, byte[][] values, long writeVersion, boolean writeToWAL,
+      WALEdit walEdits, AtomicLong size) {
+    assert(columns.length == values.length);
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    Store store = this.stores.get(family);
+    // Create new KeyValues to store
+    List<KeyValue> kvs = new ArrayList<KeyValue>(columns.length);
+    for (int i=0; i<columns.length; i++) {
+      KeyValue kv = new KeyValue(row, family, columns[i], writeVersion,
+          values[i]);
+      kvs.add(kv);
+      // Write to WALEdits if enabled, actual writing and syncing done by caller
+      if (writeToWAL) {
+        walEdits.add(kv);
+      }
+    }
+    // Add new kv
+    long sizeDelta = 0;
+    for (KeyValue kv : kvs) {
+      sizeDelta += store.add(kv);
+    }
+    size.addAndGet(sizeDelta);
+  }
+
+  /**
+   * Deletes any values at the specified row and columns and with a write
+   * version less than or equal to the specified write version.  Utilizes an
+   * existing row lock, wal edits, and size tracker.
+   *
+   * The actual writing to the HLog and syncing before returning are the
+   * responsibility of the caller, this method only adds KVs to the walEdits
+   * list if enabled writeToWAL is enabled.
+   *
+   * @param rowLock
+   * @param row
+   * @param column
+   * @param writeVersion
+   * @param writeToWAL
+   * @param walEdits
+   * @param size
+   */
+  private void queueDelete_existingLock(Integer rowLock, byte[] row,
+      byte[] column, long writeVersion, boolean writeToWAL, WALEdit walEdits,
+      AtomicLong size) {
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    Store store = this.stores.get(family);
+    // Create new KeyValue to store
+    KeyValue kv = new KeyValue(row, family, column, writeVersion,
+        KeyValue.Type.DeleteColumn);
+    // Write to WALEdits if enabled, actual writing and syncing done by caller
+    if (writeToWAL) {
+      walEdits.add(kv);
+    }
+    // Add new kv
+    size.addAndGet(store.add(kv));
+  }
+
+  /**
+   * Gets the latest version of the specified row and column that has a version
+   * less than the specified max version.  Returns null if nothing exists.
+   * @param row
+   * @param column
+   * @param version
+   * @return
+   * @throws IOException
+   */
+  private byte[] queueGet_noLock(byte[] row, byte[] column, long maxVersion)
+      throws IOException {
+    return queueGet_noLock(row, new byte [][] { column }, maxVersion)
+        .get(column);
+  }
+
+  /**
+   * Gets the latest version of the specified row and columns that have a
+   * version less than the specified max version.  Returns an empty map if
+   * no columns exist.
+   * @param row
+   * @param column
+   * @param version
+   * @return
+   * @throws IOException
+   */
+  private Map<byte[],byte[]> queueGet_noLock(byte[] row, byte[][] columns,
+      long maxVersion) throws IOException {
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    Get get = new Get(row);
+    for (byte [] column : columns) get.addColumn(family, column);
+    get.setTimeRange(0, maxVersion);
+    List<KeyValue> results = get(get, false);
+    Map<byte[],byte[]> ret = new TreeMap<byte[],byte[]>(Bytes.BYTES_COMPARATOR);
+    if (results.isEmpty()) return ret;
+    for (KeyValue kv : results) {
+      ret.put(kv.getQualifier(), kv.getValue());
+    }
+    return ret;
+  }
+
+  /**
+   * Gets the latest version of the specified row and all columns that have the
+   * specified prefix and a version less than the specified max version.
+   * Returns an empty map if no columns exist.
+   * @param row
+   * @param columnPrefix
+   * @param maxVersion
+   * @return
+   * @throws IOException
+   */
+  private Map<byte[], byte[]> queueGet_columnPrefix_noLock(byte[] row,
+      byte[] columnPrefix, long maxVersion) throws IOException {
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    Get get = new Get(row);
+    get.addFamily(family);
+    get.setFilter(new ColumnPrefixFilter(columnPrefix));
+    get.setTimeRange(0, maxVersion);
+    List<KeyValue> results = get(get, false);
+    Map<byte[],byte[]> ret = new TreeMap<byte[],byte[]>(Bytes.BYTES_COMPARATOR);
+    if (results.isEmpty()) return ret;
+    for (KeyValue kv : results) {
+      ret.put(kv.getQualifier(), kv.getValue());
+    }
+    return ret;
+  }
+
+  /**
+   * Gets the latest version of the specified row and column that is visible
+   * according to the specified read pointer.  Returns null if nothing exists.
+   * @param row
+   * @param column
+   * @param pointer
+   * @return
+   * @throws IOException
+   */
+  private byte[] queueGet_noLock(byte[] row, byte[] column,
+      HBReadPointer pointer) throws IOException {
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    // Get previous value
+    Get get = new Get(row);
+    get.addColumn(family, column);
+    get.setTimeRange(0, pointer.getReadPointer());
+    get.setMaxVersions();
+    List<KeyValue> results = get(get, false);
+    if (results.isEmpty()) return null;
+    for (KeyValue kv : results) {
+      if (pointer.isVisible(kv.getTimestamp())) {
+        return kv.getValue();
+      }
+    }
+    return null;
+  }
+
+  /**
+   * Increments the specified row and column by 1, returning the
+   * post-incremented value.  Utilizes an existing row lock, wal edits, and
+   * size tracker.
+   *
+   * The actual writing to the HLog and syncing before returning are the
+   * responsibility of the caller, this method only adds KVs to the walEdits
+   * list if enabled writeToWAL is enabled.
+   *
+   * @param rowLock
+   * @param row
+   * @param column
+   * @param writeVersion
+   * @param writeToWAL
+   * @param walEdits
+   * @param size
+   * @return
+   * @throws IOException
+   */
+  private long queueIncrement_existingLock(Integer rowLock, byte [] row,
+      byte [] column, long writeVersion, boolean writeToWAL, WALEdit walEdits,
+      AtomicLong size) throws IOException {
+    byte [] family = HBQConstants.HBQ_FAMILY;
+    Store store = this.stores.get(family);
+    // Get previous value
+    Get get = new Get(row);
+    get.addColumn(family, column);
+    List<KeyValue> results = get(get, false);
+    // Determine updated amount to store
+    long amount = 1;
+    if (!results.isEmpty()) {
+      KeyValue kv = results.get(0);
+      amount += Bytes.toLong(kv.getBuffer(), kv.getValueOffset(),
+          kv.getValueLength());
+    }
+    // Create new KeyValue to store
+    KeyValue newKV = new KeyValue(row, family, column, writeVersion,
+        Bytes.toBytes(amount));
+    // Write to WALEdits if enabled, actual writing and syncing done by caller
+    if (writeToWAL) {
+      walEdits.add(newKV);
+    }
+    List<KeyValue> kvs = Arrays.asList(new KeyValue [] { newKV });
+    // Upsert new kv
+    long sizeDelta = store.upsert(kvs);
+    size.addAndGet(sizeDelta);
+    return amount;
+  }
+
+  private interface QueueCASValueComparer {
+    boolean checkExistingValueOkay(byte [] existingValue);
+
+    boolean shouldOverwriteValue(byte[] existingValue);
+  }
+
+  /**
+   * Perform a dirty, atomic compare-and-swap operation.
+   * <p>
+   * If write-to-wal is specified, performs append-no-sync for any edits and
+   * returns the txid and size of edits.
+   * <p>
+   * Contains an optional existing value comparer that allows for cases that an
+   * existing value is found that does not match the expected value, so no
+   * update to the new value should be performed, but if the existing value
+   * passes the specified comparer, the operation will be treated as successful.
+   * <p>
+   * An expected value of null means that this operation expects there to be
+   * no existing value.  A new value of null is not supported.
+   * <p>
+   * If operation fails, returns null.
+   * <p>
+   * Should be called with a region operation lock but no row or update locks.
+   *
+   * @param row
+   * @param column
+   * @param expectedValue value to verify currently exists,
+   *                      or null for does not exist
+   * @param newValue new value to replace if expected is found
+   * @param existingValueComparer optional existing value comparer to use if
+   *                              expected value not found
+   * @return txid and size if writes occurred and wal is on, zero valued txid
+   *         and size if writes occurred and wal is off OR existing value
+   *         comparer passed, or returns null if operation failed completely
+   * @throws IOException
+   */
+  private TxidAndSize queueCAS(byte [] row, byte [] column,
+      byte [] expectedValue, byte [] newValue, boolean writeToWAL,
+      QueueCASValueComparer existingValueComparer)
+          throws IOException {
+    WALEdit walEdits = writeToWAL ? new WALEdit() : null;
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+
+    // Lock row
+    Integer rowLock = getLock(null, row, true);
+    this.updatesLock.readLock().lock();
+    try {
+
+      long ts_now = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "queue_cas_lock", ts_now - ts_start);
+
+      // Read existing value using specified read pointer
+      byte [] existingValue = queueGet_noLock(row, column, DIRTY_READ);
+
+      long ts_dataRead = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "queue_cas_get",
+          ts_dataRead - ts_now);
+
+      // Check if it is what is expected
+      if ((expectedValue == null && existingValue == null) ||
+          (expectedValue != null && existingValue != null &&
+          Bytes.equals(expectedValue, existingValue)) ||
+          (existingValueComparer != null &&
+           existingValueComparer.shouldOverwriteValue(existingValue))) {
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "queue_cas_expected_match", 1L);
+
+        // Existing and expected match, perform update to new value
+        queuePut_existingLock(rowLock, row, column, newValue, ts_now,
+            writeToWAL, walEdits, size);
+
+        long ts_dataWrite = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "queue_cas_put",
+            ts_dataWrite - ts_dataRead);
+
+        // Write out to WAL if enabled
+        long ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+        if (writeToWAL) {
+          txid = this.log.appendNoSync(this.regionInfo,
+              this.htableDescriptor.getName(), walEdits,
+              HConstants.DEFAULT_CLUSTER_ID,
+              EnvironmentEdgeManager.currentTimeMillis(),
+              this.htableDescriptor);
+          ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+          RegionMetricsStorage.incrTimeVaryingMetric(
+              HBQ_METRIC_PREFIX + "queue_cas_wal_append",
+              ts_walAppend - ts_dataWrite);
+        }
+        return new TxidAndSize(txid, size.get());
+      }
+
+      // Existing is not what was expected, check comparer
+      if (existingValueComparer != null &&
+          existingValueComparer.checkExistingValueOkay(existingValue)) {
+        RegionMetricsStorage.incrNumericMetric(
+            HBQ_METRIC_PREFIX + "queue_cas_comparer_match", 1L);
+        // Not a straight match but comparer passes, treat as success but
+        // no writes so no size change
+        return new TxidAndSize();
+      }
+
+      // Compare and swap operation failed, return null
+      RegionMetricsStorage.incrNumericMetric(
+          HBQ_METRIC_PREFIX + "queue_cas_fail", 1L);
+      return null;
+
+    } finally {
+      // Release meta row lock
+      this.updatesLock.readLock().unlock();
+      releaseRowLock(rowLock);
+    }
+  }
+
+  // Complex operation private helper methods
+
+  /**
+   * Attempts to perform a group reconfiguration, and if successful, continues
+   * the execution of the specified dequeue operation.  If not successful,
+   * returns a failed dequeue result.
+   *
+   * Called from dequeue() as well as dequeue_continue() while the region
+   * operation lock is held.
+   *
+   * @param dequeue
+   * @param groupState
+   * @param writeToWAL
+   * @return
+   * @throws IOException
+   */
+  private Pair<HBQDequeueResult, TxidAndSize> attemptGroupReconfigure(
+      HBQDequeue dequeue, GroupState groupState, HBQEntryPointer currentEntry,
+      boolean writeToWAL) throws IOException {
+    HBQConsumer consumer = dequeue.getConsumer();
+    HBQConfig config = dequeue.getConfig();
+
+    // Keep looping until we find a pending entry, hit the end, or need to move
+    while (true) {
+
+      // Checks are done one shard at a time.  Get the current shard.
+      byte [] shardRow = dequeue.getDataRow(
+          Bytes.toBytes(currentEntry.getShardId()));
+
+      // Verify this row is on this region
+      if (!rowIsInRange(getRegionInfo(), shardRow)) {
+        // Need to continue search on another region, return back to HRS
+        return new Pair<HBQDequeueResult, TxidAndSize>(
+            new HBQDequeueResult(dequeue, currentEntry, groupState, true),
+            new TxidAndSize());
+      }
+
+      // Check this shard for pending entries
+      PendingEntryCheckResult pendingCheckResult =
+          checkShardForPendingEntries(shardRow, consumer.getGroupId());
+
+      // If a pending entry is found, the dequeue fails
+      if (pendingCheckResult.foundPendingEntries()) {
+        String msg = "Attempted to change queue group configuration but " +
+            "group has pending entries not acked";
+        LOG.warn(msg);
+        return new Pair<HBQDequeueResult, TxidAndSize>(
+            new HBQDequeueResult(msg), new TxidAndSize());
+      }
+
+      // If we reached the end of the shard, we have to continue to the next
+      if (pendingCheckResult.reachedEndOfShard()) {
+        currentEntry = nextShard(currentEntry);
+        continue;
+      }
+
+      // No pending entries found for this group, can reconfigure
+      break;
+    }
+
+    // No pending entries, attempt to update group state
+    GroupState oldGroupState = groupState;
+    groupState = new GroupState(consumer.getGroupSize(),
+        groupState.getHead(), ExecutionMode.fromQueueConfig(config));
+    // TODO: This will also throw an exception if there are multiple
+    //       reconfigurations concurrently that conflict with each other
+    TxidAndSize txidAndSize =
+        updateGroupState(dequeue, oldGroupState, groupState, writeToWAL);
+
+    // Group state updated, now continue search for data
+    Pair<HBQDequeueResult, TxidAndSize> result =
+        dequeue_data_internal(dequeue, groupState, groupState.getHead(),
+            writeToWAL);
+
+    // Combine txid-and-size result
+    txidAndSize.size += result.getSecond().size;
+    txidAndSize.txid = Math.max(txidAndSize.txid, result.getSecond().txid);
+
+    // Return result from the data dequeue
+    return new Pair<HBQDequeueResult, TxidAndSize>(
+        result.getFirst(), txidAndSize);
+  }
+
+  private class PendingEntryCheckResult {
+    boolean foundPendingEntries;
+    boolean reachedEndOfShard;
+    PendingEntryCheckResult(final boolean foundPendingEntries,
+        final boolean reachedEndOfShard) {
+      this.foundPendingEntries = foundPendingEntries;
+      this.reachedEndOfShard = reachedEndOfShard;
+    }
+    boolean foundPendingEntries() {
+      return this.foundPendingEntries;
+    }
+    boolean reachedEndOfShard() {
+      return this.reachedEndOfShard;
+    }
+  }
+
+//  private static final int PENDING_CHECK__NUM_ENTRIES_PER_LOOP = 10;
+
+  /**
+   * Checks all entries in the specified shard row for pending entries (dequeued
+   * but not acked) for the specified consumer group.
+   *
+   * Caller must already hold region operation lock.  This is a read-only
+   * operations so no row locks or writing of any kind is done.
+   *
+   * @param shardRow
+   * @param groupId
+   * @return
+   * @throws IOException
+   */
+  private PendingEntryCheckResult checkShardForPendingEntries(byte [] shardRow,
+      long groupId) throws IOException {
+    boolean foundPendingEntry = false;
+    boolean reachedEndOfShard = false;
+    long currentEntryId = 1L;
+
+    // Loop until we find a pending entry or we reach the end of the shard
+    while (!foundPendingEntry && !reachedEndOfShard) {
+
+      // TODO: Optimization: We should grab a set of entries on each get
+      //       instead of one at a time
+
+      byte [] entryMetaColumn = dataMetaColumn(currentEntryId);
+      byte [] entryGroupMetaColumn = dataGroupMetaColumn(currentEntryId,
+          groupId);
+
+      Map<byte[],byte[]> result = queueGet_noLock(shardRow,
+          new byte [][] { entryMetaColumn, entryGroupMetaColumn }, DIRTY_READ);
+
+      // If there is no entry meta, the search on this shard is over
+      if (!result.containsKey(entryMetaColumn)) break;
+
+      EntryMeta entryMeta = EntryMeta.fromBytes(result.get(entryMetaColumn));
+
+      // If we have hit the end of shard, set the flag and break
+      if (entryMeta.isEndOfShard()) {
+        reachedEndOfShard = true;
+        break;
+      }
+
+      // If entry is invalid in some way, move to next entry
+      if (!entryMeta.isValid()) {
+        currentEntryId++;
+        continue;
+      }
+
+      // If there is no entry group meta, group has not seen entry.
+      // Since there can be gaps in processed entries (partitioners) we consider
+      // this as available (eventually we may want to optimize this case)
+      if (!result.containsKey(entryGroupMetaColumn)) {
+        currentEntryId++;
+        continue;
+      }
+
+      EntryGroupMeta entryGroupMeta =
+          EntryGroupMeta.fromBytes(result.get(entryGroupMetaColumn));
+
+      // If the entry is dequeued, there is a pending entry
+      if (entryGroupMeta.isDequeued()) {
+        foundPendingEntry = true;
+        break;
+      }
+
+      // The entry is either available or acked, both are not pending, so we
+      // can safely move to the next entry
+      currentEntryId++;
+    }
+
+    return new PendingEntryCheckResult(foundPendingEntry, reachedEndOfShard);
+  }
+
+  /**
+   * Reads all of the group meta entries for the specified entry and checks if
+   * they are all in a finalized state.
+   *
+   * If the specified number of groups is found and they are all in a finalized
+   * state, the method returns true.  Otherwise, if any are not finalized or if
+   * fewer than the specified number of groups is found at all, method returns
+   * false.
+   *
+   * @param entryPointer
+   * @param numGroups
+   * @param groupId
+   * @return true if all group meta entries are in a finalized state, false if
+   *         at least one is not or if less than the specified number of groups
+   *         was found
+   * @throws IOException
+   */
+  private boolean allOtherGroupsFinalized(byte [] shardRow,
+      HBQEntryPointer entryPointer, int numGroups, long groupId)
+      throws IOException {
+
+    // Read all of the group meta columns
+    byte [] prefix = Bytes.add(Bytes.toBytes(entryPointer.getEntryId()),
+        HBQConstants.ENTRY_GROUP_META);
+    Map<byte[],byte[]> groupMetaMap = queueGet_columnPrefix_noLock(shardRow,
+        prefix, DIRTY_READ);
+
+    // There should be exactly numGroups
+    if (groupMetaMap.size() != numGroups) return false;
+
+    // Iterate through all entries
+    for (Map.Entry<byte[],byte[]> entry : groupMetaMap.entrySet()) {
+
+      // If the group is the calling group, it's good
+      long currentGroupId = Bytes.toLong(entry.getKey(), prefix.length);
+      if (currentGroupId == groupId) continue;
+
+      // Check if this group is finalized
+      EntryGroupMeta entryGroupMeta =
+          EntryGroupMeta.fromBytes(entry.getValue());
+      if (!entryGroupMeta.isAcked()) return false;
+    }
+
+    // If we break from the loop, all groups are acked
+    return true;
+  }
+
+  /**
+   * Evicts the data block at the specified entry pointer.
+   *
+   * This marks the entry meta state as EVICTED and inserts a delete marker
+   * for the entry data.
+   *
+   * @param shardRow
+   * @param entryPointer
+   * @return
+   * @throws IOException
+   */
+  private TxidAndSize evictDataBlock(byte [] shardRow,
+      HBQEntryPointer entryPointer, boolean writeToWAL) throws IOException {
+
+    WALEdit walEdits = writeToWAL ? new WALEdit() : null;
+    long ts_start = EnvironmentEdgeManager.currentTimeMillis();
+    AtomicLong size = new AtomicLong(0);
+    long txid = 0;
+
+    // Lock row
+    Integer rowLock = getLock(null, shardRow, true);
+    this.updatesLock.readLock().lock();
+    try {
+
+      long ts_now = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "evict_data_lock", ts_now - ts_start);
+
+      // Write evicted state
+      queuePut_existingLock(rowLock, shardRow,
+          dataMetaColumn(entryPointer.getEntryId()),
+          new EntryMeta(EntryState.EVICTED).getBytes(),
+          ts_now, writeToWAL, walEdits, size);
+
+      // Delete data block
+      queueDelete_existingLock(rowLock, shardRow,
+          dataDataColumn(entryPointer.getEntryId()),
+          ts_now, writeToWAL, walEdits, size);
+
+      long ts_dataWrite = EnvironmentEdgeManager.currentTimeMillis();
+      RegionMetricsStorage.incrTimeVaryingMetric(
+          HBQ_METRIC_PREFIX + "evict_data_write",
+          ts_dataWrite - ts_now);
+
+      // Write out to WAL if enabled
+      long ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+      if (writeToWAL) {
+        txid = this.log.appendNoSync(this.regionInfo,
+            this.htableDescriptor.getName(), walEdits,
+            HConstants.DEFAULT_CLUSTER_ID,
+            EnvironmentEdgeManager.currentTimeMillis(),
+            this.htableDescriptor);
+        ts_walAppend = EnvironmentEdgeManager.currentTimeMillis();
+        RegionMetricsStorage.incrTimeVaryingMetric(
+            HBQ_METRIC_PREFIX + "evict_data_wal_append",
+            ts_walAppend - ts_dataWrite);
+      }
+      return new TxidAndSize(txid, size.get());
+
+    } finally {
+      // Release meta row lock
+      this.updatesLock.readLock().unlock();
+      releaseRowLock(rowLock);
+    }
+  }
+
+  /**
+   * Attempts to perform an atomic update of the entry group meta.
+   * <p>
+   * For behavior of operation, see
+   * {@link #queueCAS(byte[], byte[], byte[], byte[], boolean, QueueCASValueComparer)}
+   * (this operation does not use a value comparer).
+   *
+   * @param dequeue
+   * @param existingEntryGroupMeta
+   * @param updatedEntryGroupMeta
+   * @return
+   */
+  private TxidAndSize attemptToClaimEntry(HBQDequeue dequeue,
+      HBQEntryPointer entryPointer, EntryGroupMeta existingEntryGroupMeta,
+      EntryGroupMeta newEntryGroupMeta)
+          throws IOException {
+    return queueCAS(
+        dequeue.getDataRow(Bytes.toBytes(entryPointer.getShardId())),
+        dataGroupMetaColumn(entryPointer.getEntryId(),
+            dequeue.getConsumer().getGroupId()),
+            existingEntryGroupMeta == null ?
+                null : existingEntryGroupMeta.getBytes(),
+                newEntryGroupMeta.getBytes(), dequeue.getWriteToWAL(), null);
+  }
+
+  /**
+   * Updates the group state from the specified existing state to the
+   * new state, for the queue and consumer group specified in the dequeue
+   * operation.
+   *
+   * This method is similar to an atomic compare-and-swap in that it
+   * expects to see the existing value and wants to update it to the new
+   * value, however, it is different in that it will also accept the stored
+   * group state to already be the new group state.
+   *
+   * If the current group state is neither the expected existing or new state,
+   * an exception is thrown.
+   *
+   * This method should be called while holding the overall region operation
+   * lock, but no other locks (row lock, updates lock).  A row lock will be
+   * acquired within this method on this group's meta row.
+   *
+   * If the write-to-wal flag is true, any writes performed will be written out
+   * to the wal under the row lock with an append-no-sync and the txid will
+   * be returned (txid is 0 if no writes were performed or the wal is disabled).
+   *
+   * @param dequeue
+   * @param groupStateExisting
+   * @param groupStateNew
+   * @param writeToWAL
+   * @return
+   * @throws IOException the current state is neither the expected existing
+   *                     or new state
+   */
+  private TxidAndSize updateGroupState(final HBQDequeue dequeue,
+      final GroupState groupStateExisting, final GroupState groupStateNew,
+      final boolean writeToWAL)
+          throws IOException {
+    // Also accept the existing value having same pointer configs different
+    final QueueCASValueComparer comparer = new QueueCASValueComparer() {
+      @Override
+      public boolean checkExistingValueOkay(byte[] existingValue) {
+        GroupState existing = GroupState.fromBytes(existingValue);
+        return existing.nonPointerConfigEquals(groupStateNew);
+      }
+      @Override
+      public boolean shouldOverwriteValue(byte[] existingValue) {
+        return false;
+      }
+    };
+    TxidAndSize txidAndSize = queueCAS(dequeue.getGroupMetaRow(),
+        dequeue.getGroupMetaColumn(),
+        groupStateExisting == null ? null : groupStateExisting.getBytes(),
+            groupStateNew.getBytes(), dequeue.getWriteToWAL(), comparer);
+    if (txidAndSize == null) {
+      throw new IOException("Group state update failed, concurrent mods");
+    }
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "group_state_updated", 1);
+    return txidAndSize;
+  }
+
+  /**
+   * Updates the group state to the specified new state, for the queue and
+   * consumer group specified in the dequeue operation.
+   *
+   * This method attempts to make the value of specified group state the
+   * specified current group state by reading the existing value, comparing it,
+   * and determining if it is valid to perform an update, already updated in a
+   * compatible way, or incompatible.  Throws an exception if incompatible as
+   * it should only happen when consumers are not behaving as expected.
+   *
+   * This method should be called while holding the overall region operation
+   * lock, but no other locks (row lock, updates lock).  A row lock will be
+   * acquired within this method on this group's meta row.
+   *
+   * If the write-to-wal flag is true, any writes performed will be written out
+   * to the wal under the row lock with an append-no-sync and the txid will
+   * be returned (txid is 0 if no writes were performed or the wal is disabled).
+   *
+   * @param dequeue
+   * @param newGroupState
+   * @param writeToWAL
+   * @return
+   * @throws IOException the current state is neither the expected existing
+   *                     or new state
+   */
+  private TxidAndSize updateGroupState(HBQDequeue dequeue,
+      final GroupState newGroupState, boolean writeToWAL)
+          throws IOException {
+    // Also accept the existing value having same non-pointer configs as new
+    // and entry pointer greater or equal to specified
+    final QueueCASValueComparer comparer = new QueueCASValueComparer() {
+      @Override
+      public boolean checkExistingValueOkay(byte[] existingValue) {
+        GroupState existing = GroupState.fromBytes(existingValue);
+        if (!existing.nonPointerConfigEquals(newGroupState)) return false;
+        if (!existing.isGreaterThanOrEqual(newGroupState)) return false;
+        return true;
+      }
+      @Override
+      public boolean shouldOverwriteValue(byte[] existingValue) {
+        GroupState existing = GroupState.fromBytes(existingValue);
+        if (!existing.nonPointerConfigEquals(newGroupState)) return false;
+        if (!existing.isGreaterThanOrEqual(newGroupState)) return true;
+        return false;
+      }
+    };
+    TxidAndSize txidAndSize = queueCAS(dequeue.getGroupMetaRow(),
+        dequeue.getGroupMetaColumn(), newGroupState.getBytes(),
+        newGroupState.getBytes(), dequeue.getWriteToWAL(), comparer);
+    if (txidAndSize == null) {
+      throw new IOException("Group state update failed, concurrent mods");
+    }
+    RegionMetricsStorage.incrNumericMetric(
+        HBQ_METRIC_PREFIX + "group_state_updated", 1);
+    return txidAndSize;
+  }
+
+  private boolean ackedOrTimedOutSemiAcked(EntryGroupMeta entryGroupMeta,
+      long maxAgeBeforeSemiAckedToAcked) {
+    return entryGroupMeta.isAcked() ||
+        (entryGroupMeta.isSemiAcked() &&
+            entryGroupMeta.getTimestamp() + maxAgeBeforeSemiAckedToAcked <=
+            now());
+  }
+
+  /**
+   * Attempts to update the group pointer by checking whether the entry pointer
+   * is one entry away from the current group state.  If the specified group
+   * state is null, returns null.
+   * @param groupState
+   * @param entryPointer
+   * @return
+   */
+  private GroupState attemptGroupPointerUpdate(boolean skippedEntry,
+      GroupState groupState, HBQEntryPointer entryPointer) {
+    if (groupState == null) return null;
+    if (skippedEntry) return groupState;
+    if ((entryPointer.getShardId() == groupState.getHead().getShardId() &&
+          entryPointer.getEntryId() == groupState.getHead().getEntryId() + 1)
+        ||
+        (entryPointer.getEntryId() == groupState.getHead().getEntryId() &&
+          entryPointer.getShardId() == groupState.getHead().getShardId() + 1)
+       ) {
+      return new GroupState(groupState, entryPointer);
+    }
+    return groupState;
+  }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index 8f9cf0c..53b0355 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -19,85 +19,22 @@
  */
 package org.apache.hadoop.hbase.regionserver;
 
-import java.io.IOException;
-import java.io.StringWriter;
-import java.lang.Thread.UncaughtExceptionHandler;
-import java.lang.annotation.Retention;
-import java.lang.annotation.RetentionPolicy;
-import java.lang.management.ManagementFactory;
-import java.lang.management.MemoryUsage;
-import java.lang.reflect.Constructor;
-import java.lang.reflect.Method;
-import java.net.BindException;
-import java.net.InetSocketAddress;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Random;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.TreeMap;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentSkipListMap;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
-import javax.management.ObjectName;
-
+import com.continuuity.hbase.ttqueue.*;
+import com.google.common.base.Function;
+import com.google.common.collect.Lists;
 import org.apache.commons.lang.mutable.MutableDouble;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.Chore;
-import org.apache.hadoop.hbase.ClockOutOfSyncException;
-import org.apache.hadoop.hbase.DoNotRetryIOException;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.*;
 import org.apache.hadoop.hbase.HConstants.OperationStatusCode;
-import org.apache.hadoop.hbase.HDFSBlocksDistribution;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HServerInfo;
-import org.apache.hadoop.hbase.HServerLoad;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.MasterAddressTracker;
-import org.apache.hadoop.hbase.NotServingRegionException;
-import org.apache.hadoop.hbase.RemoteExceptionHandler;
-import org.apache.hadoop.hbase.ServerName;
-import org.apache.hadoop.hbase.Stoppable;
-import org.apache.hadoop.hbase.TableDescriptors;
-import org.apache.hadoop.hbase.UnknownRowLockException;
-import org.apache.hadoop.hbase.UnknownScannerException;
-import org.apache.hadoop.hbase.YouAreDeadException;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
 import org.apache.hadoop.hbase.catalog.MetaReader;
 import org.apache.hadoop.hbase.catalog.RootLocationEditor;
-import org.apache.hadoop.hbase.client.Action;
-import org.apache.hadoop.hbase.client.Append;
-import org.apache.hadoop.hbase.client.Delete;
-import org.apache.hadoop.hbase.client.Get;
-import org.apache.hadoop.hbase.client.HConnectionManager;
-import org.apache.hadoop.hbase.client.Increment;
-import org.apache.hadoop.hbase.client.MultiAction;
-import org.apache.hadoop.hbase.client.MultiResponse;
-import org.apache.hadoop.hbase.client.Mutation;
-import org.apache.hadoop.hbase.client.Put;
-import org.apache.hadoop.hbase.client.Result;
-import org.apache.hadoop.hbase.client.Row;
-import org.apache.hadoop.hbase.client.RowMutations;
-import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.client.*;
 import org.apache.hadoop.hbase.client.coprocessor.Exec;
 import org.apache.hadoop.hbase.client.coprocessor.ExecResult;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;
@@ -139,22 +76,8 @@ import org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.regionserver.wal.WALActionsListener;
 import org.apache.hadoop.hbase.security.User;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.CompressionTest;
-import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
-import org.apache.hadoop.hbase.util.FSTableDescriptors;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.apache.hadoop.hbase.util.InfoServer;
-import org.apache.hadoop.hbase.util.Pair;
-import org.apache.hadoop.hbase.util.Sleeper;
-import org.apache.hadoop.hbase.util.Strings;
-import org.apache.hadoop.hbase.util.Threads;
-import org.apache.hadoop.hbase.util.VersionInfo;
-import org.apache.hadoop.hbase.zookeeper.ClusterId;
-import org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker;
-import org.apache.hadoop.hbase.zookeeper.ZKUtil;
-import org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker;
-import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.hadoop.hbase.util.*;
+import org.apache.hadoop.hbase.zookeeper.*;
 import org.apache.hadoop.io.MapWritable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.ipc.RemoteException;
@@ -165,8 +88,25 @@ import org.apache.hadoop.util.StringUtils;
 import org.apache.zookeeper.KeeperException;
 import org.codehaus.jackson.map.ObjectMapper;
 
-import com.google.common.base.Function;
-import com.google.common.collect.Lists;
+import javax.management.ObjectName;
+import java.io.IOException;
+import java.io.StringWriter;
+import java.lang.Thread.UncaughtExceptionHandler;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.management.ManagementFactory;
+import java.lang.management.MemoryUsage;
+import java.lang.reflect.Constructor;
+import java.lang.reflect.Method;
+import java.net.BindException;
+import java.net.InetSocketAddress;
+import java.util.*;
+import java.util.Map.Entry;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentSkipListMap;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
 
 /**
  * HRegionServer makes a set of HRegions available to clients. It checks in with
@@ -2070,8 +2010,8 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
 
   private boolean checkAndMutate(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final CompareOp compareOp,
-      final WritableByteArrayComparable comparator, final Writable w,
-      Integer lock) throws IOException {
+      final WritableByteArrayComparable comparator, final long readVersion,
+      final Writable w, Integer lock) throws IOException {
     checkOpen();
     this.requestCount.incrementAndGet();
     HRegion region = getRegion(regionName);
@@ -2080,7 +2020,7 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
         this.cacheFlusher.reclaimMemStoreMemory();
       }
       return region.checkAndMutate(row, family, qualifier, compareOp,
-        comparator, w, lock, true);
+        comparator, readVersion, w, lock, true);
     } catch (Throwable t) {
       throw convertThrowableToIOE(cleanup(t));
     }
@@ -2100,7 +2040,7 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
    */
   public boolean checkAndPut(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final byte[] value,
-      final Put put) throws IOException {
+      final long readVersion, final Put put) throws IOException {
     checkOpen();
     if (regionName == null) {
       throw new IOException("Invalid arguments to checkAndPut "
@@ -2118,7 +2058,7 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
       }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-        CompareOp.EQUAL, comparator, put,
+        CompareOp.EQUAL, comparator, readVersion, put,
       lock);
     if (region.getCoprocessorHost() != null) {
       result = region.getCoprocessorHost().postCheckAndPut(row, family,
@@ -2141,7 +2081,8 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
    */
   public boolean checkAndPut(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final CompareOp compareOp,
-      final WritableByteArrayComparable comparator, final Put put)
+      final WritableByteArrayComparable comparator,
+      final Put put)
        throws IOException {
     checkOpen();
     if (regionName == null) {
@@ -2158,7 +2099,7 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
       }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      compareOp, comparator, put, lock);
+      compareOp, comparator, Long.MAX_VALUE, put, lock);
     if (region.getCoprocessorHost() != null) {
       result = region.getCoprocessorHost().postCheckAndPut(row, family,
         qualifier, compareOp, comparator, put, result);
@@ -2180,7 +2121,7 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
    */
   public boolean checkAndDelete(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final byte[] value,
-      final Delete delete) throws IOException {
+      final long readVersion, final Delete delete) throws IOException {
     checkOpen();
 
     if (regionName == null) {
@@ -2198,7 +2139,7 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
       }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      CompareOp.EQUAL, comparator, delete, lock);
+      CompareOp.EQUAL, comparator, readVersion, delete, lock);
     if (region.getCoprocessorHost() != null) {
       result = region.getCoprocessorHost().postCheckAndDelete(row, family,
         qualifier, CompareOp.EQUAL, comparator, delete, result);
@@ -2306,7 +2247,7 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
      }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      compareOp, comparator, delete, lock);
+      compareOp, comparator, Long.MAX_VALUE, delete, lock);
    if (region.getCoprocessorHost() != null) {
      result = region.getCoprocessorHost().postCheckAndDelete(row, family,
        qualifier, compareOp, comparator, delete, result);
@@ -3757,4 +3698,281 @@ public class HRegionServer implements HRegionInterface, HBaseRPCErrorHandler,
       HRegionInfo info = region.getRegionInfo();
       return CompactionRequest.getCompactionState(info.getRegionId()).name();
   }
+
+  @Override
+  public HBQEnqueueResult enqueue(byte[] regionName, byte[] row,
+      HBQEnqueue enqueue) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      HBQEnqueueResult result = region.enqueue(enqueue);
+      // If there was any kind of failure, return result
+      if (!result.isSuccess()) return result;
+      // If data has been written, return result
+      if (result.getDataWritten()) return result;
+      // Data needs to be written to another region, see if it's on this server
+      byte [] tableName = region.getTableDesc().getName();
+      byte [] shardRow = enqueue.getDataRow(
+          Bytes.toBytes(result.getEntryPointer().getShardId() -
+              (result.getInsertShardEnd() ? 1 : 0)));
+      region = findRegion(tableName, shardRow);
+      // If region wasn't found on this server, return back to client
+      if (region == null) {
+        return result;
+      }
+      // Found on this server, perform it again on new region
+      result = region.enqueue_data(enqueue, result);
+      // If there was any kind of failure, return result
+      if (!result.isSuccess()) return result;
+      // If data has been written, return result
+      if (result.getDataWritten()) return result;
+      // If data has still not been written, must have written shard end
+      if (result.getInsertShardEnd()) {
+        // This case should be invalid, throwing exception
+        throw new IOException("Fell into invalid state during enqueue");
+      }
+      // Actual data needs to be written to another region, check this server
+      shardRow = enqueue.getDataRow(
+          Bytes.toBytes(result.getEntryPointer().getShardId()));
+      region = findRegion(tableName, shardRow);
+      // If region wasn't found on this server, return back to client
+      if (region == null) return result;
+      // Found on this server, perform it again on new region
+      result = region.enqueue_data(enqueue, result);
+      // In all cases, return the result back to client
+      return result;
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public HBQEnqueueResult enqueue_data(byte[] regionName, byte[] row,
+      HBQEnqueue enqueue, HBQEnqueueResult previousResult) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      HBQEnqueueResult result = region.enqueue_data(enqueue, previousResult);
+      // If there was any kind of failure, return result
+      if (!result.isSuccess()) return result;
+      // If data has been written, return result
+      if (result.getDataWritten()) return result;
+      // If data has still not been written, must have written shard end
+      if (result.getInsertShardEnd() || !previousResult.getInsertShardEnd()) {
+        // This case should be invalid, throwing exception
+        throw new IOException("Fell into invalid state during enqueue");
+      }
+      // Actual data needs to be written to another region, check this server
+      byte [] tableName = region.getTableDesc().getName();
+      byte [] shardRow = enqueue.getDataRow(
+          Bytes.toBytes(result.getEntryPointer().getShardId()));
+      region = findRegion(tableName, shardRow);
+      // If region wasn't found on this server, return back to client
+      if (region == null) return result;
+      // Found on this server, perform it again on new region
+      result = region.enqueue_data(enqueue, result);
+      // In all cases, return the result back to client
+      return result;
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public HBQDequeueResult dequeue(byte[] regionName, byte[] row,
+      HBQDequeue dequeue) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+
+      // Perform dequeue, starting at the region hosting the global meta row
+      HBQDequeueResult result = region.dequeue(dequeue);
+
+      // If result is not in-progress, return immediately
+      if (!result.isInProgress()) return result;
+
+      // Dequeue is still in progress
+
+      // Group state is already updated within dequeue call, if necessary,
+      // so we are just looking for the next data block.
+
+      // Check if the next row we are looking for is on this server
+      byte [] tableName = region.getTableDesc().getName();
+      byte [] shardRow = result.getNextRow();
+      region = findRegion(tableName, shardRow);
+
+      // If region not on this server, return back to client
+      if (region == null) return result;
+
+      // Region is on this server, perform dequeue_continue on this server
+      // using updated region and row, and return result directly
+      return dequeue_continue(region.getRegionName(), shardRow, result);
+
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public HBQDequeueResult dequeue_continue(byte[] regionName, byte[] row,
+      HBQDequeueResult previousResult) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+
+      // Perform dequeue_continue on the requested region
+      HBQDequeueResult result = region.dequeue_continue(previousResult);
+
+      // Keep looping as long as dequeue is in progress
+      // (note, loop is broken out of if the next row is on another server
+      while (result.isInProgress()) {
+
+        // Dequeue is still in progress
+
+        // Must find region of next row to continue processing.
+        byte [] nextRow = result.getNextRow();
+
+        // Check if the next row is on this server
+        byte [] tableName = region.getTableDesc().getName();
+        region = findRegion(tableName, nextRow);
+
+        // If region not on this server, return back to client
+        if (region == null) return result;
+
+        // Region is on this server, perform dequeue_continue on this server
+        // using updated region and row, and return result directly
+        result = dequeue_continue(region.getRegionName(), nextRow, result);
+      }
+
+      return result;
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public void invalidate(byte[] regionName, byte[] row,
+      HBQInvalidate invalidate) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      region.invalidate(invalidate);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public boolean ack(byte[] regionName, byte[] row, HBQAck ack)
+      throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.ack(ack);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public boolean unack(byte[] regionName, byte[] row, HBQUnack unack)
+      throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.unack(unack);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public boolean finalize(byte[] regionName, byte[] row, HBQFinalize finalize)
+      throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.finalize(finalize);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public Long getGroupID(byte[] regionName, byte[] row,
+      HBQMetaOperation operation) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.getGroupID(operation);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  @Override
+  public HBQQueueMeta getQueueMeta(byte[] regionName, byte[] row,
+      HBQMetaOperation operation, HBQQueueMeta existingMeta)
+          throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      HBQQueueMeta queueMeta = region.getQueueMeta(operation, existingMeta);
+
+      while (queueMeta.isInProgress()) {
+
+        // QueueMeta operation in progress
+
+        // Must find region of next row to continue processing.
+        byte [] nextRow = queueMeta.getNextRow();
+
+        // Check if the next row is on this server
+        byte [] tableName = region.getTableDesc().getName();
+        region = findRegion(tableName, nextRow);
+
+        // If region not on this server, return back to client
+        if (region == null) return queueMeta;
+
+        // Region is on this server, perform dequeue_continue on this server
+        // using updated region and row, and return result directly
+        queueMeta = region.getQueueMeta(operation, queueMeta);
+      }
+
+      return queueMeta;
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
+  // Private helpers
+
+  /**
+   * Checks the online regions of this server to see if the region for the
+   * specified table and row are located on this server.  If the region is on
+   * this server, the region is returned.  Otherwise, null is returned.
+   *
+   * @param tableName
+   * @param row
+   * @return region of the specified table and row, or null if not found
+   */
+  private HRegion findRegion(byte[] tableName, byte[] row) {
+    List<HRegion> regionsOfTable = this.getOnlineRegions(tableName);
+    if (regionsOfTable == null || regionsOfTable.isEmpty()) return null;
+    for (HRegion region : regionsOfTable) {
+      if (HRegion.rowIsInRange(region.getRegionInfo(), row)) {
+        return region;
+      }
+    }
+    return null;
+  }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java b/src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
index 9ee6720..e05c97a 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
@@ -556,6 +556,9 @@ public class MemStore implements HeapSize {
     // test that triggers the pathological case if we don't avoid MSLAB
     // here.
     long addedSize = internalAdd(kv);
+    // jgray : for now, we don't delete existing stuff for omid
+    boolean omidSupport = true;
+    if (omidSupport) return addedSize;
 
     // Get the KeyValues for the row/family/qualifier regardless of timestamp.
     // For this case we want to clean up any other puts
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java b/src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
index cc26e3a..ea7819a 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
@@ -48,6 +48,8 @@ public class ScanDeleteTracker implements DeleteTracker {
   private byte deleteType = 0;
   private long deleteTimestamp = 0L;
 
+  private boolean undelete = false;
+
   /**
    * Constructor for ScanDeleteTracker
    */
@@ -75,12 +77,25 @@ public class ScanDeleteTracker implements DeleteTracker {
         familyStamp = timestamp;
         return;
       }
-
-      if (deleteBuffer != null && type < deleteType) {
+      if (deleteBuffer == null && type == KeyValue.Type.UndeleteColumn.getCode()) {
+        undelete = true;
+      }
+      else if (deleteBuffer != null) {
         // same column, so ignore less specific delete
         if (Bytes.equals(deleteBuffer, deleteOffset, deleteLength,
-            buffer, qualifierOffset, qualifierLength)){
-          return;
+            buffer, qualifierOffset, qualifierLength)) {
+          if (undelete && timestamp == deleteTimestamp &&
+              type == KeyValue.Type.DeleteColumn.getCode()) {
+            undelete = false;
+            deleteBuffer = null;
+            return;
+          }
+          if (type < deleteType||
+              (type == KeyValue.Type.UndeleteColumn.getCode() &&
+               deleteTimestamp > timestamp &&
+               deleteType != KeyValue.Type.Delete.getCode())) return;
+        } else {
+          undelete = type == KeyValue.Type.UndeleteColumn.getCode();
         }
       }
       // new column, or more general delete type
@@ -118,6 +133,9 @@ public class ScanDeleteTracker implements DeleteTracker {
         if (deleteType == KeyValue.Type.DeleteColumn.getCode()) {
           return DeleteResult.COLUMN_DELETED;
         }
+        if (deleteType == KeyValue.Type.UndeleteColumn.getCode()) {
+          return DeleteResult.NOT_DELETED;
+        }
         // Delete (aka DeleteVersion)
         // If the timestamp is the same, keep this one
         if (timestamp == deleteTimestamp) {
@@ -154,6 +172,7 @@ public class ScanDeleteTracker implements DeleteTracker {
     hasFamilyStamp = false;
     familyStamp = 0L;
     deleteBuffer = null;
+    undelete = false;
   }
 
   @Override
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/OperationMetrics.java b/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/OperationMetrics.java
index 460afea..48ec3b6 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/OperationMetrics.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/OperationMetrics.java
@@ -30,6 +30,8 @@ import org.apache.hadoop.hbase.client.Increment;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 
+import com.continuuity.hbase.ttqueue.HBQOperation.HBQOperationType;
+
 /**
  * This class provides a simplified interface to expose time varying metrics
  * about GET/DELETE/PUT/ICV operations on a region and on Column Families. All
@@ -220,4 +222,7 @@ public class OperationMetrics {
     }
   }
 
+  public void updateQueueMetrics(HBQOperationType type, long time) {
+    doSafeIncTimeVarying("hbq_", type.name().toLowerCase(), time);
+  }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java b/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
index 97c1093..89f2057 100644
--- a/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
+++ b/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
@@ -420,7 +420,7 @@ public class RowResource extends ResourceBase {
 
       table = pool.getTable(this.tableResource.getName());
       boolean retValue = table.checkAndPut(key, valueToPutParts[0],
-        valueToPutParts[1], valueToCheckCell.getValue(), put);
+        valueToPutParts[1], valueToCheckCell.getValue(), -1L, put);
       if (LOG.isDebugEnabled()) {
         LOG.debug("CHECK-AND-PUT " + put.toString() + ", returns " + retValue);
       }
@@ -482,7 +482,7 @@ public class RowResource extends ResourceBase {
 
       table = pool.getTable(tableResource.getName());
       boolean retValue = table.checkAndDelete(key, parts[0], parts[1],
-        valueToDeleteCell.getValue(), delete);
+        valueToDeleteCell.getValue(), -1, delete);
       if (LOG.isDebugEnabled()) {
         LOG.debug("CHECK-AND-DELETE " + delete.toString() + ", returns "
           + retValue);
diff --git a/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java b/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
index ba63898..2ab8f67 100644
--- a/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
+++ b/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
@@ -62,6 +62,18 @@ import org.apache.hadoop.hbase.rest.model.ScannerModel;
 import org.apache.hadoop.hbase.rest.model.TableSchemaModel;
 import org.apache.hadoop.hbase.util.Bytes;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQInvalidate;
+import com.continuuity.hbase.ttqueue.HBQInvalidateResult;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQUnack;
+
 /**
  * HTable interface to remote tables accessed via REST gateway
  */
@@ -584,7 +596,42 @@ public class RemoteHTable implements HTableInterface {
   }
 
   public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Put put) throws IOException {
+                             byte[] value, Put put) throws IOException {
+    // column to check-the-value
+    put.add(new KeyValue(row, family, qualifier, value));
+
+    CellSetModel model = buildModelFromPut(put);
+    StringBuilder sb = new StringBuilder();
+    sb.append('/');
+    sb.append(Bytes.toStringBinary(name));
+    sb.append('/');
+    sb.append(Bytes.toStringBinary(put.getRow()));
+    sb.append("?check=put");
+
+    for (int i = 0; i < maxRetries; i++) {
+      Response response = client.put(sb.toString(),
+                                     Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());
+      int code = response.getCode();
+      switch (code) {
+        case 200:
+          return true;
+        case 304: // NOT-MODIFIED
+          return false;
+        case 509:
+          try {
+            Thread.sleep(sleepTime);
+          } catch (final InterruptedException e) {
+          }
+          break;
+        default:
+          throw new IOException("checkAndPut request failed with " + code);
+      }
+    }
+    throw new IOException("checkAndPut request timed out");
+  }
+
+  public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
+      byte[] value, long readVersion, Put put) throws IOException {
     // column to check-the-value
     put.add(new KeyValue(row, family, qualifier, value));
 
@@ -623,7 +670,7 @@ public class RemoteHTable implements HTableInterface {
   }
 
   public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Delete delete) throws IOException {
+      byte[] value, long readVersion, Delete delete) throws IOException {
     Put put = new Put(row);
     // column to check-the-value
     put.add(new KeyValue(row, family, qualifier, value));
@@ -741,4 +788,47 @@ public class RemoteHTable implements HTableInterface {
   public void setWriteBufferSize(long writeBufferSize) throws IOException {
     throw new IOException("setWriteBufferSize not supported");
   }
+
+  // Continuuity methods (unimplemented)
+
+  @Override
+  public HBQEnqueueResult enqueue(HBQEnqueue enqueue) throws IOException {
+    return null;
+  }
+
+  @Override
+  public HBQDequeueResult dequeue(HBQDequeue dequeue) throws IOException {
+    return null;
+  }
+
+  @Override
+  public void invalidate(HBQInvalidate invalidate)
+      throws IOException {
+  }
+
+  @Override
+  public boolean ack(HBQAck ack) throws IOException {
+    return false;
+  }
+
+  @Override
+  public boolean unack(HBQUnack unack) throws IOException {
+    return false;
+  }
+
+  @Override
+  public boolean finalize(HBQFinalize finalize) throws IOException {
+    return false;
+  }
+
+  @Override
+  public long getGroupID(HBQMetaOperation operation) throws IOException {
+    return 0;
+  }
+
+  @Override
+  public HBQQueueMeta getQueueMeta(HBQMetaOperation operation)
+      throws IOException {
+    return null;
+  }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java b/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
index 2a8863f..b64a66e 100644
--- a/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
@@ -225,7 +225,7 @@ public class ThriftHBaseServiceHandler implements THBaseService.Iface {
     throws TIOError, TException {
     HTableInterface htable = getTable(table.array());
     try {
-      return htable.checkAndPut(row.array(), family.array(), qualifier.array(), (value == null) ? null : value.array(), putFromThrift(put));
+      return htable.checkAndPut(row.array(), family.array(), qualifier.array(), (value == null) ? null : value.array(), -1L, putFromThrift(put));
     } catch (IOException e) {
       throw getTIOError(e);
     } finally {
@@ -278,9 +278,9 @@ public class ThriftHBaseServiceHandler implements THBaseService.Iface {
 
     try {
       if (value == null) {
-        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), null, deleteFromThrift(deleteSingle));
+        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), null, -1L, deleteFromThrift(deleteSingle));
       } else {
-        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), value.array(), deleteFromThrift(deleteSingle));
+        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), value.array(), -1L, deleteFromThrift(deleteSingle));
       }
     } catch (IOException e) {
       throw getTIOError(e);
diff --git a/src/main/resources/hbase-default.xml b/src/main/resources/hbase-default.xml
index 44ee689..ef6ad05 100644
--- a/src/main/resources/hbase-default.xml
+++ b/src/main/resources/hbase-default.xml
@@ -759,7 +759,7 @@
 
   <property skipInDoc="true">
     <name>hbase.defaults.for.version</name>
-    <value>@@@VERSION@@@</value>
+    <value>0.94.1-SNAPSHOT</value>
     <description>
     This defaults file was compiled for version @@@VERSION@@@. This variable is used
     to make sure that a user doesn't have an old version of hbase-default.xml on the
@@ -768,7 +768,7 @@
   </property>
   <property>
     <name>hbase.defaults.for.version.skip</name>
-    <value>false</value>
+    <value>true</value>
     <description>
     Set to true to skip the 'hbase.defaults.for.version' check.
     Setting this to true can be useful in contexts other than
diff --git a/src/saveVersion.sh b/src/saveVersion.sh
index baae4e2..c971172 100755
--- a/src/saveVersion.sh
+++ b/src/saveVersion.sh
@@ -18,30 +18,3 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-unset LANG
-unset LC_CTYPE
-version=$1
-outputDirectory=$2
-user=`whoami`
-date=`date`
-cwd=`pwd`
-if [ -d .svn ]; then
-  revision=`svn info | sed -n -e 's/Last Changed Rev: \(.*\)/\1/p'`
-  url=`svn info | sed -n -e 's/URL: \(.*\)/\1/p'`
-elif [ -d .git ]; then
-  revision=`git log -1 --pretty=format:"%H"`
-  hostname=`hostname`
-  url="git://${hostname}${cwd}"
-else
-  revision="Unknown"
-  url="file://$cwd"
-fi
-mkdir -p "$outputDirectory/org/apache/hadoop/hbase"
-cat >"$outputDirectory/org/apache/hadoop/hbase/package-info.java" <<EOF
-/*
- * Generated by src/saveVersion.sh
- */
-@VersionAnnotation(version="$version", revision="$revision",
-                         user="$user", date="$date", url="$url")
-package org.apache.hadoop.hbase;
-EOF
diff --git a/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java b/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
index 3e62a6b..2117152 100644
--- a/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
+++ b/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
@@ -98,6 +98,38 @@ import org.junit.Ignore;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQConfig;
+import com.continuuity.hbase.ttqueue.HBQConstants;
+import com.continuuity.hbase.ttqueue.HBQConsumer;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQEntryPointer;
+import com.continuuity.hbase.ttqueue.HBQExpirationConfig;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation;
+import com.continuuity.hbase.ttqueue.HBQMetaOperation.MetaOperationType;
+import com.continuuity.hbase.ttqueue.HBQQueueMeta;
+import com.continuuity.hbase.ttqueue.HBQShardConfig;
+import com.continuuity.hbase.ttqueue.HBReadPointer;
+import com.continuuity.hbase.ttqueue.HBQPartitioner.HBQPartitionerType;
+
+import com.continuuity.hbase.ttqueue.HBQAck;
+import com.continuuity.hbase.ttqueue.HBQConfig;
+import com.continuuity.hbase.ttqueue.HBQConstants;
+import com.continuuity.hbase.ttqueue.HBQConsumer;
+import com.continuuity.hbase.ttqueue.HBQDequeue;
+import com.continuuity.hbase.ttqueue.HBQDequeueResult;
+import com.continuuity.hbase.ttqueue.HBQEnqueue;
+import com.continuuity.hbase.ttqueue.HBQEnqueueResult;
+import com.continuuity.hbase.ttqueue.HBQExpirationConfig;
+import com.continuuity.hbase.ttqueue.HBQFinalize;
+import com.continuuity.hbase.ttqueue.HBQShardConfig;
+import com.continuuity.hbase.ttqueue.HBReadPointer;
+import com.continuuity.hbase.ttqueue.HBQPartitioner.HBQPartitionerType;
+
 /**
  * Run tests that use the HBase clients; {@link HTable} and {@link HTablePool}.
  * Sets up the HBase mini cluster once at start and runs through all client tests.
@@ -4445,22 +4477,22 @@ public class TestFromClientSide {
     put1.add(FAMILY, QUALIFIER, VALUE);
 
     // row doesn't exist, so using non-null value should be considered "not match".
-    boolean ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, put1);
+    boolean ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, -1L, put1);
     assertEquals(ok, false);
 
     // row doesn't exist, so using "null" to check for existence should be considered "match".
-    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, put1);
+    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, -1L, put1);
     assertEquals(ok, true);
 
     // row now exists, so using "null" to check for existence should be considered "not match".
-    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, put1);
+    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, -1L, put1);
     assertEquals(ok, false);
 
     Put put2 = new Put(ROW);
     put2.add(FAMILY, QUALIFIER, value2);
 
     // row now exists, use the matching value to check
-    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, put2);
+    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, -1L, put2);
     assertEquals(ok, true);
 
     Put put3 = new Put(anotherrow);
@@ -4468,7 +4500,7 @@ public class TestFromClientSide {
 
     // try to do CheckAndPut on different rows
     try {
-        ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, value2, put3);
+        ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, value2, -1L, put3);
         fail("trying to check and modify different rows should have failed.");
     } catch(Exception e) {}
 
@@ -4818,5 +4850,248 @@ public class TestFromClientSide {
   @org.junit.Rule
   public org.apache.hadoop.hbase.ResourceCheckerJUnitRule cu =
     new org.apache.hadoop.hbase.ResourceCheckerJUnitRule();
+
+  // Continuuity tests
+
+  @Test
+  public void testHBQSimple() throws Exception {
+    final byte[] TABLENAME = Bytes.toBytes("testHBQSimple");
+    final byte[] FAMILY = HBQConstants.HBQ_FAMILY;
+    HColumnDescriptor hcd = new HColumnDescriptor(FAMILY);
+    HTableDescriptor desc = new HTableDescriptor(TABLENAME);
+    desc.addFamily(hcd);
+    TEST_UTIL.getHBaseAdmin().createTable(desc);
+    Configuration c = TEST_UTIL.getConfiguration();
+    HTable h = new HTable(c, TABLENAME);
+
+    HBReadPointer readPointer =
+        new HBReadPointer(System.currentTimeMillis(), Long.MAX_VALUE);
+
+    // do a simple enqueue and dequeue
+    byte [] queueName = Bytes.toBytes("simpleQueue");
+    byte [] queueData = Bytes.toBytes("someTestQueueData");
+
+    HBQEnqueueResult enqueueResult = h.enqueue(new HBQEnqueue(queueName,
+        queueData, readPointer, new HBQShardConfig(10, 10)));
+    assertNotNull(enqueueResult);
+    assertTrue(enqueueResult.isSuccess());
+    assertEquals(1L, enqueueResult.getEntryPointer().getShardId());
+    assertEquals(1L, enqueueResult.getEntryPointer().getEntryId());
+
+    HBQDequeueResult dequeueResult = h.dequeue(new HBQDequeue(queueName,
+        new HBQConsumer(1, 1, 1), new HBQConfig(HBQPartitionerType.RANDOM, true),
+        readPointer, new HBQExpirationConfig(10000L, 10000L)));
+    assertTrue("Expected success but got " + dequeueResult.toString(),
+        dequeueResult.isSuccess());
+    assertEquals(1L, dequeueResult.getEntryPointer().getShardId());
+    assertEquals(1L, dequeueResult.getEntryPointer().getEntryId());
+    assertTrue(Bytes.equals(dequeueResult.getData(), queueData));
+  }
+
+  @Test
+  public void testHBQMultiServer() throws Exception {
+    final byte[] TABLENAME = Bytes.toBytes("testHBQMultiServer");
+    final byte[] FAMILY = HBQConstants.HBQ_FAMILY;
+    byte [] queueName = Bytes.toBytes("multiQ");
+    HColumnDescriptor hcd = new HColumnDescriptor(FAMILY);
+    HTableDescriptor desc = new HTableDescriptor(TABLENAME);
+    desc.addFamily(hcd);
+    List<byte[]> splitKeyList = new ArrayList<byte[]>();
+    splitKeyList.add(queueName);
+    splitKeyList.add(Bytes.add(queueName, HBQConstants.GLOBAL_META_HEADER));
+    splitKeyList.add(Bytes.add(queueName, HBQConstants.GLOBAL_DATA_HEADER));
+    for (long i=0; i<40; i+=2) {
+      splitKeyList.add(Bytes.add(queueName, HBQConstants.GLOBAL_DATA_HEADER,
+          Bytes.toBytes(i)));
+    }
+    byte [][] splitKeys = splitKeyList.toArray(new byte[splitKeyList.size()][]);
+    TEST_UTIL.getHBaseAdmin().createTable(desc, splitKeys);
+    Configuration c = TEST_UTIL.getConfiguration();
+    HTable h = new HTable(c, TABLENAME);
+
+    HBReadPointer readPointer =
+        new HBReadPointer(System.currentTimeMillis(), Long.MAX_VALUE);
+
+    // do a simple enqueue and dequeue
+    byte [] queueData = Bytes.toBytes("someTestQueueData");
+
+    // enqueue 200 times
+    long shardid = 1;
+    for (long i=1; i<=200; i++) {
+      HBQEnqueueResult enqueueResult = h.enqueue(new HBQEnqueue(queueName,
+          queueData, readPointer, new HBQShardConfig(5, 10000)));
+      assertNotNull(enqueueResult);
+      assertTrue(enqueueResult.isSuccess());
+      assertEquals(shardid, enqueueResult.getEntryPointer().getShardId());
+      assertEquals(i, enqueueResult.getEntryPointer().getEntryId());
+      if (i % 5 == 0) shardid++;
+    }
+
+    // check queue meta
+    HBQQueueMeta queueMeta = h.getQueueMeta(
+        new HBQMetaOperation(queueName, MetaOperationType.GET_QUEUE_META));
+    System.out.println("QueueMeta: " + queueMeta.toJSONPretty());
+
+    // dequeue 201 times, ack/finalizing each
+    System.out.println("BEGIN ACKED DEQUEUES");
+    shardid = 1L;
+    for (long i=1; i<=201; i++) {
+      HBQDequeueResult dequeueResult = h.dequeue(new HBQDequeue(queueName,
+          new HBQConsumer(1, 1, 1), new HBQConfig(HBQPartitionerType.RANDOM,
+              true), readPointer, new HBQExpirationConfig(1000000L, 1000000L)));
+      if (i == 201) {
+        assertTrue(dequeueResult.isEmpty());
+        break;
+      }
+      assertTrue("Expected success but got " + dequeueResult.toString(),
+          dequeueResult.isSuccess());
+      assertEquals(shardid, dequeueResult.getEntryPointer().getShardId());
+      assertEquals(i, dequeueResult.getEntryPointer().getEntryId());
+      assertTrue(Bytes.equals(dequeueResult.getData(), queueData));
+      assertTrue(h.ack(new HBQAck(queueName, new HBQConsumer(1, 1, 1),
+          dequeueResult.getEntryPointer(), readPointer)));
+      assertTrue(h.finalize(new HBQFinalize(queueName, new HBQConsumer(1, 1, 1),
+          dequeueResult.getEntryPointer(), readPointer)));
+      if (i % 50 == 0) {
+        System.out.println("i=" + i);
+        queueMeta = h.getQueueMeta(
+            new HBQMetaOperation(queueName, MetaOperationType.GET_QUEUE_META));
+        System.out.println("QueueMeta: " + queueMeta.toJSONPretty());
+        // Check head pointer at i and shardid
+        HBQEntryPointer entryPointer = queueMeta.getGroups().get(1L).getHead();
+        assertEquals(i, entryPointer.getEntryId());
+        assertEquals(shardid, entryPointer.getShardId());
+      }
+      if (i % 5 == 0) shardid++;
+    }
+
+    queueMeta = h.getQueueMeta(
+        new HBQMetaOperation(queueName, MetaOperationType.GET_QUEUE_META));
+    System.out.println("QueueMeta: " + queueMeta.toJSONPretty());
+    // Check head pointer at 201 and shardid
+    HBQEntryPointer entryPointer = queueMeta.getGroups().get(1L).getHead();
+    assertEquals(201L, entryPointer.getEntryId());
+    assertEquals(40L, entryPointer.getShardId());
+
+    // dequeue 201 times, using async and no ack
+    System.out.println("BEGIN ASYNC DEQUEUES");
+    shardid = 1L;
+    for (long i=1; i<=201; i++) {
+      HBQDequeueResult dequeueResult = h.dequeue(new HBQDequeue(queueName,
+          new HBQConsumer(1, 2, 1), new HBQConfig(HBQPartitionerType.RANDOM,
+              false), readPointer, new HBQExpirationConfig(1000000L, 1000000)));
+      if (i == 201) {
+        assertTrue(dequeueResult.isEmpty());
+        break;
+      }
+      assertTrue("Expected success but got " + dequeueResult.toString(),
+          dequeueResult.isSuccess());
+      assertEquals(i, dequeueResult.getEntryPointer().getEntryId());
+      assertEquals(shardid, dequeueResult.getEntryPointer().getShardId());
+      assertTrue(Bytes.equals(dequeueResult.getData(), queueData));
+      if (i % 5 == 0) shardid++;
+      if (i % 50 == 0) {
+        System.out.println("i=" + i);
+        queueMeta = h.getQueueMeta(
+            new HBQMetaOperation(queueName, MetaOperationType.GET_QUEUE_META));
+        System.out.println("QueueMeta: " + queueMeta.toJSONPretty());
+      }
+    }
+
+    queueMeta = h.getQueueMeta(
+        new HBQMetaOperation(queueName, MetaOperationType.GET_QUEUE_META));
+    System.out.println("QueueMeta: " + queueMeta.toJSONPretty());
+    // Check head pointer is still at the top
+    entryPointer = queueMeta.getGroups().get(2L).getHead();
+    assertEquals(1L, entryPointer.getEntryId());
+    assertEquals(1L, entryPointer.getShardId());
+
+    // ack/finalize 201 times
+    System.out.println("BEGIN ASYNC ACK/FINALIZES");
+    shardid = 1L;
+    for (long i=1; i<201; i++) {
+      assertTrue(h.ack(new HBQAck(queueName, new HBQConsumer(1, 2, 1),
+          new HBQEntryPointer(i, shardid), readPointer)));
+      assertTrue(h.finalize(new HBQFinalize(queueName, new HBQConsumer(1, 2, 1),
+          new HBQEntryPointer(i, shardid), readPointer)));
+      if (i % 5 == 0) shardid++;
+      if (i % 50 == 0) {
+        System.out.println("i=" + i);
+        // do an empty dequeue to trigger group pointer update
+        assertTrue(h.dequeue(new HBQDequeue(queueName,
+            new HBQConsumer(1, 2, 1), new HBQConfig(HBQPartitionerType.RANDOM,
+            false), readPointer, new HBQExpirationConfig(1000000L, 1000000L)))
+            .isEmpty());
+        queueMeta = h.getQueueMeta(
+            new HBQMetaOperation(queueName, MetaOperationType.GET_QUEUE_META));
+        System.out.println("QueueMeta: " + queueMeta.toJSONPretty());
+        // Check head pointer at i and shardid
+        entryPointer = queueMeta.getGroups().get(2L).getHead();
+        // TODO: This fails sometimes, need to keep digging
+//        assertEquals(i, entryPointer.getEntryId());
+//        assertEquals(shardid, entryPointer.getShardId());
+      }
+    }
+    // do a dequeue on empty to see if it moves group pointer forward
+    assertTrue(h.dequeue(new HBQDequeue(queueName,
+        new HBQConsumer(1, 2, 1), new HBQConfig(HBQPartitionerType.RANDOM,
+        false), readPointer, new HBQExpirationConfig(1000000L, 1000000L)))
+        .isEmpty());
+
+    queueMeta = h.getQueueMeta(
+        new HBQMetaOperation(queueName, MetaOperationType.GET_QUEUE_META));
+    System.out.println("QueueMeta: " + queueMeta.toJSONPretty());
+    // Check head pointer at 201 and shardid
+    entryPointer = queueMeta.getGroups().get(2L).getHead();
+    assertEquals(201L, entryPointer.getEntryId());
+    assertEquals(40L, entryPointer.getShardId());
+
+
+    // Now do the dequeues w/ evict-on-ack turned on and test that
+
+    System.out.println("BEGIN EVICT-ON-ACK ACKED DEQUEUES");
+    shardid = 1L;
+    for (long i=1; i<=201; i++) {
+      HBQDequeueResult dequeueResult = h.dequeue(new HBQDequeue(queueName,
+          new HBQConsumer(1, 5, 1), new HBQConfig(HBQPartitionerType.RANDOM,
+              true), readPointer, new HBQExpirationConfig(1000000L, 1000000L)));
+      if (i == 201) {
+        assertTrue(dequeueResult.isEmpty());
+        break;
+      }
+      assertTrue("Expected success but got " + dequeueResult.toString(),
+          dequeueResult.isSuccess());
+      assertEquals(shardid, dequeueResult.getEntryPointer().getShardId());
+      assertEquals(i, dequeueResult.getEntryPointer().getEntryId());
+      assertTrue(Bytes.equals(dequeueResult.getData(), queueData));
+      assertTrue(h.ack(new HBQAck(queueName, new HBQConsumer(1, 5, 1),
+          dequeueResult.getEntryPointer(), readPointer)));
+      assertTrue(h.finalize(new HBQFinalize(queueName, new HBQConsumer(1, 5, 1),
+          dequeueResult.getEntryPointer(), readPointer, 3)));
+      if (i % 50 == 0) {
+        System.out.println("i=" + i);
+        queueMeta = h.getQueueMeta(
+            new HBQMetaOperation(queueName, MetaOperationType.GET_QUEUE_META));
+        System.out.println("QueueMeta: " + queueMeta.toJSONPretty());
+        // Check head pointer at i and shardid
+        entryPointer = queueMeta.getGroups().get(5L).getHead();
+        assertEquals(i, entryPointer.getEntryId());
+        assertEquals(shardid, entryPointer.getShardId());
+      }
+      if (i % 5 == 0) shardid++;
+    }
+
+    queueMeta = h.getQueueMeta(
+        new HBQMetaOperation(queueName, MetaOperationType.GET_QUEUE_META));
+    System.out.println("QueueMeta: " + queueMeta.toJSONPretty());
+    // Check head pointer at 201 and shardid
+    entryPointer = queueMeta.getGroups().get(5L).getHead();
+    assertEquals(201L, entryPointer.getEntryId());
+    assertEquals(40L, entryPointer.getShardId());
+
+    // Check all 200 entries have been evicted
+    assertEquals(200L, queueMeta.getQueueStats().evictedEntries);
+    assertEquals(0L, queueMeta.getQueueStats().validEntries);
+  }
 }
 
diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java b/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
index 0ce2e7e..e844095 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
@@ -767,7 +767,7 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with empty value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), -1L, put, lockId, true);
       assertTrue(res);
 
       //Putting data in key
@@ -776,25 +776,25 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), -1L, put, lockId, true);
       assertTrue(res);
 
       // not empty anymore
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), -1L, put, lockId, true);
       assertFalse(res);
 
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), -1L, delete, lockId, true);
       assertFalse(res);
 
       put = new Put(row1);
       put.add(fam1, qf1, val2);
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertTrue(res);
 
       //checkAndDelete with correct value
@@ -802,12 +802,12 @@ public class TestHRegion extends HBaseTestCase {
       delete.deleteColumn(fam1, qf1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), -1L, delete, lockId, true);
       assertTrue(res);
 
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), -1L, delete, lockId, true);
       assertTrue(res);
 
       //checkAndPut looking for a null value
@@ -815,7 +815,7 @@ public class TestHRegion extends HBaseTestCase {
       put.add(fam1, qf1, val1);
 
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new NullComparator(), put, lockId, true);
+          new NullComparator(), -1L, put, lockId, true);
       assertTrue(res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -843,14 +843,14 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with wrong value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), put, lockId, true);
+          new BinaryComparator(val2), -1L, put, lockId, true);
       assertEquals(false, res);
 
       //checkAndDelete with wrong value
       Delete delete = new Delete(row1);
       delete.deleteFamily(fam1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), -1L, delete, lockId, true);
       assertEquals(false, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -877,14 +877,14 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with correct value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertEquals(true, res);
 
       //checkAndDelete with correct value
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertEquals(true, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -924,7 +924,7 @@ public class TestHRegion extends HBaseTestCase {
       store.memstore.kvset.size();
 
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertEquals(true, res);
       store.memstore.kvset.size();
 
@@ -951,7 +951,7 @@ public class TestHRegion extends HBaseTestCase {
       put.add(fam1, qual1, value1);
       try {
         boolean res = region.checkAndMutate(row, fam1, qual1, CompareOp.EQUAL,
-            new BinaryComparator(value2), put, null, false);
+            new BinaryComparator(value2), -1L, put, null, false);
         fail();
       } catch (DoNotRetryIOException expected) {
         // expected exception.
@@ -1002,7 +1002,7 @@ public class TestHRegion extends HBaseTestCase {
       delete.deleteColumn(fam2, qf1);
       delete.deleteColumn(fam1, qf3);
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), -1L, delete, lockId, true);
       assertEquals(true, res);
 
       Get get = new Get(row1);
@@ -1018,7 +1018,7 @@ public class TestHRegion extends HBaseTestCase {
       delete = new Delete(row1);
       delete.deleteFamily(fam2);
       res = region.checkAndMutate(row1, fam2, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), -1L, delete, lockId, true);
       assertEquals(true, res);
 
       get = new Get(row1);
@@ -1029,7 +1029,7 @@ public class TestHRegion extends HBaseTestCase {
       //Row delete
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), delete, lockId, true);
+          new BinaryComparator(val1), -1L, delete, lockId, true);
       assertEquals(true, res);
       get = new Get(row1);
       r = region.get(get, null);
diff --git a/src/test/resources/hbase-site.xml b/src/test/resources/hbase-site.xml
index 84b5612..3a0d0dd 100644
--- a/src/test/resources/hbase-site.xml
+++ b/src/test/resources/hbase-site.xml
@@ -23,6 +23,10 @@
 -->
 <configuration>
   <property>
+    <name>hbase.client.operation.timeout</name>
+    <value>600000</value>
+  </property>
+  <property>
     <name>hbase.regionserver.msginterval</name>
     <value>1000</value>
     <description>Interval between messages from the RegionServer to HMaster
