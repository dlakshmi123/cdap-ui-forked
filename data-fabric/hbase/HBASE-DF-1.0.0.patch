Index: src/saveVersion.sh
===================================================================
--- src/saveVersion.sh	(revision 1355202)
+++ src/saveVersion.sh	(working copy)
@@ -18,30 +18,3 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-unset LANG
-unset LC_CTYPE
-version=$1
-outputDirectory=$2
-user=`whoami`
-date=`date`
-cwd=`pwd`
-if [ -d .svn ]; then
-  revision=`svn info | sed -n -e 's/Last Changed Rev: \(.*\)/\1/p'`
-  url=`svn info | sed -n -e 's/URL: \(.*\)/\1/p'`
-elif [ -d .git ]; then
-  revision=`git log -1 --pretty=format:"%H"`
-  hostname=`hostname`
-  url="git://${hostname}${cwd}"
-else
-  revision="Unknown"
-  url="file://$cwd"
-fi
-mkdir -p "$outputDirectory/org/apache/hadoop/hbase"
-cat >"$outputDirectory/org/apache/hadoop/hbase/package-info.java" <<EOF
-/*
- * Generated by src/saveVersion.sh
- */
-@VersionAnnotation(version="$version", revision="$revision",
-                         user="$user", date="$date", url="$url")
-package org.apache.hadoop.hbase;
-EOF
Index: src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java	(revision 1355202)
+++ src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java	(working copy)
@@ -4389,22 +4389,22 @@
     put1.add(FAMILY, QUALIFIER, VALUE);
 
     // row doesn't exist, so using non-null value should be considered "not match".
-    boolean ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, put1);
+    boolean ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, -1L, put1);
     assertEquals(ok, false);
 
     // row doesn't exist, so using "null" to check for existence should be considered "match".
-    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, put1);
+    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, -1L, put1);
     assertEquals(ok, true);
 
     // row now exists, so using "null" to check for existence should be considered "not match".
-    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, put1);
+    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, null, -1L, put1);
     assertEquals(ok, false);
 
     Put put2 = new Put(ROW);
     put2.add(FAMILY, QUALIFIER, value2);
 
     // row now exists, use the matching value to check
-    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, put2);
+    ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, VALUE, -1L, put2);
     assertEquals(ok, true);
 
     Put put3 = new Put(anotherrow);
@@ -4412,7 +4412,7 @@
 
     // try to do CheckAndPut on different rows
     try {
-        ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, value2, put3);
+        ok = table.checkAndPut(ROW, FAMILY, QUALIFIER, value2, -1L, put3);
         fail("trying to check and modify different rows should have failed.");
     } catch(Exception e) {}
 
Index: src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java	(revision 1355202)
+++ src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java	(working copy)
@@ -642,7 +642,7 @@
 
       //checkAndPut with empty value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), -1L, put, lockId, true);
       assertTrue(res);
 
       //Putting data in key
@@ -651,25 +651,25 @@
 
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), -1L, put, lockId, true);
       assertTrue(res);
 
       // not empty anymore
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), -1L, put, lockId, true);
       assertFalse(res);
 
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), -1L, delete, lockId, true);
       assertFalse(res);
 
       put = new Put(row1);
       put.add(fam1, qf1, val2);
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertTrue(res);
 
       //checkAndDelete with correct value
@@ -677,12 +677,12 @@
       delete.deleteColumn(fam1, qf1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), -1L, delete, lockId, true);
       assertTrue(res);
 
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), -1L, delete, lockId, true);
       assertTrue(res);
 
       //checkAndPut looking for a null value
@@ -690,7 +690,7 @@
       put.add(fam1, qf1, val1);
 
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new NullComparator(), put, lockId, true);
+          new NullComparator(), -1L, put, lockId, true);
       assertTrue(res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -718,14 +718,14 @@
 
       //checkAndPut with wrong value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), put, lockId, true);
+          new BinaryComparator(val2), -1L, put, lockId, true);
       assertEquals(false, res);
 
       //checkAndDelete with wrong value
       Delete delete = new Delete(row1);
       delete.deleteFamily(fam1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), -1L, delete, lockId, true);
       assertEquals(false, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -752,14 +752,14 @@
 
       //checkAndPut with correct value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertEquals(true, res);
 
       //checkAndDelete with correct value
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertEquals(true, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -799,7 +799,7 @@
       store.memstore.kvset.size();
 
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), -1L, put, lockId, true);
       assertEquals(true, res);
       store.memstore.kvset.size();
 
@@ -826,7 +826,7 @@
       put.add(fam1, qual1, value1);
       try {
         boolean res = region.checkAndMutate(row, fam1, qual1, CompareOp.EQUAL,
-            new BinaryComparator(value2), put, null, false);
+            new BinaryComparator(value2), -1L, put, null, false);
         fail();
       } catch (DoNotRetryIOException expected) {
         // expected exception.
@@ -877,7 +877,7 @@
       delete.deleteColumn(fam2, qf1);
       delete.deleteColumn(fam1, qf3);
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), -1L, delete, lockId, true);
       assertEquals(true, res);
 
       Get get = new Get(row1);
@@ -893,7 +893,7 @@
       delete = new Delete(row1);
       delete.deleteFamily(fam2);
       res = region.checkAndMutate(row1, fam2, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), -1L, delete, lockId, true);
       assertEquals(true, res);
 
       get = new Get(row1);
@@ -904,7 +904,7 @@
       //Row delete
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), delete, lockId, true);
+          new BinaryComparator(val1), -1L, delete, lockId, true);
       assertEquals(true, res);
       get = new Get(row1);
       r = region.get(get, null);
Index: src/main/resources/hbase-default.xml
===================================================================
--- src/main/resources/hbase-default.xml	(revision 1355202)
+++ src/main/resources/hbase-default.xml	(working copy)
@@ -759,7 +759,7 @@
 
   <property skipInDoc="true">
     <name>hbase.defaults.for.version</name>
-    <value>@@@VERSION@@@</value>
+    <value>0.94.1-SNAPSHOT</value>
     <description>
     This defaults file was compiled for version @@@VERSION@@@. This variable is used
     to make sure that a user doesn't have an old version of hbase-default.xml on the
Index: src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java	(working copy)
@@ -414,13 +414,13 @@
       }
 
       public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-          byte[] value, Put put) throws IOException {
-        return table.checkAndPut(row, family, qualifier, value, put);
+          byte[] value, long readVersion, Put put) throws IOException {
+        return table.checkAndPut(row, family, qualifier, value, readVersion, put);
       }
 
       public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-          byte[] value, Delete delete) throws IOException {
-        return table.checkAndDelete(row, family, qualifier, value, delete);
+          byte[] value, long readVersion, Delete delete) throws IOException {
+        return table.checkAndDelete(row, family, qualifier, value, readVersion, delete);
       }
 
       public long incrementColumnValue(byte[] row, byte[] family,
Index: src/main/java/org/apache/hadoop/hbase/KeyValue.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/KeyValue.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/KeyValue.java	(working copy)
@@ -173,6 +173,7 @@
 
     Delete((byte)8),
     DeleteColumn((byte)12),
+    UndeleteColumn((byte)13),
     DeleteFamily((byte)14),
 
     // Maximum is used when searching; you look from maximum on down.
Index: src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(working copy)
@@ -223,7 +223,7 @@
    */
   public boolean checkAndPut(final byte[] regionName, final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Put put)
+      final long readVersion, final Put put)
   throws IOException;
 
 
@@ -237,13 +237,14 @@
    * @param family column family
    * @param qualifier column qualifier
    * @param value the expected value
+   * @param readVersion 
    * @param delete data to delete if check succeeds
    * @throws IOException e
    * @return true if the new delete was execute, false otherwise
    */
   public boolean checkAndDelete(final byte[] regionName, final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Delete delete)
+      long readVersion, final Delete delete)
   throws IOException;
 
   /**
Index: src/main/java/org/apache/hadoop/hbase/client/Increment.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/Increment.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/client/Increment.java	(working copy)
@@ -27,6 +27,7 @@
 import java.util.Set;
 import java.util.TreeMap;
 
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.Writable;
@@ -52,6 +53,7 @@
   private TimeRange tr = new TimeRange();
   private Map<byte [], NavigableMap<byte [], Long>> familyMap =
     new TreeMap<byte [], NavigableMap<byte [], Long>>(Bytes.BYTES_COMPARATOR);
+  private long writeVersion = HConstants.LATEST_TIMESTAMP;
 
   /** Constructor for Writable.  DO NOT USE */
   public Increment() {}
@@ -101,6 +103,24 @@
     return this;
   }
 
+  /**
+   * Sets the write version to use for this increment.
+   * @param writeVersion
+   * @return this Increment object
+   */
+  public Increment setWriteVersion(long writeVersion) {
+    this.writeVersion = writeVersion;
+    return this;
+  }
+  
+  /**
+   * Returns the write version to use for this increment.
+   * @return the write version to use
+   */
+  public long getWriteVersion() {
+    return this.writeVersion;
+  }
+  
   /* Accessors */
 
   /**
@@ -274,6 +294,7 @@
     this.tr = new TimeRange();
     tr.readFields(in);
     this.lockId = in.readLong();
+    this.writeVersion = in.readLong();
     int numFamilies = in.readInt();
     if (numFamilies == 0) {
       throw new IOException("At least one column required");
@@ -307,6 +328,7 @@
     Bytes.writeByteArray(out, this.row);
     tr.write(out);
     out.writeLong(this.lockId);
+    out.writeLong(this.writeVersion);
     if (familyMap.size() == 0) {
       throw new IOException("At least one column required");
     }
Index: src/main/java/org/apache/hadoop/hbase/client/Delete.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/Delete.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/client/Delete.java	(working copy)
@@ -212,6 +212,17 @@
     return this;
   }
 
+  public Delete undeleteColumns(byte [] family, byte [] qualifier, long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if (list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(new KeyValue(this.row, family, qualifier, timestamp,
+      KeyValue.Type.UndeleteColumn));
+    familyMap.put(family, list);
+    return this;
+  }
+  
   /**
    * Delete the latest version of the specified column.
    * This is an expensive call in that on the server-side, it first does a
Index: src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java	(working copy)
@@ -224,7 +224,7 @@
    * @return true if the new put was executed, false otherwise
    */
   boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Put put) throws IOException;
+      byte[] value, long readVersion, Put put) throws IOException;
 
   /**
    * Deletes the specified cells/row.
@@ -262,7 +262,7 @@
    * @return true if the new delete was executed, false otherwise
    */
   boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Delete delete) throws IOException;
+      byte[] value, long readVersion, Delete delete) throws IOException;
 
   /**
    * Performs multiple mutations atomically on a single row. Currently
Index: src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/HTablePool.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/client/HTablePool.java	(working copy)
@@ -404,8 +404,8 @@
 
     @Override
     public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-        byte[] value, Put put) throws IOException {
-      return table.checkAndPut(row, family, qualifier, value, put);
+        byte[] value, long readVersion, Put put) throws IOException {
+      return table.checkAndPut(row, family, qualifier, value, readVersion, put);
     }
 
     @Override
@@ -420,8 +420,8 @@
 
     @Override
     public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-        byte[] value, Delete delete) throws IOException {
-      return table.checkAndDelete(row, family, qualifier, value, delete);
+        byte[] value, long readVersion, Delete delete) throws IOException {
+      return table.checkAndDelete(row, family, qualifier, value, readVersion, delete);
     }
 
     @Override
Index: src/main/java/org/apache/hadoop/hbase/client/HTable.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/client/HTable.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/client/HTable.java	(working copy)
@@ -863,12 +863,12 @@
   @Override
   public boolean checkAndPut(final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Put put)
+      final long readVersion, final Put put)
   throws IOException {
     return new ServerCallable<Boolean>(connection, tableName, row, operationTimeout) {
           public Boolean call() throws IOException {
             return server.checkAndPut(location.getRegionInfo().getRegionName(),
-                row, family, qualifier, value, put) ? Boolean.TRUE : Boolean.FALSE;
+                row, family, qualifier, value, readVersion, put) ? Boolean.TRUE : Boolean.FALSE;
           }
         }.withRetries();
   }
@@ -880,13 +880,13 @@
   @Override
   public boolean checkAndDelete(final byte [] row,
       final byte [] family, final byte [] qualifier, final byte [] value,
-      final Delete delete)
+      final long readVersion, final Delete delete)
   throws IOException {
     return new ServerCallable<Boolean>(connection, tableName, row, operationTimeout) {
           public Boolean call() throws IOException {
             return server.checkAndDelete(
                 location.getRegionInfo().getRegionName(),
-                row, family, qualifier, value, delete)
+                row, family, qualifier, value, readVersion, delete)
             ? Boolean.TRUE : Boolean.FALSE;
           }
         }.withRetries();
Index: src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java	(working copy)
@@ -48,6 +48,8 @@
   private byte deleteType = 0;
   private long deleteTimestamp = 0L;
 
+  private boolean undelete = false;
+  
   /**
    * Constructor for ScanDeleteTracker
    */
@@ -75,12 +77,25 @@
         familyStamp = timestamp;
         return;
       }
-
-      if (deleteBuffer != null && type < deleteType) {
+      if (deleteBuffer == null && type == KeyValue.Type.UndeleteColumn.getCode()) {
+        undelete = true;
+      }
+      else if (deleteBuffer != null) {
         // same column, so ignore less specific delete
         if (Bytes.equals(deleteBuffer, deleteOffset, deleteLength,
-            buffer, qualifierOffset, qualifierLength)){
-          return;
+            buffer, qualifierOffset, qualifierLength)) {
+          if (undelete && timestamp == deleteTimestamp &&
+              type == KeyValue.Type.DeleteColumn.getCode()) {
+            undelete = false;
+            deleteBuffer = null;
+            return;
+          }
+          if (type < deleteType||
+              (type == KeyValue.Type.UndeleteColumn.getCode() &&
+               deleteTimestamp > timestamp &&
+               deleteType != KeyValue.Type.Delete.getCode())) return;
+        } else {
+          undelete = type == KeyValue.Type.UndeleteColumn.getCode();
         }
       }
       // new column, or more general delete type
@@ -118,6 +133,9 @@
         if (deleteType == KeyValue.Type.DeleteColumn.getCode()) {
           return DeleteResult.COLUMN_DELETED;
         }
+        if (deleteType == KeyValue.Type.UndeleteColumn.getCode()) {
+          return DeleteResult.NOT_DELETED;
+        }
         // Delete (aka DeleteVersion)
         // If the timestamp is the same, keep this one
         if (timestamp == deleteTimestamp) {
@@ -154,6 +172,7 @@
     hasFamilyStamp = false;
     familyStamp = 0L;
     deleteBuffer = null;
+    undelete = false;
   }
 
   @Override
Index: src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(working copy)
@@ -2242,13 +2242,14 @@
    * @param qualifier
    * @param compareOp
    * @param comparator
+   * @param readVersion 
    * @param lockId
    * @param writeToWAL
    * @throws IOException
    * @return true if the new put was execute, false otherwise
    */
   public boolean checkAndMutate(byte [] row, byte [] family, byte [] qualifier,
-      CompareOp compareOp, WritableByteArrayComparable comparator, Writable w,
+      CompareOp compareOp, WritableByteArrayComparable comparator, long readVersion, Writable w,
       Integer lockId, boolean writeToWAL)
   throws IOException{
     checkReadOnly();
@@ -2270,6 +2271,8 @@
       Get get = new Get(row, lock);
       checkFamily(family);
       get.addColumn(family, qualifier);
+      get.setTimeRange(0, readVersion <= 0 || readVersion == Long.MAX_VALUE ?
+        Long.MAX_VALUE : readVersion + 1);
 
       // Lock row
       Integer lid = getLock(lockId, get.getRow(), true);
@@ -4497,7 +4500,9 @@
       Integer lid = getLock(lockid, row, true);
       this.updatesLock.readLock().lock();
       try {
-        long now = EnvironmentEdgeManager.currentTimeMillis();
+        long now = increment.getWriteVersion() == HConstants.LATEST_TIMESTAMP ?
+            EnvironmentEdgeManager.currentTimeMillis() :
+            increment.getWriteVersion();
         // Process each family
         for (Map.Entry<byte [], NavigableMap<byte [], Long>> family :
           increment.getFamilyMap().entrySet()) {
Index: src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(working copy)
@@ -2045,8 +2045,8 @@
 
   private boolean checkAndMutate(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final CompareOp compareOp,
-      final WritableByteArrayComparable comparator, final Writable w,
-      Integer lock) throws IOException {
+      final WritableByteArrayComparable comparator, final long readVersion,
+      final Writable w, Integer lock) throws IOException {
     checkOpen();
     this.requestCount.incrementAndGet();
     HRegion region = getRegion(regionName);
@@ -2055,7 +2055,7 @@
         this.cacheFlusher.reclaimMemStoreMemory();
       }
       return region.checkAndMutate(row, family, qualifier, compareOp,
-        comparator, w, lock, true);
+        comparator, readVersion, w, lock, true);
     } catch (Throwable t) {
       throw convertThrowableToIOE(cleanup(t));
     }
@@ -2075,7 +2075,7 @@
    */
   public boolean checkAndPut(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final byte[] value,
-      final Put put) throws IOException {
+      final long readVersion, final Put put) throws IOException {
     checkOpen();
     if (regionName == null) {
       throw new IOException("Invalid arguments to checkAndPut "
@@ -2093,7 +2093,7 @@
       }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      CompareOp.EQUAL, new BinaryComparator(value), put,
+      CompareOp.EQUAL, new BinaryComparator(value), readVersion, put,
       lock);
     if (region.getCoprocessorHost() != null) {
       result = region.getCoprocessorHost().postCheckAndPut(row, family,
@@ -2116,7 +2116,8 @@
    */
   public boolean checkAndPut(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final CompareOp compareOp,
-      final WritableByteArrayComparable comparator, final Put put)
+      final WritableByteArrayComparable comparator,
+      final Put put)
        throws IOException {
     checkOpen();
     if (regionName == null) {
@@ -2133,7 +2134,7 @@
       }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      compareOp, comparator, put, lock);
+      compareOp, comparator, Long.MAX_VALUE, put, lock);
     if (region.getCoprocessorHost() != null) {
       result = region.getCoprocessorHost().postCheckAndPut(row, family,
         qualifier, compareOp, comparator, put, result);
@@ -2155,7 +2156,7 @@
    */
   public boolean checkAndDelete(final byte[] regionName, final byte[] row,
       final byte[] family, final byte[] qualifier, final byte[] value,
-      final Delete delete) throws IOException {
+      final long readVersion, final Delete delete) throws IOException {
     checkOpen();
 
     if (regionName == null) {
@@ -2173,7 +2174,7 @@
       }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      CompareOp.EQUAL, comparator, delete, lock);
+      CompareOp.EQUAL, comparator, readVersion, delete, lock);
     if (region.getCoprocessorHost() != null) {
       result = region.getCoprocessorHost().postCheckAndDelete(row, family,
         qualifier, CompareOp.EQUAL, comparator, delete, result);
@@ -2281,7 +2282,7 @@
      }
     }
     boolean result = checkAndMutate(regionName, row, family, qualifier,
-      compareOp, comparator, delete, lock);
+      compareOp, comparator, Long.MAX_VALUE, delete, lock);
    if (region.getCoprocessorHost() != null) {
      result = region.getCoprocessorHost().postCheckAndDelete(row, family,
        qualifier, compareOp, comparator, delete, result);
Index: src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java	(working copy)
@@ -556,6 +556,9 @@
     // test that triggers the pathological case if we don't avoid MSLAB
     // here.
     long addedSize = internalAdd(kv);
+    // jgray : for now, we don't delete existing stuff for omid
+    boolean omidSupport = true;
+    if (omidSupport) return addedSize;
 
     // Get the KeyValues for the row/family/qualifier regardless of timestamp.
     // For this case we want to clean up any other puts
Index: src/main/java/org/apache/hadoop/hbase/io/hfile/slab/SingleSizeCache.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/io/hfile/slab/SingleSizeCache.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/io/hfile/slab/SingleSizeCache.java	(working copy)
@@ -38,8 +38,10 @@
 import org.apache.hadoop.hbase.util.ClassSize;
 import org.apache.hadoop.util.StringUtils;
 
-import com.google.common.collect.MapEvictionListener;
-import com.google.common.collect.MapMaker;
+import com.google.common.cache.CacheBuilder;
+import com.google.common.cache.CacheLoader;
+import com.google.common.cache.RemovalListener;
+import com.google.common.cache.RemovalNotification;
 
 /**
  * SingleSizeCache is a slab allocated cache that caches elements up to a single
@@ -91,18 +93,20 @@
     // This evictionListener is called whenever the cache automatically
     // evicts
     // something.
-    MapEvictionListener<BlockCacheKey, CacheablePair> listener = new MapEvictionListener<BlockCacheKey, CacheablePair>() {
+    RemovalListener<BlockCacheKey, CacheablePair> listener =
+        new RemovalListener<BlockCacheKey, CacheablePair>() {
       @Override
-      public void onEviction(BlockCacheKey key, CacheablePair value) {
+      public void onRemoval(
+          RemovalNotification<BlockCacheKey, CacheablePair> value) {
         timeSinceLastAccess.set(System.nanoTime()
-            - value.recentlyAccessed.get());
+            - value.getValue().recentlyAccessed.get());
         stats.evict();
-        doEviction(key, value);
+        doEviction(value.getKey(), value.getValue());
       }
     };
 
-    backingMap = new MapMaker().maximumSize(numBlocks - 1)
-        .evictionListener(listener).makeMap();
+    backingMap = CacheBuilder.newBuilder().maximumSize(numBlocks - 1)
+        .removalListener(listener).build().asMap();
 
   }
 
Index: src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java	(working copy)
@@ -584,7 +584,7 @@
   }
 
   public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Put put) throws IOException {
+      byte[] value, long readVersion, Put put) throws IOException {
     // column to check-the-value
     put.add(new KeyValue(row, family, qualifier, value));
 
@@ -623,7 +623,7 @@
   }
 
   public boolean checkAndDelete(byte[] row, byte[] family, byte[] qualifier,
-      byte[] value, Delete delete) throws IOException {
+      byte[] value, long readVersion, Delete delete) throws IOException {
     Put put = new Put(row);
     // column to check-the-value
     put.add(new KeyValue(row, family, qualifier, value));
Index: src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/rest/RowResource.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/rest/RowResource.java	(working copy)
@@ -420,7 +420,7 @@
 
       table = pool.getTable(this.tableResource.getName());
       boolean retValue = table.checkAndPut(key, valueToPutParts[0],
-        valueToPutParts[1], valueToCheckCell.getValue(), put);
+        valueToPutParts[1], valueToCheckCell.getValue(), -1L, put);
       if (LOG.isDebugEnabled()) {
         LOG.debug("CHECK-AND-PUT " + put.toString() + ", returns " + retValue);
       }
@@ -482,7 +482,7 @@
 
       table = pool.getTable(tableResource.getName());
       boolean retValue = table.checkAndDelete(key, parts[0], parts[1],
-        valueToDeleteCell.getValue(), delete);
+        valueToDeleteCell.getValue(), -1, delete);
       if (LOG.isDebugEnabled()) {
         LOG.debug("CHECK-AND-DELETE " + delete.toString() + ", returns "
           + retValue);
Index: src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java	(revision 1355202)
+++ src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java	(working copy)
@@ -225,7 +225,7 @@
     throws TIOError, TException {
     HTableInterface htable = getTable(table.array());
     try {
-      return htable.checkAndPut(row.array(), family.array(), qualifier.array(), (value == null) ? null : value.array(), putFromThrift(put));
+      return htable.checkAndPut(row.array(), family.array(), qualifier.array(), (value == null) ? null : value.array(), -1L, putFromThrift(put));
     } catch (IOException e) {
       throw getTIOError(e);
     } finally {
@@ -278,9 +278,9 @@
 
     try {
       if (value == null) {
-        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), null, deleteFromThrift(deleteSingle));
+        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), null, -1L, deleteFromThrift(deleteSingle));
       } else {
-        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), value.array(), deleteFromThrift(deleteSingle));
+        return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), value.array(), -1L, deleteFromThrift(deleteSingle));
       }
     } catch (IOException e) {
       throw getTIOError(e);
Index: pom.xml
===================================================================
--- pom.xml	(revision 1355202)
+++ pom.xml	(working copy)
@@ -831,9 +831,9 @@
                              package="org.apache.hadoop.hbase.generated.regionserver"
                              webxml="${build.webapps}/regionserver/WEB-INF/web.xml"/>
 
-                <exec executable="sh">
+                <!--exec executable="sh">
                   <arg line="${basedir}/src/saveVersion.sh ${project.version} ${generated.sources}/java"/>
-                </exec>
+                </exec-->
               </target>
             </configuration>
             <goals>
@@ -977,7 +977,7 @@
     <commons-math.version>2.1</commons-math.version>
     <commons-configuration.version>1.6</commons-configuration.version>
     <metrics-core.version>2.1.2</metrics-core.version>
-    <guava.version>r09</guava.version>
+    <guava.version>12.0</guava.version>
     <jackson.version>1.8.8</jackson.version>
     <jasper.version>5.5.23</jasper.version>
     <jaxb-api.version>2.1</jaxb-api.version>
